"""Utilities for loading and validating Physionet EEG datasets.

Filtrage EEG 8–40 Hz (FIR/IIR) avec padding (FIR auto ~2–4*fs, IIR ordre 4).

Le filtre par défaut suit la contrainte WBS 3.1.1 : bande 8–40 Hz avec FIR
zero-phase (longueur auto MNE équivalente à un ordre ~401 sur des segments
>1s) ou IIR Butterworth (ordre 4) et un padding réfléchissant de 0.5 seconde
pour limiter les effets de bord sur les segments fenêtrés.
"""

# Garantit la compatibilité des annotations de type pour l’ensemble du module
from __future__ import annotations

# Préserve les fonctions de hachage pour valider l’intégrité des datasets
import hashlib

# Conserve json pour formater les rapports d’erreurs de comptage de runs
import json

# Capture les avertissements de lecture pour neutraliser les faux positifs
import warnings

# Utilise pathlib pour assurer la portabilité des interactions fichiers
# Introduit dataclass pour regrouper la configuration de rapport
from dataclasses import dataclass

# Utilise pathlib pour assurer la portabilité des interactions fichiers
from pathlib import Path

# Centralise les hints pour clarifier les attentes des appels et des tests
from typing import Any, Dict, List, Mapping, Tuple

# MNE est obligatoire pour le parsing EDF/BDF et la gestion des epochs
import mne

# Numpy offre des masques vectorisés pour filtrer rapidement les événements
import numpy as np

# Pandas gère les métadonnées annotées pour le contrôle qualité
import pandas as pd

# Typing numpy clarifie les formes et types pour mypy et les tests
from numpy.typing import NDArray

# Mappe les noms de canaux bruts Physionet vers le montage standard 10-20
RAW_TO_MONTAGE_CHANNEL_MAP: Dict[str, str] = {
    "Af3.": "AF3",
    "Af4.": "AF4",
    "Af7.": "AF7",
    "Af8.": "AF8",
    "Afz.": "AFz",
    "C1..": "C1",
    "C2..": "C2",
    "C3..": "C3",
    "C4..": "C4",
    "C5..": "C5",
    "C6..": "C6",
    "Cp1.": "CP1",
    "Cp2.": "CP2",
    "Cp3.": "CP3",
    "Cp4.": "CP4",
    "Cp5.": "CP5",
    "Cp6.": "CP6",
    "Cpz.": "CPz",
    "F1..": "F1",
    "F2..": "F2",
    "F3..": "F3",
    "F4..": "F4",
    "F5..": "F5",
    "F6..": "F6",
    "F7..": "F7",
    "F8..": "F8",
    "Fc1.": "FC1",
    "Fc2.": "FC2",
    "Fc3.": "FC3",
    "Fc4.": "FC4",
    "Fc5.": "FC5",
    "Fc6.": "FC6",
    "Fcz.": "FCz",
    "Fp1.": "Fp1",
    "Fp2.": "Fp2",
    "Fpz.": "Fpz",
    "Ft7.": "FT7",
    "Ft8.": "FT8",
    "Iz..": "Iz",
    "O1..": "O1",
    "O2..": "O2",
    "Oz..": "Oz",
    "P1..": "P1",
    "P2..": "P2",
    "P3..": "P3",
    "P4..": "P4",
    "P5..": "P5",
    "P6..": "P6",
    "P7..": "P7",
    "P8..": "P8",
    "Po3.": "PO3",
    "Po4.": "PO4",
    "Po7.": "PO7",
    "Po8.": "PO8",
    "Poz.": "POz",
    "T10.": "T10",
    "T7..": "T7",
    "T8..": "T8",
    "T9..": "T9",
    "Tp7.": "TP7",
    "Tp8.": "TP8",
    "Tz..": "Tz",
    "Cz..": "Cz",
    "Fz..": "Fz",
    "Pz..": "Pz",
}


# Provide a default mapping consistent with Physionet motor imagery labels
PHYSIONET_LABEL_MAP: Dict[str, int] = {"T0": 0, "T1": 1, "T2": 2}
# Provide an explicit mapping from Physionet events to motor imagery labels
MOTOR_EVENT_LABELS: Dict[str, str] = {"T1": "A", "T2": "B"}
# Déclare les étiquettes attendues pour vérifier la présence des classes
EXPECTED_LABELS: Tuple[str, str] = ("A", "B")
# Fixe la méthode de filtrage par défaut pour la cohérence API et tests
DEFAULT_FILTER_METHOD = "fir"
# Fixe la méthode de normalisation par défaut pour les canaux EEG
DEFAULT_NORMALIZE_METHOD = "zscore"
# Fixe l'epsilon de stabilisation pour la normalisation par défaut
DEFAULT_NORMALIZE_EPSILON = 1e-8
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


def x__rename_channels_for_montage__mutmut_orig(
    raw: mne.io.BaseRaw,
    channel_map: Mapping[str, str] = RAW_TO_MONTAGE_CHANNEL_MAP,
) -> mne.io.BaseRaw:
    """Renomme les canaux bruts pour les aligner sur le montage choisi."""

    # Construit un sous-mapping ne contenant que les canaux présents
    present_map = {old: new for old, new in channel_map.items() if old in raw.ch_names}
    # Applique le renommage uniquement sur les canaux existants
    if present_map:
        raw.rename_channels(present_map)
    # Retourne l'objet Raw avec des noms compatibles montage 10-20
    return raw


def x__rename_channels_for_montage__mutmut_1(
    raw: mne.io.BaseRaw,
    channel_map: Mapping[str, str] = RAW_TO_MONTAGE_CHANNEL_MAP,
) -> mne.io.BaseRaw:
    """Renomme les canaux bruts pour les aligner sur le montage choisi."""

    # Construit un sous-mapping ne contenant que les canaux présents
    present_map = None
    # Applique le renommage uniquement sur les canaux existants
    if present_map:
        raw.rename_channels(present_map)
    # Retourne l'objet Raw avec des noms compatibles montage 10-20
    return raw


def x__rename_channels_for_montage__mutmut_2(
    raw: mne.io.BaseRaw,
    channel_map: Mapping[str, str] = RAW_TO_MONTAGE_CHANNEL_MAP,
) -> mne.io.BaseRaw:
    """Renomme les canaux bruts pour les aligner sur le montage choisi."""

    # Construit un sous-mapping ne contenant que les canaux présents
    present_map = {old: new for old, new in channel_map.items() if old not in raw.ch_names}
    # Applique le renommage uniquement sur les canaux existants
    if present_map:
        raw.rename_channels(present_map)
    # Retourne l'objet Raw avec des noms compatibles montage 10-20
    return raw


def x__rename_channels_for_montage__mutmut_3(
    raw: mne.io.BaseRaw,
    channel_map: Mapping[str, str] = RAW_TO_MONTAGE_CHANNEL_MAP,
) -> mne.io.BaseRaw:
    """Renomme les canaux bruts pour les aligner sur le montage choisi."""

    # Construit un sous-mapping ne contenant que les canaux présents
    present_map = {old: new for old, new in channel_map.items() if old in raw.ch_names}
    # Applique le renommage uniquement sur les canaux existants
    if present_map:
        raw.rename_channels(None)
    # Retourne l'objet Raw avec des noms compatibles montage 10-20
    return raw

x__rename_channels_for_montage__mutmut_mutants : ClassVar[MutantDict] = {
'x__rename_channels_for_montage__mutmut_1': x__rename_channels_for_montage__mutmut_1, 
    'x__rename_channels_for_montage__mutmut_2': x__rename_channels_for_montage__mutmut_2, 
    'x__rename_channels_for_montage__mutmut_3': x__rename_channels_for_montage__mutmut_3
}

def _rename_channels_for_montage(*args, **kwargs):
    result = _mutmut_trampoline(x__rename_channels_for_montage__mutmut_orig, x__rename_channels_for_montage__mutmut_mutants, args, kwargs)
    return result 

_rename_channels_for_montage.__signature__ = _mutmut_signature(x__rename_channels_for_montage__mutmut_orig)
x__rename_channels_for_montage__mutmut_orig.__name__ = 'x__rename_channels_for_montage'


def x_apply_bandpass_filter__mutmut_orig(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_1(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 1.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_2(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = None
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_3(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = None
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_4(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.upper()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_5(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_6(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"XXfirXX", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_7(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"FIR", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_8(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "XXiirXX"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_9(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "IIR"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_10(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError(None)
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_11(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("XXmethod must be 'fir' or 'iir'XX")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_12(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("METHOD MUST BE 'FIR' OR 'IIR'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_13(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = None
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_14(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(None)
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_15(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["XXsfreqXX"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_16(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["SFREQ"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_17(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = None
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_18(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = None
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_19(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(None, 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_20(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), None)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_21(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_22(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), )
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_23(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(None), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_24(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(None)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_25(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration / sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_26(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 1)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_27(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = None
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_28(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples >= 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_29(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 1:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_30(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = None
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_31(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(None, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_32(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, None, mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_33(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode=None)
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_34(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_35(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_36(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), )
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_37(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((1, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_38(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 1), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_39(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="XXreflectXX")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_40(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="REFLECT")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_41(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = None
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_42(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method != "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_43(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "XXfirXX":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_44(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "FIR":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_45(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = None
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_46(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_47(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "XXautoXX"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_48(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "AUTO"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_49(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = None
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_50(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "XXmethodXX": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_51(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "METHOD": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_52(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "XXfirXX",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_53(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "FIR",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_54(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "XXfir_designXX": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_55(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "FIR_DESIGN": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_56(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "XXfirwinXX",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_57(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "FIRWIN",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_58(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "XXfir_windowXX": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_59(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "FIR_WINDOW": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_60(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "XXhammingXX",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_61(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "HAMMING",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_62(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "XXfilter_lengthXX": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_63(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "FILTER_LENGTH": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_64(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "XXphaseXX": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_65(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "PHASE": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_66(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "XXzero-doubleXX",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_67(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "ZERO-DOUBLE",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_68(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = None
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_69(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(None) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_70(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_71(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 5
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_72(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = None
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_73(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "XXmethodXX": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_74(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "METHOD": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_75(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "XXiirXX",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_76(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "IIR",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_77(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "XXiir_paramsXX": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_78(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "IIR_PARAMS": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_79(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"XXorderXX": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_80(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"ORDER": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_81(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "XXftypeXX": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_82(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "FTYPE": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_83(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "XXbutterXX"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_84(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "BUTTER"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_85(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "XXphaseXX": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_86(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "PHASE": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_87(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "XXzero-doubleXX",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_88(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "ZERO-DOUBLE",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_89(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = None
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_90(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        None,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_91(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=None,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_92(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=None,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_93(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=None,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_94(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=None,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_95(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_96(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_97(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_98(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_99(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_100(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_101(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=True,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_102(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples >= 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_103(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 1:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_104(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = None
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_105(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:+pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = filtered_data
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw


def x_apply_bandpass_filter__mutmut_106(
    raw: mne.io.BaseRaw,
    method: str = DEFAULT_FILTER_METHOD,
    freq_band: Tuple[float, float] = (8.0, 40.0),
    order: int | str | None = None,
    pad_duration: float = 0.5,
) -> mne.io.BaseRaw:
    """Apply a padded 8–40 Hz band-pass filter using FIR or IIR designs."""

    # Clone the raw object to avoid mutating caller buffers during filtering
    filtered_raw = raw.copy().load_data()
    # Normalize the method string to simplify downstream comparisons
    normalized_method = method.lower()
    # Enforce supported methods to avoid silent fallbacks inside MNE
    if normalized_method not in {"fir", "iir"}:
        # Raise early to force callers to pick an explicit filter family
        raise ValueError("method must be 'fir' or 'iir'")
    # Extract the sampling frequency to derive padding and design parameters
    sampling_rate = float(filtered_raw.info["sfreq"])
    # Décompose la bande en fréquences basse et haute pour le filtrage
    l_freq, h_freq = freq_band
    # Translate the padding duration into sample counts for symmetrical padding
    pad_samples = max(int(round(pad_duration * sampling_rate)), 0)
    # Fetch the data array once to avoid repeated MNE access overhead
    data = filtered_raw.get_data()
    # Build a reflect-padded buffer to minimize edge artifacts during filtering
    if pad_samples > 0:
        # Use symmetric reflection to keep boundary continuity without phase jumps
        padded_data = np.pad(data, ((0, 0), (pad_samples, pad_samples)), mode="reflect")
    else:
        # Skip padding when the caller explicitly disables it via pad_duration=0.0
        padded_data = data
    # Prepare FIR-specific arguments when a linear-phase design is required
    if normalized_method == "fir":
        # Sélectionne l’ordre FIR explicite ou l’auto-calcul par défaut
        fir_length = order if order is not None else "auto"
        # Configure FIR parameters to balance roll-off and latency for MI bands
        filter_kwargs: Dict[str, Any] = {
            "method": "fir",
            "fir_design": "firwin",
            "fir_window": "hamming",
            "filter_length": fir_length,
            "phase": "zero-double",
        }
    else:
        # Définit l’ordre IIR à 4 par défaut pour limiter la latence
        iir_order = int(order) if order is not None else 4
        # Configure a Butterworth IIR design to minimize latency during streaming
        filter_kwargs = {
            "method": "iir",
            "iir_params": {"order": iir_order, "ftype": "butter"},
            "phase": "zero-double",
        }
    # Apply the selected filter to the padded buffer to obtain band-limited data
    filtered_data = mne.filter.filter_data(
        padded_data,
        sfreq=sampling_rate,
        l_freq=l_freq,
        h_freq=h_freq,
        verbose=False,
        **filter_kwargs,
    )
    # Remove artificial padding to restore the original signal duration
    if pad_samples > 0:
        # Slice out the central segment corresponding to the unpadded recording
        filtered_data = filtered_data[:, pad_samples:-pad_samples]
    # Assign the filtered samples back into the cloned Raw object for return
    filtered_raw._data = None
    # Return the filtered recording to feed downstream epoching and features
    return filtered_raw

x_apply_bandpass_filter__mutmut_mutants : ClassVar[MutantDict] = {
'x_apply_bandpass_filter__mutmut_1': x_apply_bandpass_filter__mutmut_1, 
    'x_apply_bandpass_filter__mutmut_2': x_apply_bandpass_filter__mutmut_2, 
    'x_apply_bandpass_filter__mutmut_3': x_apply_bandpass_filter__mutmut_3, 
    'x_apply_bandpass_filter__mutmut_4': x_apply_bandpass_filter__mutmut_4, 
    'x_apply_bandpass_filter__mutmut_5': x_apply_bandpass_filter__mutmut_5, 
    'x_apply_bandpass_filter__mutmut_6': x_apply_bandpass_filter__mutmut_6, 
    'x_apply_bandpass_filter__mutmut_7': x_apply_bandpass_filter__mutmut_7, 
    'x_apply_bandpass_filter__mutmut_8': x_apply_bandpass_filter__mutmut_8, 
    'x_apply_bandpass_filter__mutmut_9': x_apply_bandpass_filter__mutmut_9, 
    'x_apply_bandpass_filter__mutmut_10': x_apply_bandpass_filter__mutmut_10, 
    'x_apply_bandpass_filter__mutmut_11': x_apply_bandpass_filter__mutmut_11, 
    'x_apply_bandpass_filter__mutmut_12': x_apply_bandpass_filter__mutmut_12, 
    'x_apply_bandpass_filter__mutmut_13': x_apply_bandpass_filter__mutmut_13, 
    'x_apply_bandpass_filter__mutmut_14': x_apply_bandpass_filter__mutmut_14, 
    'x_apply_bandpass_filter__mutmut_15': x_apply_bandpass_filter__mutmut_15, 
    'x_apply_bandpass_filter__mutmut_16': x_apply_bandpass_filter__mutmut_16, 
    'x_apply_bandpass_filter__mutmut_17': x_apply_bandpass_filter__mutmut_17, 
    'x_apply_bandpass_filter__mutmut_18': x_apply_bandpass_filter__mutmut_18, 
    'x_apply_bandpass_filter__mutmut_19': x_apply_bandpass_filter__mutmut_19, 
    'x_apply_bandpass_filter__mutmut_20': x_apply_bandpass_filter__mutmut_20, 
    'x_apply_bandpass_filter__mutmut_21': x_apply_bandpass_filter__mutmut_21, 
    'x_apply_bandpass_filter__mutmut_22': x_apply_bandpass_filter__mutmut_22, 
    'x_apply_bandpass_filter__mutmut_23': x_apply_bandpass_filter__mutmut_23, 
    'x_apply_bandpass_filter__mutmut_24': x_apply_bandpass_filter__mutmut_24, 
    'x_apply_bandpass_filter__mutmut_25': x_apply_bandpass_filter__mutmut_25, 
    'x_apply_bandpass_filter__mutmut_26': x_apply_bandpass_filter__mutmut_26, 
    'x_apply_bandpass_filter__mutmut_27': x_apply_bandpass_filter__mutmut_27, 
    'x_apply_bandpass_filter__mutmut_28': x_apply_bandpass_filter__mutmut_28, 
    'x_apply_bandpass_filter__mutmut_29': x_apply_bandpass_filter__mutmut_29, 
    'x_apply_bandpass_filter__mutmut_30': x_apply_bandpass_filter__mutmut_30, 
    'x_apply_bandpass_filter__mutmut_31': x_apply_bandpass_filter__mutmut_31, 
    'x_apply_bandpass_filter__mutmut_32': x_apply_bandpass_filter__mutmut_32, 
    'x_apply_bandpass_filter__mutmut_33': x_apply_bandpass_filter__mutmut_33, 
    'x_apply_bandpass_filter__mutmut_34': x_apply_bandpass_filter__mutmut_34, 
    'x_apply_bandpass_filter__mutmut_35': x_apply_bandpass_filter__mutmut_35, 
    'x_apply_bandpass_filter__mutmut_36': x_apply_bandpass_filter__mutmut_36, 
    'x_apply_bandpass_filter__mutmut_37': x_apply_bandpass_filter__mutmut_37, 
    'x_apply_bandpass_filter__mutmut_38': x_apply_bandpass_filter__mutmut_38, 
    'x_apply_bandpass_filter__mutmut_39': x_apply_bandpass_filter__mutmut_39, 
    'x_apply_bandpass_filter__mutmut_40': x_apply_bandpass_filter__mutmut_40, 
    'x_apply_bandpass_filter__mutmut_41': x_apply_bandpass_filter__mutmut_41, 
    'x_apply_bandpass_filter__mutmut_42': x_apply_bandpass_filter__mutmut_42, 
    'x_apply_bandpass_filter__mutmut_43': x_apply_bandpass_filter__mutmut_43, 
    'x_apply_bandpass_filter__mutmut_44': x_apply_bandpass_filter__mutmut_44, 
    'x_apply_bandpass_filter__mutmut_45': x_apply_bandpass_filter__mutmut_45, 
    'x_apply_bandpass_filter__mutmut_46': x_apply_bandpass_filter__mutmut_46, 
    'x_apply_bandpass_filter__mutmut_47': x_apply_bandpass_filter__mutmut_47, 
    'x_apply_bandpass_filter__mutmut_48': x_apply_bandpass_filter__mutmut_48, 
    'x_apply_bandpass_filter__mutmut_49': x_apply_bandpass_filter__mutmut_49, 
    'x_apply_bandpass_filter__mutmut_50': x_apply_bandpass_filter__mutmut_50, 
    'x_apply_bandpass_filter__mutmut_51': x_apply_bandpass_filter__mutmut_51, 
    'x_apply_bandpass_filter__mutmut_52': x_apply_bandpass_filter__mutmut_52, 
    'x_apply_bandpass_filter__mutmut_53': x_apply_bandpass_filter__mutmut_53, 
    'x_apply_bandpass_filter__mutmut_54': x_apply_bandpass_filter__mutmut_54, 
    'x_apply_bandpass_filter__mutmut_55': x_apply_bandpass_filter__mutmut_55, 
    'x_apply_bandpass_filter__mutmut_56': x_apply_bandpass_filter__mutmut_56, 
    'x_apply_bandpass_filter__mutmut_57': x_apply_bandpass_filter__mutmut_57, 
    'x_apply_bandpass_filter__mutmut_58': x_apply_bandpass_filter__mutmut_58, 
    'x_apply_bandpass_filter__mutmut_59': x_apply_bandpass_filter__mutmut_59, 
    'x_apply_bandpass_filter__mutmut_60': x_apply_bandpass_filter__mutmut_60, 
    'x_apply_bandpass_filter__mutmut_61': x_apply_bandpass_filter__mutmut_61, 
    'x_apply_bandpass_filter__mutmut_62': x_apply_bandpass_filter__mutmut_62, 
    'x_apply_bandpass_filter__mutmut_63': x_apply_bandpass_filter__mutmut_63, 
    'x_apply_bandpass_filter__mutmut_64': x_apply_bandpass_filter__mutmut_64, 
    'x_apply_bandpass_filter__mutmut_65': x_apply_bandpass_filter__mutmut_65, 
    'x_apply_bandpass_filter__mutmut_66': x_apply_bandpass_filter__mutmut_66, 
    'x_apply_bandpass_filter__mutmut_67': x_apply_bandpass_filter__mutmut_67, 
    'x_apply_bandpass_filter__mutmut_68': x_apply_bandpass_filter__mutmut_68, 
    'x_apply_bandpass_filter__mutmut_69': x_apply_bandpass_filter__mutmut_69, 
    'x_apply_bandpass_filter__mutmut_70': x_apply_bandpass_filter__mutmut_70, 
    'x_apply_bandpass_filter__mutmut_71': x_apply_bandpass_filter__mutmut_71, 
    'x_apply_bandpass_filter__mutmut_72': x_apply_bandpass_filter__mutmut_72, 
    'x_apply_bandpass_filter__mutmut_73': x_apply_bandpass_filter__mutmut_73, 
    'x_apply_bandpass_filter__mutmut_74': x_apply_bandpass_filter__mutmut_74, 
    'x_apply_bandpass_filter__mutmut_75': x_apply_bandpass_filter__mutmut_75, 
    'x_apply_bandpass_filter__mutmut_76': x_apply_bandpass_filter__mutmut_76, 
    'x_apply_bandpass_filter__mutmut_77': x_apply_bandpass_filter__mutmut_77, 
    'x_apply_bandpass_filter__mutmut_78': x_apply_bandpass_filter__mutmut_78, 
    'x_apply_bandpass_filter__mutmut_79': x_apply_bandpass_filter__mutmut_79, 
    'x_apply_bandpass_filter__mutmut_80': x_apply_bandpass_filter__mutmut_80, 
    'x_apply_bandpass_filter__mutmut_81': x_apply_bandpass_filter__mutmut_81, 
    'x_apply_bandpass_filter__mutmut_82': x_apply_bandpass_filter__mutmut_82, 
    'x_apply_bandpass_filter__mutmut_83': x_apply_bandpass_filter__mutmut_83, 
    'x_apply_bandpass_filter__mutmut_84': x_apply_bandpass_filter__mutmut_84, 
    'x_apply_bandpass_filter__mutmut_85': x_apply_bandpass_filter__mutmut_85, 
    'x_apply_bandpass_filter__mutmut_86': x_apply_bandpass_filter__mutmut_86, 
    'x_apply_bandpass_filter__mutmut_87': x_apply_bandpass_filter__mutmut_87, 
    'x_apply_bandpass_filter__mutmut_88': x_apply_bandpass_filter__mutmut_88, 
    'x_apply_bandpass_filter__mutmut_89': x_apply_bandpass_filter__mutmut_89, 
    'x_apply_bandpass_filter__mutmut_90': x_apply_bandpass_filter__mutmut_90, 
    'x_apply_bandpass_filter__mutmut_91': x_apply_bandpass_filter__mutmut_91, 
    'x_apply_bandpass_filter__mutmut_92': x_apply_bandpass_filter__mutmut_92, 
    'x_apply_bandpass_filter__mutmut_93': x_apply_bandpass_filter__mutmut_93, 
    'x_apply_bandpass_filter__mutmut_94': x_apply_bandpass_filter__mutmut_94, 
    'x_apply_bandpass_filter__mutmut_95': x_apply_bandpass_filter__mutmut_95, 
    'x_apply_bandpass_filter__mutmut_96': x_apply_bandpass_filter__mutmut_96, 
    'x_apply_bandpass_filter__mutmut_97': x_apply_bandpass_filter__mutmut_97, 
    'x_apply_bandpass_filter__mutmut_98': x_apply_bandpass_filter__mutmut_98, 
    'x_apply_bandpass_filter__mutmut_99': x_apply_bandpass_filter__mutmut_99, 
    'x_apply_bandpass_filter__mutmut_100': x_apply_bandpass_filter__mutmut_100, 
    'x_apply_bandpass_filter__mutmut_101': x_apply_bandpass_filter__mutmut_101, 
    'x_apply_bandpass_filter__mutmut_102': x_apply_bandpass_filter__mutmut_102, 
    'x_apply_bandpass_filter__mutmut_103': x_apply_bandpass_filter__mutmut_103, 
    'x_apply_bandpass_filter__mutmut_104': x_apply_bandpass_filter__mutmut_104, 
    'x_apply_bandpass_filter__mutmut_105': x_apply_bandpass_filter__mutmut_105, 
    'x_apply_bandpass_filter__mutmut_106': x_apply_bandpass_filter__mutmut_106
}

def apply_bandpass_filter(*args, **kwargs):
    result = _mutmut_trampoline(x_apply_bandpass_filter__mutmut_orig, x_apply_bandpass_filter__mutmut_mutants, args, kwargs)
    return result 

apply_bandpass_filter.__signature__ = _mutmut_signature(x_apply_bandpass_filter__mutmut_orig)
x_apply_bandpass_filter__mutmut_orig.__name__ = 'x_apply_bandpass_filter'


def x__is_bad_description__mutmut_orig(description: str) -> bool:
    """Return True when an annotation description denotes a BAD marker."""

    # Normalize the description to uppercase for case-insensitive detection
    normalized_description = description.upper()
    # Identify MNE BAD-prefixed annotations regardless of original casing
    return normalized_description.startswith("BAD")


def x__is_bad_description__mutmut_1(description: str) -> bool:
    """Return True when an annotation description denotes a BAD marker."""

    # Normalize the description to uppercase for case-insensitive detection
    normalized_description = None
    # Identify MNE BAD-prefixed annotations regardless of original casing
    return normalized_description.startswith("BAD")


def x__is_bad_description__mutmut_2(description: str) -> bool:
    """Return True when an annotation description denotes a BAD marker."""

    # Normalize the description to uppercase for case-insensitive detection
    normalized_description = description.lower()
    # Identify MNE BAD-prefixed annotations regardless of original casing
    return normalized_description.startswith("BAD")


def x__is_bad_description__mutmut_3(description: str) -> bool:
    """Return True when an annotation description denotes a BAD marker."""

    # Normalize the description to uppercase for case-insensitive detection
    normalized_description = description.upper()
    # Identify MNE BAD-prefixed annotations regardless of original casing
    return normalized_description.startswith(None)


def x__is_bad_description__mutmut_4(description: str) -> bool:
    """Return True when an annotation description denotes a BAD marker."""

    # Normalize the description to uppercase for case-insensitive detection
    normalized_description = description.upper()
    # Identify MNE BAD-prefixed annotations regardless of original casing
    return normalized_description.startswith("XXBADXX")


def x__is_bad_description__mutmut_5(description: str) -> bool:
    """Return True when an annotation description denotes a BAD marker."""

    # Normalize the description to uppercase for case-insensitive detection
    normalized_description = description.upper()
    # Identify MNE BAD-prefixed annotations regardless of original casing
    return normalized_description.startswith("bad")

x__is_bad_description__mutmut_mutants : ClassVar[MutantDict] = {
'x__is_bad_description__mutmut_1': x__is_bad_description__mutmut_1, 
    'x__is_bad_description__mutmut_2': x__is_bad_description__mutmut_2, 
    'x__is_bad_description__mutmut_3': x__is_bad_description__mutmut_3, 
    'x__is_bad_description__mutmut_4': x__is_bad_description__mutmut_4, 
    'x__is_bad_description__mutmut_5': x__is_bad_description__mutmut_5
}

def _is_bad_description(*args, **kwargs):
    result = _mutmut_trampoline(x__is_bad_description__mutmut_orig, x__is_bad_description__mutmut_mutants, args, kwargs)
    return result 

_is_bad_description.__signature__ = _mutmut_signature(x__is_bad_description__mutmut_orig)
x__is_bad_description__mutmut_orig.__name__ = 'x__is_bad_description'


def x_load_mne_raw_checked__mutmut_orig(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_1(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = None
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_2(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(None).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_3(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = None
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_4(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.upper()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_5(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_6(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {"XX.edfXX", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_7(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".EDF", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_8(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", "XX.bdfXX"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_9(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".BDF"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_10(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            None
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_11(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                None
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_12(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_13(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_14(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "XXUnsupported file formatXX",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_15(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_16(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "UNSUPPORTED FILE FORMAT",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_17(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "XXpathXX": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_18(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "PATH": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_19(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(None),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_20(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "XXsuffixXX": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_21(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "SUFFIX": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_22(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = None
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_23(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(None, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_24(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=None, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_25(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=None)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_26(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_27(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_28(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, )
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_29(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=False, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_30(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=True)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_31(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = None
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_32(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(None)
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_33(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["XXsfreqXX"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_34(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["SFREQ"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_35(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_36(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(None, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_37(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, None):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_38(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_39(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, ):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_40(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            None
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_41(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = None
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_42(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(None)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_43(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = None
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_44(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(None)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_45(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = None
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_46(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(None)
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_47(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) + set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_48(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(None) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_49(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(None))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_50(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = None
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_51(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(None)
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_52(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) + set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_53(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(None) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_54(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(None))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_55(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            None
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_56(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                None
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_57(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_58(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_59(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "XXChannel mismatchXX",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_60(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_61(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "CHANNEL MISMATCH",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_62(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "XXextraXX": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_63(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "EXTRA": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_64(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "XXmissingXX": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_65(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "MISSING": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_66(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(None, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_67(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing=None)
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_68(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_69(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, )
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_70(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="XXwarnXX")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_71(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="WARN")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_72(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = None
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_73(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is not None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_74(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(None)
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_75(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = None
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_76(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(None)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_77(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = None
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_78(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(None)
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_79(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels + set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_80(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(None))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_81(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = None
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_82(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(None)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_83(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) + montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_84(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(None) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_85(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            None
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_86(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                None
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_87(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_88(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_89(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "XXMontage missing expected channelsXX",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_90(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_91(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "MONTAGE MISSING EXPECTED CHANNELS",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_92(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "XXmissing_channelsXX": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_93(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "MISSING_CHANNELS": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_94(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "XXextraXX": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_95(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "EXTRA": extra_montage_channels,
                    "montage": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_96(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "XXmontageXX": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw


def x_load_mne_raw_checked__mutmut_97(
    file_path: Path,
    expected_montage: str,
    expected_sampling_rate: float,
    expected_channels: List[str],
) -> mne.io.BaseRaw:
    """Load a raw MNE file and validate montage, sampling rate, and channels."""

    # Normalize the file path to avoid surprises from relative inputs
    normalized_path = Path(file_path).expanduser().resolve()
    # Capture the suffix to enforce EDF/BDF compatibility explicitly
    file_suffix = normalized_path.suffix.lower()
    # Reject unsupported formats early to avoid ambiguous MNE errors
    if file_suffix not in {".edf", ".bdf"}:
        # Raise a clear error when the extension does not match EDF/BDF
        raise ValueError(
            json.dumps(
                {
                    "error": "Unsupported file format",
                    "path": str(normalized_path),
                    "suffix": file_suffix,
                }
            )
        )
    # Load the raw file with preload enabled for immediate validation
    raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Extract the sampling frequency reported by the recording
    sampling_rate = float(raw.info["sfreq"])
    # Validate the sampling frequency against the expected configuration
    if not np.isclose(sampling_rate, expected_sampling_rate):
        # Raise a descriptive error when the sampling rate deviates
        raise ValueError(
            f"Expected sampling rate {expected_sampling_rate}Hz "
            f"but got {sampling_rate}Hz"
        )
    # Renomme les canaux bruts pour les aligner sur le montage standard
    raw = _rename_channels_for_montage(raw)
    # Gather channel names from the recording for consistency checks
    channel_names = list(raw.ch_names)
    # Identify unexpected channels that would break downstream spatial filters
    extra_channels = sorted(set(channel_names) - set(expected_channels))
    # Identify missing channels that would prevent feature extraction
    missing_channels = sorted(set(expected_channels) - set(channel_names))
    # Raise early when unexpected channels would trigger montage warnings
    if extra_channels:
        # Compose a readable error describing both types of discrepancies
        raise ValueError(
            json.dumps(
                {
                    "error": "Channel mismatch",
                    "extra": extra_channels,
                    "missing": missing_channels,
                }
            )
        )
    # Apply the montage to ensure spatial layout matches expectations
    raw.set_montage(expected_montage, on_missing="warn")
    # Retrieve the effective montage to confirm it has been attached
    montage = raw.get_montage()
    # Fail loudly when the montage could not be established on the recording
    if montage is None:
        # Raise a clear error describing the missing montage configuration
        raise ValueError(f"Montage '{expected_montage}' could not be applied")
    # Capture montage channel names to compare against expected layout
    montage_channels = set(montage.ch_names)
    # Capture surplus montage channels to document unexpected electrodes
    extra_montage_channels = sorted(montage_channels - set(expected_channels))
    # Identify montage omissions that would break 10–20 assumptions
    missing_montage_channels = sorted(set(expected_channels) - montage_channels)
    # Raise explicit error when the montage lacks required 10–20 electrodes
    if missing_montage_channels:
        # Include missing channels in a structured report for debugging
        raise ValueError(
            json.dumps(
                {
                    "error": "Montage missing expected channels",
                    "missing_channels": missing_montage_channels,
                    "extra": extra_montage_channels,
                    "MONTAGE": expected_montage,
                }
            )
        )
    # Return the validated raw object for downstream preprocessing steps
    return raw

x_load_mne_raw_checked__mutmut_mutants : ClassVar[MutantDict] = {
'x_load_mne_raw_checked__mutmut_1': x_load_mne_raw_checked__mutmut_1, 
    'x_load_mne_raw_checked__mutmut_2': x_load_mne_raw_checked__mutmut_2, 
    'x_load_mne_raw_checked__mutmut_3': x_load_mne_raw_checked__mutmut_3, 
    'x_load_mne_raw_checked__mutmut_4': x_load_mne_raw_checked__mutmut_4, 
    'x_load_mne_raw_checked__mutmut_5': x_load_mne_raw_checked__mutmut_5, 
    'x_load_mne_raw_checked__mutmut_6': x_load_mne_raw_checked__mutmut_6, 
    'x_load_mne_raw_checked__mutmut_7': x_load_mne_raw_checked__mutmut_7, 
    'x_load_mne_raw_checked__mutmut_8': x_load_mne_raw_checked__mutmut_8, 
    'x_load_mne_raw_checked__mutmut_9': x_load_mne_raw_checked__mutmut_9, 
    'x_load_mne_raw_checked__mutmut_10': x_load_mne_raw_checked__mutmut_10, 
    'x_load_mne_raw_checked__mutmut_11': x_load_mne_raw_checked__mutmut_11, 
    'x_load_mne_raw_checked__mutmut_12': x_load_mne_raw_checked__mutmut_12, 
    'x_load_mne_raw_checked__mutmut_13': x_load_mne_raw_checked__mutmut_13, 
    'x_load_mne_raw_checked__mutmut_14': x_load_mne_raw_checked__mutmut_14, 
    'x_load_mne_raw_checked__mutmut_15': x_load_mne_raw_checked__mutmut_15, 
    'x_load_mne_raw_checked__mutmut_16': x_load_mne_raw_checked__mutmut_16, 
    'x_load_mne_raw_checked__mutmut_17': x_load_mne_raw_checked__mutmut_17, 
    'x_load_mne_raw_checked__mutmut_18': x_load_mne_raw_checked__mutmut_18, 
    'x_load_mne_raw_checked__mutmut_19': x_load_mne_raw_checked__mutmut_19, 
    'x_load_mne_raw_checked__mutmut_20': x_load_mne_raw_checked__mutmut_20, 
    'x_load_mne_raw_checked__mutmut_21': x_load_mne_raw_checked__mutmut_21, 
    'x_load_mne_raw_checked__mutmut_22': x_load_mne_raw_checked__mutmut_22, 
    'x_load_mne_raw_checked__mutmut_23': x_load_mne_raw_checked__mutmut_23, 
    'x_load_mne_raw_checked__mutmut_24': x_load_mne_raw_checked__mutmut_24, 
    'x_load_mne_raw_checked__mutmut_25': x_load_mne_raw_checked__mutmut_25, 
    'x_load_mne_raw_checked__mutmut_26': x_load_mne_raw_checked__mutmut_26, 
    'x_load_mne_raw_checked__mutmut_27': x_load_mne_raw_checked__mutmut_27, 
    'x_load_mne_raw_checked__mutmut_28': x_load_mne_raw_checked__mutmut_28, 
    'x_load_mne_raw_checked__mutmut_29': x_load_mne_raw_checked__mutmut_29, 
    'x_load_mne_raw_checked__mutmut_30': x_load_mne_raw_checked__mutmut_30, 
    'x_load_mne_raw_checked__mutmut_31': x_load_mne_raw_checked__mutmut_31, 
    'x_load_mne_raw_checked__mutmut_32': x_load_mne_raw_checked__mutmut_32, 
    'x_load_mne_raw_checked__mutmut_33': x_load_mne_raw_checked__mutmut_33, 
    'x_load_mne_raw_checked__mutmut_34': x_load_mne_raw_checked__mutmut_34, 
    'x_load_mne_raw_checked__mutmut_35': x_load_mne_raw_checked__mutmut_35, 
    'x_load_mne_raw_checked__mutmut_36': x_load_mne_raw_checked__mutmut_36, 
    'x_load_mne_raw_checked__mutmut_37': x_load_mne_raw_checked__mutmut_37, 
    'x_load_mne_raw_checked__mutmut_38': x_load_mne_raw_checked__mutmut_38, 
    'x_load_mne_raw_checked__mutmut_39': x_load_mne_raw_checked__mutmut_39, 
    'x_load_mne_raw_checked__mutmut_40': x_load_mne_raw_checked__mutmut_40, 
    'x_load_mne_raw_checked__mutmut_41': x_load_mne_raw_checked__mutmut_41, 
    'x_load_mne_raw_checked__mutmut_42': x_load_mne_raw_checked__mutmut_42, 
    'x_load_mne_raw_checked__mutmut_43': x_load_mne_raw_checked__mutmut_43, 
    'x_load_mne_raw_checked__mutmut_44': x_load_mne_raw_checked__mutmut_44, 
    'x_load_mne_raw_checked__mutmut_45': x_load_mne_raw_checked__mutmut_45, 
    'x_load_mne_raw_checked__mutmut_46': x_load_mne_raw_checked__mutmut_46, 
    'x_load_mne_raw_checked__mutmut_47': x_load_mne_raw_checked__mutmut_47, 
    'x_load_mne_raw_checked__mutmut_48': x_load_mne_raw_checked__mutmut_48, 
    'x_load_mne_raw_checked__mutmut_49': x_load_mne_raw_checked__mutmut_49, 
    'x_load_mne_raw_checked__mutmut_50': x_load_mne_raw_checked__mutmut_50, 
    'x_load_mne_raw_checked__mutmut_51': x_load_mne_raw_checked__mutmut_51, 
    'x_load_mne_raw_checked__mutmut_52': x_load_mne_raw_checked__mutmut_52, 
    'x_load_mne_raw_checked__mutmut_53': x_load_mne_raw_checked__mutmut_53, 
    'x_load_mne_raw_checked__mutmut_54': x_load_mne_raw_checked__mutmut_54, 
    'x_load_mne_raw_checked__mutmut_55': x_load_mne_raw_checked__mutmut_55, 
    'x_load_mne_raw_checked__mutmut_56': x_load_mne_raw_checked__mutmut_56, 
    'x_load_mne_raw_checked__mutmut_57': x_load_mne_raw_checked__mutmut_57, 
    'x_load_mne_raw_checked__mutmut_58': x_load_mne_raw_checked__mutmut_58, 
    'x_load_mne_raw_checked__mutmut_59': x_load_mne_raw_checked__mutmut_59, 
    'x_load_mne_raw_checked__mutmut_60': x_load_mne_raw_checked__mutmut_60, 
    'x_load_mne_raw_checked__mutmut_61': x_load_mne_raw_checked__mutmut_61, 
    'x_load_mne_raw_checked__mutmut_62': x_load_mne_raw_checked__mutmut_62, 
    'x_load_mne_raw_checked__mutmut_63': x_load_mne_raw_checked__mutmut_63, 
    'x_load_mne_raw_checked__mutmut_64': x_load_mne_raw_checked__mutmut_64, 
    'x_load_mne_raw_checked__mutmut_65': x_load_mne_raw_checked__mutmut_65, 
    'x_load_mne_raw_checked__mutmut_66': x_load_mne_raw_checked__mutmut_66, 
    'x_load_mne_raw_checked__mutmut_67': x_load_mne_raw_checked__mutmut_67, 
    'x_load_mne_raw_checked__mutmut_68': x_load_mne_raw_checked__mutmut_68, 
    'x_load_mne_raw_checked__mutmut_69': x_load_mne_raw_checked__mutmut_69, 
    'x_load_mne_raw_checked__mutmut_70': x_load_mne_raw_checked__mutmut_70, 
    'x_load_mne_raw_checked__mutmut_71': x_load_mne_raw_checked__mutmut_71, 
    'x_load_mne_raw_checked__mutmut_72': x_load_mne_raw_checked__mutmut_72, 
    'x_load_mne_raw_checked__mutmut_73': x_load_mne_raw_checked__mutmut_73, 
    'x_load_mne_raw_checked__mutmut_74': x_load_mne_raw_checked__mutmut_74, 
    'x_load_mne_raw_checked__mutmut_75': x_load_mne_raw_checked__mutmut_75, 
    'x_load_mne_raw_checked__mutmut_76': x_load_mne_raw_checked__mutmut_76, 
    'x_load_mne_raw_checked__mutmut_77': x_load_mne_raw_checked__mutmut_77, 
    'x_load_mne_raw_checked__mutmut_78': x_load_mne_raw_checked__mutmut_78, 
    'x_load_mne_raw_checked__mutmut_79': x_load_mne_raw_checked__mutmut_79, 
    'x_load_mne_raw_checked__mutmut_80': x_load_mne_raw_checked__mutmut_80, 
    'x_load_mne_raw_checked__mutmut_81': x_load_mne_raw_checked__mutmut_81, 
    'x_load_mne_raw_checked__mutmut_82': x_load_mne_raw_checked__mutmut_82, 
    'x_load_mne_raw_checked__mutmut_83': x_load_mne_raw_checked__mutmut_83, 
    'x_load_mne_raw_checked__mutmut_84': x_load_mne_raw_checked__mutmut_84, 
    'x_load_mne_raw_checked__mutmut_85': x_load_mne_raw_checked__mutmut_85, 
    'x_load_mne_raw_checked__mutmut_86': x_load_mne_raw_checked__mutmut_86, 
    'x_load_mne_raw_checked__mutmut_87': x_load_mne_raw_checked__mutmut_87, 
    'x_load_mne_raw_checked__mutmut_88': x_load_mne_raw_checked__mutmut_88, 
    'x_load_mne_raw_checked__mutmut_89': x_load_mne_raw_checked__mutmut_89, 
    'x_load_mne_raw_checked__mutmut_90': x_load_mne_raw_checked__mutmut_90, 
    'x_load_mne_raw_checked__mutmut_91': x_load_mne_raw_checked__mutmut_91, 
    'x_load_mne_raw_checked__mutmut_92': x_load_mne_raw_checked__mutmut_92, 
    'x_load_mne_raw_checked__mutmut_93': x_load_mne_raw_checked__mutmut_93, 
    'x_load_mne_raw_checked__mutmut_94': x_load_mne_raw_checked__mutmut_94, 
    'x_load_mne_raw_checked__mutmut_95': x_load_mne_raw_checked__mutmut_95, 
    'x_load_mne_raw_checked__mutmut_96': x_load_mne_raw_checked__mutmut_96, 
    'x_load_mne_raw_checked__mutmut_97': x_load_mne_raw_checked__mutmut_97
}

def load_mne_raw_checked(*args, **kwargs):
    result = _mutmut_trampoline(x_load_mne_raw_checked__mutmut_orig, x_load_mne_raw_checked__mutmut_mutants, args, kwargs)
    return result 

load_mne_raw_checked.__signature__ = _mutmut_signature(x_load_mne_raw_checked__mutmut_orig)
x_load_mne_raw_checked__mutmut_orig.__name__ = 'x_load_mne_raw_checked'


def x_load_mne_motor_run__mutmut_orig(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_1(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "XXstandard_1020XX",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_2(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "STANDARD_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_3(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = None
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_4(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        None,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_5(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=None,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_6(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=None,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_7(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=None,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_8(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_9(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_10(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_11(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(raw)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_12(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = None
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels


def x_load_mne_motor_run__mutmut_13(
    file_path: Path,
    expected_sampling_rate: float,
    expected_channels: List[str],
    expected_montage: str = "standard_1020",
) -> Tuple[mne.io.BaseRaw, np.ndarray, Dict[str, int], List[str]]:
    """Load an EDF/BDF run and expose motor labels A/B."""

    # Vérifie et charge le fichier en imposant montage et fréquence attendus
    raw = load_mne_raw_checked(
        file_path,
        expected_montage=expected_montage,
        expected_sampling_rate=expected_sampling_rate,
        expected_channels=expected_channels,
    )
    # Mappe les événements vers des étiquettes motrices compatibles A/B
    events, event_id, motor_labels = map_events_to_motor_labels(None)
    # Retourne l'enregistrement et les structures nécessaires au découpage
    return raw, events, event_id, motor_labels

x_load_mne_motor_run__mutmut_mutants : ClassVar[MutantDict] = {
'x_load_mne_motor_run__mutmut_1': x_load_mne_motor_run__mutmut_1, 
    'x_load_mne_motor_run__mutmut_2': x_load_mne_motor_run__mutmut_2, 
    'x_load_mne_motor_run__mutmut_3': x_load_mne_motor_run__mutmut_3, 
    'x_load_mne_motor_run__mutmut_4': x_load_mne_motor_run__mutmut_4, 
    'x_load_mne_motor_run__mutmut_5': x_load_mne_motor_run__mutmut_5, 
    'x_load_mne_motor_run__mutmut_6': x_load_mne_motor_run__mutmut_6, 
    'x_load_mne_motor_run__mutmut_7': x_load_mne_motor_run__mutmut_7, 
    'x_load_mne_motor_run__mutmut_8': x_load_mne_motor_run__mutmut_8, 
    'x_load_mne_motor_run__mutmut_9': x_load_mne_motor_run__mutmut_9, 
    'x_load_mne_motor_run__mutmut_10': x_load_mne_motor_run__mutmut_10, 
    'x_load_mne_motor_run__mutmut_11': x_load_mne_motor_run__mutmut_11, 
    'x_load_mne_motor_run__mutmut_12': x_load_mne_motor_run__mutmut_12, 
    'x_load_mne_motor_run__mutmut_13': x_load_mne_motor_run__mutmut_13
}

def load_mne_motor_run(*args, **kwargs):
    result = _mutmut_trampoline(x_load_mne_motor_run__mutmut_orig, x_load_mne_motor_run__mutmut_mutants, args, kwargs)
    return result 

load_mne_motor_run.__signature__ = _mutmut_signature(x_load_mne_motor_run__mutmut_orig)
x_load_mne_motor_run__mutmut_orig.__name__ = 'x_load_mne_motor_run'


def x_load_physionet_raw__mutmut_orig(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_1(
    file_path: Path, montage: str = "XXstandard_1020XX"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_2(
    file_path: Path, montage: str = "STANDARD_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_3(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = None
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_4(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(None).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_5(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            None,
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_6(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message=None,
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_7(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=None,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_8(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module=None,
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_9(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_10(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_11(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_12(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_13(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "XXignoreXX",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_14(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "IGNORE",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_15(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="XXLimited .*annotation.*outside the data rangeXX",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_16(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_17(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="LIMITED .*ANNOTATION.*OUTSIDE THE DATA RANGE",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_18(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="XXmneXX",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_19(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="MNE",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_20(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = None
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_21(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(None, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_22(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=None, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_23(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=None)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_24(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_25(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_26(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, )
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_27(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=False, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_28(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=True)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_29(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = None
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_30(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(None)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_31(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(None, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_32(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing=None)
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_33(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_34(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, )
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_35(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="XXwarnXX")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_36(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="WARN")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_37(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = None
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_38(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(None)
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_39(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["XXsfreqXX"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_40(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["SFREQ"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_41(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = None
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_42(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(None)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_43(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = None
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_44(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = None
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_45(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "XXsampling_rateXX": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_46(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "SAMPLING_RATE": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_47(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "XXchannel_namesXX": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_48(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "CHANNEL_NAMES": channel_names,
        "montage": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_49(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "XXmontageXX": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_50(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "MONTAGE": montage_name,
        "path": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_51(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "XXpathXX": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_52(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "PATH": str(normalized_path),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata


def x_load_physionet_raw__mutmut_53(
    file_path: Path, montage: str = "standard_1020"
) -> Tuple[mne.io.BaseRaw, Dict[str, object]]:
    """Load an EDF/BDF Physionet file with metadata."""

    # Resolve the input path to avoid surprises with relative locations
    normalized_path = Path(file_path).expanduser().resolve()
    # Encadre la lecture pour ignorer l'avertissement sur les annotations tronquées
    with warnings.catch_warnings():
        # Filtre l'avertissement MNE lorsque la durée dépasse la trace
        warnings.filterwarnings(
            "ignore",
            message="Limited .*annotation.*outside the data range",
            category=RuntimeWarning,
            module="mne",
        )
        # Load the recording with preload to enable immediate validation steps
        raw = mne.io.read_raw_edf(normalized_path, preload=True, verbose=False)
    # Renomme les canaux pour les aligner sur le montage 10-20 utilisé
    raw = _rename_channels_for_montage(raw)
    # Attach the montage so downstream spatial filters assume 10-20 layout
    raw.set_montage(montage, on_missing="warn")
    # Extract sampling rate to guide later filtering and epoch durations
    sampling_rate = float(raw.info["sfreq"])
    # Capture channel names to expose them to downstream feature builders
    channel_names = list(raw.ch_names)
    # Record the montage name for traceability in integrity reports
    montage_name = montage
    # Bundle metadata for callers that need reproducible preprocessing config
    metadata = {
        "sampling_rate": sampling_rate,
        "channel_names": channel_names,
        "montage": montage_name,
        "path": str(None),
    }
    # Return both signal and metadata to keep the loader side-effect free
    return raw, metadata

x_load_physionet_raw__mutmut_mutants : ClassVar[MutantDict] = {
'x_load_physionet_raw__mutmut_1': x_load_physionet_raw__mutmut_1, 
    'x_load_physionet_raw__mutmut_2': x_load_physionet_raw__mutmut_2, 
    'x_load_physionet_raw__mutmut_3': x_load_physionet_raw__mutmut_3, 
    'x_load_physionet_raw__mutmut_4': x_load_physionet_raw__mutmut_4, 
    'x_load_physionet_raw__mutmut_5': x_load_physionet_raw__mutmut_5, 
    'x_load_physionet_raw__mutmut_6': x_load_physionet_raw__mutmut_6, 
    'x_load_physionet_raw__mutmut_7': x_load_physionet_raw__mutmut_7, 
    'x_load_physionet_raw__mutmut_8': x_load_physionet_raw__mutmut_8, 
    'x_load_physionet_raw__mutmut_9': x_load_physionet_raw__mutmut_9, 
    'x_load_physionet_raw__mutmut_10': x_load_physionet_raw__mutmut_10, 
    'x_load_physionet_raw__mutmut_11': x_load_physionet_raw__mutmut_11, 
    'x_load_physionet_raw__mutmut_12': x_load_physionet_raw__mutmut_12, 
    'x_load_physionet_raw__mutmut_13': x_load_physionet_raw__mutmut_13, 
    'x_load_physionet_raw__mutmut_14': x_load_physionet_raw__mutmut_14, 
    'x_load_physionet_raw__mutmut_15': x_load_physionet_raw__mutmut_15, 
    'x_load_physionet_raw__mutmut_16': x_load_physionet_raw__mutmut_16, 
    'x_load_physionet_raw__mutmut_17': x_load_physionet_raw__mutmut_17, 
    'x_load_physionet_raw__mutmut_18': x_load_physionet_raw__mutmut_18, 
    'x_load_physionet_raw__mutmut_19': x_load_physionet_raw__mutmut_19, 
    'x_load_physionet_raw__mutmut_20': x_load_physionet_raw__mutmut_20, 
    'x_load_physionet_raw__mutmut_21': x_load_physionet_raw__mutmut_21, 
    'x_load_physionet_raw__mutmut_22': x_load_physionet_raw__mutmut_22, 
    'x_load_physionet_raw__mutmut_23': x_load_physionet_raw__mutmut_23, 
    'x_load_physionet_raw__mutmut_24': x_load_physionet_raw__mutmut_24, 
    'x_load_physionet_raw__mutmut_25': x_load_physionet_raw__mutmut_25, 
    'x_load_physionet_raw__mutmut_26': x_load_physionet_raw__mutmut_26, 
    'x_load_physionet_raw__mutmut_27': x_load_physionet_raw__mutmut_27, 
    'x_load_physionet_raw__mutmut_28': x_load_physionet_raw__mutmut_28, 
    'x_load_physionet_raw__mutmut_29': x_load_physionet_raw__mutmut_29, 
    'x_load_physionet_raw__mutmut_30': x_load_physionet_raw__mutmut_30, 
    'x_load_physionet_raw__mutmut_31': x_load_physionet_raw__mutmut_31, 
    'x_load_physionet_raw__mutmut_32': x_load_physionet_raw__mutmut_32, 
    'x_load_physionet_raw__mutmut_33': x_load_physionet_raw__mutmut_33, 
    'x_load_physionet_raw__mutmut_34': x_load_physionet_raw__mutmut_34, 
    'x_load_physionet_raw__mutmut_35': x_load_physionet_raw__mutmut_35, 
    'x_load_physionet_raw__mutmut_36': x_load_physionet_raw__mutmut_36, 
    'x_load_physionet_raw__mutmut_37': x_load_physionet_raw__mutmut_37, 
    'x_load_physionet_raw__mutmut_38': x_load_physionet_raw__mutmut_38, 
    'x_load_physionet_raw__mutmut_39': x_load_physionet_raw__mutmut_39, 
    'x_load_physionet_raw__mutmut_40': x_load_physionet_raw__mutmut_40, 
    'x_load_physionet_raw__mutmut_41': x_load_physionet_raw__mutmut_41, 
    'x_load_physionet_raw__mutmut_42': x_load_physionet_raw__mutmut_42, 
    'x_load_physionet_raw__mutmut_43': x_load_physionet_raw__mutmut_43, 
    'x_load_physionet_raw__mutmut_44': x_load_physionet_raw__mutmut_44, 
    'x_load_physionet_raw__mutmut_45': x_load_physionet_raw__mutmut_45, 
    'x_load_physionet_raw__mutmut_46': x_load_physionet_raw__mutmut_46, 
    'x_load_physionet_raw__mutmut_47': x_load_physionet_raw__mutmut_47, 
    'x_load_physionet_raw__mutmut_48': x_load_physionet_raw__mutmut_48, 
    'x_load_physionet_raw__mutmut_49': x_load_physionet_raw__mutmut_49, 
    'x_load_physionet_raw__mutmut_50': x_load_physionet_raw__mutmut_50, 
    'x_load_physionet_raw__mutmut_51': x_load_physionet_raw__mutmut_51, 
    'x_load_physionet_raw__mutmut_52': x_load_physionet_raw__mutmut_52, 
    'x_load_physionet_raw__mutmut_53': x_load_physionet_raw__mutmut_53
}

def load_physionet_raw(*args, **kwargs):
    result = _mutmut_trampoline(x_load_physionet_raw__mutmut_orig, x_load_physionet_raw__mutmut_mutants, args, kwargs)
    return result 

load_physionet_raw.__signature__ = _mutmut_signature(x_load_physionet_raw__mutmut_orig)
x_load_physionet_raw__mutmut_orig.__name__ = 'x_load_physionet_raw'


def x__extract_bad_intervals__mutmut_orig(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_1(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = None
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_2(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        None,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_3(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        None,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_4(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        None,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_5(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=None,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_6(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_7(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_8(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_9(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_10(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=False,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_11(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_12(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(None):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_13(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            break
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_14(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append(None)
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_15(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(None), float(onset + duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_16(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(None)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals


def x__extract_bad_intervals__mutmut_17(raw: mne.io.BaseRaw) -> List[Tuple[float, float]]:
    """Return BAD annotation windows as (start, end) times in seconds."""

    # Start with an empty list to accumulate invalid windows
    bad_intervals: List[Tuple[float, float]] = []
    # Iterate annotations to translate BAD markers into explicit intervals
    for onset, duration, desc in zip(
        raw.annotations.onset,
        raw.annotations.duration,
        raw.annotations.description,
        strict=True,
    ):
        # Skip annotations not flagged BAD to avoid overzealous filtering
        if not _is_bad_description(desc):
            # Continue looping when the annotation is not an invalid segment
            continue
        # Append the interval boundaries to help later event rejection
        bad_intervals.append((float(onset), float(onset - duration)))
    # Return all invalid windows for consumers that exclude unsafe events
    return bad_intervals

x__extract_bad_intervals__mutmut_mutants : ClassVar[MutantDict] = {
'x__extract_bad_intervals__mutmut_1': x__extract_bad_intervals__mutmut_1, 
    'x__extract_bad_intervals__mutmut_2': x__extract_bad_intervals__mutmut_2, 
    'x__extract_bad_intervals__mutmut_3': x__extract_bad_intervals__mutmut_3, 
    'x__extract_bad_intervals__mutmut_4': x__extract_bad_intervals__mutmut_4, 
    'x__extract_bad_intervals__mutmut_5': x__extract_bad_intervals__mutmut_5, 
    'x__extract_bad_intervals__mutmut_6': x__extract_bad_intervals__mutmut_6, 
    'x__extract_bad_intervals__mutmut_7': x__extract_bad_intervals__mutmut_7, 
    'x__extract_bad_intervals__mutmut_8': x__extract_bad_intervals__mutmut_8, 
    'x__extract_bad_intervals__mutmut_9': x__extract_bad_intervals__mutmut_9, 
    'x__extract_bad_intervals__mutmut_10': x__extract_bad_intervals__mutmut_10, 
    'x__extract_bad_intervals__mutmut_11': x__extract_bad_intervals__mutmut_11, 
    'x__extract_bad_intervals__mutmut_12': x__extract_bad_intervals__mutmut_12, 
    'x__extract_bad_intervals__mutmut_13': x__extract_bad_intervals__mutmut_13, 
    'x__extract_bad_intervals__mutmut_14': x__extract_bad_intervals__mutmut_14, 
    'x__extract_bad_intervals__mutmut_15': x__extract_bad_intervals__mutmut_15, 
    'x__extract_bad_intervals__mutmut_16': x__extract_bad_intervals__mutmut_16, 
    'x__extract_bad_intervals__mutmut_17': x__extract_bad_intervals__mutmut_17
}

def _extract_bad_intervals(*args, **kwargs):
    result = _mutmut_trampoline(x__extract_bad_intervals__mutmut_orig, x__extract_bad_intervals__mutmut_mutants, args, kwargs)
    return result 

_extract_bad_intervals.__signature__ = _mutmut_signature(x__extract_bad_intervals__mutmut_orig)
x__extract_bad_intervals__mutmut_orig.__name__ = 'x__extract_bad_intervals'


def x_map_events_and_validate__mutmut_orig(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_1(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = None
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_2(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(None) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_3(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_4(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(None)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_5(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(None, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_6(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, None)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_7(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_8(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, )
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_9(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = None
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_10(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        None
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_11(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc not in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_12(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_13(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(None)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_14(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present and motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_15(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_16(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            None,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_17(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            None,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_18(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            None,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_19(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_20(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_21(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_22(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_23(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = None
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_24(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(None)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_25(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = None
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_26(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        None, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_27(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=None, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_28(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=None
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_29(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_30(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_31(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_32(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=True
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_33(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = None
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_34(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(None)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_35(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = None
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_36(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(None, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_37(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, None, bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_38(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], None)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_39(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_40(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_41(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], )
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_42(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["XXsfreqXX"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_43(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["SFREQ"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_44(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = None
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_45(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(None, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_46(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, None, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_47(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=None) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_48(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_49(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_50(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, ) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_51(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=False) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_52(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is not True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_53(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is False
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(filtered_events_list)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_54(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = None
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id


def x_map_events_and_validate__mutmut_55(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int]]:
    """Map annotations to events while checking label consistency."""

    # Use caller-provided label map or default Physionet mapping for labels
    effective_label_map = (
        dict(label_map) if label_map is not None else dict(PHYSIONET_LABEL_MAP)
    )
    # Confirm annotations contain only labels that the mapping can handle
    _validate_annotation_labels(raw, effective_label_map)
    # Detect whether motor mapping validation should be applied
    motor_labels_present = any(
        desc in MOTOR_EVENT_LABELS
        for desc in raw.annotations.description
        if not _is_bad_description(desc)
    )
    # Build a motor mapping to make motor imagery labels explicit when needed
    if motor_labels_present or motor_label_map is not None:
        # Validate the mapping either provided by the caller or defaulted
        _validate_motor_mapping(
            raw,
            effective_label_map,
            motor_label_map if motor_label_map is not None else MOTOR_EVENT_LABELS,
        )
    # Extract invalid windows to support removal of corrupted epochs
    bad_intervals = _extract_bad_intervals(raw)
    # Convert annotations into events that MNE Epochs can consume
    events, _ = mne.events_from_annotations(
        raw, event_id=effective_label_map, verbose=False
    )
    # Preserve the full label map even if some labels are absent in a run
    event_id = dict(effective_label_map)
    # Build a boolean mask describing which events survive BAD intervals
    keep_mask = _build_keep_mask(events, raw.info["sfreq"], bad_intervals)
    # Gather events whose mask entries remain explicitly True
    filtered_events_list = [
        event for flag, event in zip(keep_mask, events, strict=True) if flag is True
    ]
    # Convert the preserved events back to a NumPy array for downstream consumers
    filtered_events = np.array(None)
    # Return clean events and the mapping for downstream epoch creation
    return filtered_events, event_id

x_map_events_and_validate__mutmut_mutants : ClassVar[MutantDict] = {
'x_map_events_and_validate__mutmut_1': x_map_events_and_validate__mutmut_1, 
    'x_map_events_and_validate__mutmut_2': x_map_events_and_validate__mutmut_2, 
    'x_map_events_and_validate__mutmut_3': x_map_events_and_validate__mutmut_3, 
    'x_map_events_and_validate__mutmut_4': x_map_events_and_validate__mutmut_4, 
    'x_map_events_and_validate__mutmut_5': x_map_events_and_validate__mutmut_5, 
    'x_map_events_and_validate__mutmut_6': x_map_events_and_validate__mutmut_6, 
    'x_map_events_and_validate__mutmut_7': x_map_events_and_validate__mutmut_7, 
    'x_map_events_and_validate__mutmut_8': x_map_events_and_validate__mutmut_8, 
    'x_map_events_and_validate__mutmut_9': x_map_events_and_validate__mutmut_9, 
    'x_map_events_and_validate__mutmut_10': x_map_events_and_validate__mutmut_10, 
    'x_map_events_and_validate__mutmut_11': x_map_events_and_validate__mutmut_11, 
    'x_map_events_and_validate__mutmut_12': x_map_events_and_validate__mutmut_12, 
    'x_map_events_and_validate__mutmut_13': x_map_events_and_validate__mutmut_13, 
    'x_map_events_and_validate__mutmut_14': x_map_events_and_validate__mutmut_14, 
    'x_map_events_and_validate__mutmut_15': x_map_events_and_validate__mutmut_15, 
    'x_map_events_and_validate__mutmut_16': x_map_events_and_validate__mutmut_16, 
    'x_map_events_and_validate__mutmut_17': x_map_events_and_validate__mutmut_17, 
    'x_map_events_and_validate__mutmut_18': x_map_events_and_validate__mutmut_18, 
    'x_map_events_and_validate__mutmut_19': x_map_events_and_validate__mutmut_19, 
    'x_map_events_and_validate__mutmut_20': x_map_events_and_validate__mutmut_20, 
    'x_map_events_and_validate__mutmut_21': x_map_events_and_validate__mutmut_21, 
    'x_map_events_and_validate__mutmut_22': x_map_events_and_validate__mutmut_22, 
    'x_map_events_and_validate__mutmut_23': x_map_events_and_validate__mutmut_23, 
    'x_map_events_and_validate__mutmut_24': x_map_events_and_validate__mutmut_24, 
    'x_map_events_and_validate__mutmut_25': x_map_events_and_validate__mutmut_25, 
    'x_map_events_and_validate__mutmut_26': x_map_events_and_validate__mutmut_26, 
    'x_map_events_and_validate__mutmut_27': x_map_events_and_validate__mutmut_27, 
    'x_map_events_and_validate__mutmut_28': x_map_events_and_validate__mutmut_28, 
    'x_map_events_and_validate__mutmut_29': x_map_events_and_validate__mutmut_29, 
    'x_map_events_and_validate__mutmut_30': x_map_events_and_validate__mutmut_30, 
    'x_map_events_and_validate__mutmut_31': x_map_events_and_validate__mutmut_31, 
    'x_map_events_and_validate__mutmut_32': x_map_events_and_validate__mutmut_32, 
    'x_map_events_and_validate__mutmut_33': x_map_events_and_validate__mutmut_33, 
    'x_map_events_and_validate__mutmut_34': x_map_events_and_validate__mutmut_34, 
    'x_map_events_and_validate__mutmut_35': x_map_events_and_validate__mutmut_35, 
    'x_map_events_and_validate__mutmut_36': x_map_events_and_validate__mutmut_36, 
    'x_map_events_and_validate__mutmut_37': x_map_events_and_validate__mutmut_37, 
    'x_map_events_and_validate__mutmut_38': x_map_events_and_validate__mutmut_38, 
    'x_map_events_and_validate__mutmut_39': x_map_events_and_validate__mutmut_39, 
    'x_map_events_and_validate__mutmut_40': x_map_events_and_validate__mutmut_40, 
    'x_map_events_and_validate__mutmut_41': x_map_events_and_validate__mutmut_41, 
    'x_map_events_and_validate__mutmut_42': x_map_events_and_validate__mutmut_42, 
    'x_map_events_and_validate__mutmut_43': x_map_events_and_validate__mutmut_43, 
    'x_map_events_and_validate__mutmut_44': x_map_events_and_validate__mutmut_44, 
    'x_map_events_and_validate__mutmut_45': x_map_events_and_validate__mutmut_45, 
    'x_map_events_and_validate__mutmut_46': x_map_events_and_validate__mutmut_46, 
    'x_map_events_and_validate__mutmut_47': x_map_events_and_validate__mutmut_47, 
    'x_map_events_and_validate__mutmut_48': x_map_events_and_validate__mutmut_48, 
    'x_map_events_and_validate__mutmut_49': x_map_events_and_validate__mutmut_49, 
    'x_map_events_and_validate__mutmut_50': x_map_events_and_validate__mutmut_50, 
    'x_map_events_and_validate__mutmut_51': x_map_events_and_validate__mutmut_51, 
    'x_map_events_and_validate__mutmut_52': x_map_events_and_validate__mutmut_52, 
    'x_map_events_and_validate__mutmut_53': x_map_events_and_validate__mutmut_53, 
    'x_map_events_and_validate__mutmut_54': x_map_events_and_validate__mutmut_54, 
    'x_map_events_and_validate__mutmut_55': x_map_events_and_validate__mutmut_55
}

def map_events_and_validate(*args, **kwargs):
    result = _mutmut_trampoline(x_map_events_and_validate__mutmut_orig, x_map_events_and_validate__mutmut_mutants, args, kwargs)
    return result 

map_events_and_validate.__signature__ = _mutmut_signature(x_map_events_and_validate__mutmut_orig)
x_map_events_and_validate__mutmut_orig.__name__ = 'x_map_events_and_validate'


def x_map_events_to_motor_labels__mutmut_orig(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_1(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = None
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_2(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(None) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_3(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_4(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = None
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_5(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        None, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_6(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=None, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_7(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=None
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_8(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_9(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_10(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_11(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = None
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_12(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = None
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_13(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(None)
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_14(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(None))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_15(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = None
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_16(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = None
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_17(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = None
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_18(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = None
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_19(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(None)
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_20(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[3])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_21(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is not None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_22(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(None)
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_23(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(None))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_24(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[3]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_25(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            break
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_26(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_27(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            break
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_28(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(None)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_29(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(None)
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_30(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            None
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_31(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps(None)
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_32(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"XXerrorXX": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_33(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"ERROR": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_34(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "XXUnknown event codesXX", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_35(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_36(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "UNKNOWN EVENT CODES", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_37(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "XXunknown_codesXX": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_38(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "UNKNOWN_CODES": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_39(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_40(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            None
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_41(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                None
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_42(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_43(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_44(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "XXNo motor events presentXX",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_45(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "no motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_46(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "NO MOTOR EVENTS PRESENT",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_47(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "XXavailable_labelsXX": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_48(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "AVAILABLE_LABELS": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_49(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = None
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_50(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label not in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(filtered_events)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_51(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = None
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels


def x_map_events_to_motor_labels__mutmut_52(
    raw: mne.io.BaseRaw,
    label_map: Mapping[str, int] | None = None,
    motor_label_map: Mapping[str, str] | None = None,
) -> Tuple[np.ndarray, Dict[str, int], List[str]]:
    """
    Map EEGMMI (PhysioNet) annotations (T0, T1, T2, ...) to motor labels.

    Returns
    -------
    events : np.ndarray
        Tableau d'événements MNE (n_events, 3), filtré pour ne garder
        que les essais moteurs (T1, T2, T3, T4 si présents).
    event_id : dict[str, int]
        Dictionnaire MNE ne contenant que les codes moteurs présents.
    motor_labels : list[str]
        Liste des labels moteurs *par essai* après filtrage, alignée
        sur `events` (ex: ['A', 'B', 'A', ...]).

        Par convention de ce projet :
        - T1 -> 'A'
        - T2 -> 'B'
        Les autres codes (T3, T4) sont conservés tels quels si présents.
    """

    # Construit une cartographie de labels moteurs personnalisable
    effective_motor_map = (
        dict(motor_label_map) if motor_label_map is not None else MOTOR_EVENT_LABELS
    )
    # Récupère les événements validés pour respecter les filtres existants
    events, event_id = map_events_and_validate(
        raw, label_map=label_map, motor_label_map=effective_motor_map
    )
    # Inverse le mapping pour résoudre les codes entiers en labels texte
    inv_event_id = {v: k for k, v in event_id.items()}
    # Construit la liste des labels présents pour reporter les erreurs
    available_labels = sorted(set(inv_event_id.values()))
    # Initialise le conteneur pour les événements moteurs conservés
    filtered_events: List[np.ndarray] = []
    # Initialise le conteneur pour les labels moteurs alignés sur events
    motor_labels: List[str] = []
    # Initialise le collecteur des codes inconnus pour un message d'erreur clair
    unknown_codes: List[int] = []
    # Parcourt chaque événement filtré pour traduire les codes en labels moteurs
    for event in events:
        # Identifie le label texte correspondant au code de l'événement
        label = inv_event_id.get(event[2])
        # Accumule les codes inconnus pour fournir un diagnostic explicite
        if label is None:
            unknown_codes.append(int(event[2]))
            continue
        # Ignore les événements non moteurs pour concentrer l'analyse
        if label not in effective_motor_map:
            continue
        # Conserve l'événement aligné sur un essai moteur
        filtered_events.append(event)
        # Transforme le label en étiquette moteur selon la convention projet
        motor_labels.append(effective_motor_map[label])
    # Lève une erreur explicite si des codes inconnus sont rencontrés
    if unknown_codes:
        raise ValueError(
            json.dumps({"error": "Unknown event codes", "unknown_codes": unknown_codes})
        )
    # Lève une erreur si aucun essai moteur n'est disponible après validation
    if not motor_labels:
        raise ValueError(
            json.dumps(
                {
                    "error": "No motor events present",
                    "available_labels": available_labels,
                }
            )
        )
    # Restreint le mapping aux codes moteurs effectivement rencontrés
    motor_event_id = {
        label: code for label, code in event_id.items() if label in effective_motor_map
    }
    # Convertit la liste d'événements filtrés en tableau NumPy pour MNE
    filtered_array = np.array(None)
    # Retourne les événements filtrés, le mapping réduit et les labels moteurs
    return filtered_array, motor_event_id, motor_labels

x_map_events_to_motor_labels__mutmut_mutants : ClassVar[MutantDict] = {
'x_map_events_to_motor_labels__mutmut_1': x_map_events_to_motor_labels__mutmut_1, 
    'x_map_events_to_motor_labels__mutmut_2': x_map_events_to_motor_labels__mutmut_2, 
    'x_map_events_to_motor_labels__mutmut_3': x_map_events_to_motor_labels__mutmut_3, 
    'x_map_events_to_motor_labels__mutmut_4': x_map_events_to_motor_labels__mutmut_4, 
    'x_map_events_to_motor_labels__mutmut_5': x_map_events_to_motor_labels__mutmut_5, 
    'x_map_events_to_motor_labels__mutmut_6': x_map_events_to_motor_labels__mutmut_6, 
    'x_map_events_to_motor_labels__mutmut_7': x_map_events_to_motor_labels__mutmut_7, 
    'x_map_events_to_motor_labels__mutmut_8': x_map_events_to_motor_labels__mutmut_8, 
    'x_map_events_to_motor_labels__mutmut_9': x_map_events_to_motor_labels__mutmut_9, 
    'x_map_events_to_motor_labels__mutmut_10': x_map_events_to_motor_labels__mutmut_10, 
    'x_map_events_to_motor_labels__mutmut_11': x_map_events_to_motor_labels__mutmut_11, 
    'x_map_events_to_motor_labels__mutmut_12': x_map_events_to_motor_labels__mutmut_12, 
    'x_map_events_to_motor_labels__mutmut_13': x_map_events_to_motor_labels__mutmut_13, 
    'x_map_events_to_motor_labels__mutmut_14': x_map_events_to_motor_labels__mutmut_14, 
    'x_map_events_to_motor_labels__mutmut_15': x_map_events_to_motor_labels__mutmut_15, 
    'x_map_events_to_motor_labels__mutmut_16': x_map_events_to_motor_labels__mutmut_16, 
    'x_map_events_to_motor_labels__mutmut_17': x_map_events_to_motor_labels__mutmut_17, 
    'x_map_events_to_motor_labels__mutmut_18': x_map_events_to_motor_labels__mutmut_18, 
    'x_map_events_to_motor_labels__mutmut_19': x_map_events_to_motor_labels__mutmut_19, 
    'x_map_events_to_motor_labels__mutmut_20': x_map_events_to_motor_labels__mutmut_20, 
    'x_map_events_to_motor_labels__mutmut_21': x_map_events_to_motor_labels__mutmut_21, 
    'x_map_events_to_motor_labels__mutmut_22': x_map_events_to_motor_labels__mutmut_22, 
    'x_map_events_to_motor_labels__mutmut_23': x_map_events_to_motor_labels__mutmut_23, 
    'x_map_events_to_motor_labels__mutmut_24': x_map_events_to_motor_labels__mutmut_24, 
    'x_map_events_to_motor_labels__mutmut_25': x_map_events_to_motor_labels__mutmut_25, 
    'x_map_events_to_motor_labels__mutmut_26': x_map_events_to_motor_labels__mutmut_26, 
    'x_map_events_to_motor_labels__mutmut_27': x_map_events_to_motor_labels__mutmut_27, 
    'x_map_events_to_motor_labels__mutmut_28': x_map_events_to_motor_labels__mutmut_28, 
    'x_map_events_to_motor_labels__mutmut_29': x_map_events_to_motor_labels__mutmut_29, 
    'x_map_events_to_motor_labels__mutmut_30': x_map_events_to_motor_labels__mutmut_30, 
    'x_map_events_to_motor_labels__mutmut_31': x_map_events_to_motor_labels__mutmut_31, 
    'x_map_events_to_motor_labels__mutmut_32': x_map_events_to_motor_labels__mutmut_32, 
    'x_map_events_to_motor_labels__mutmut_33': x_map_events_to_motor_labels__mutmut_33, 
    'x_map_events_to_motor_labels__mutmut_34': x_map_events_to_motor_labels__mutmut_34, 
    'x_map_events_to_motor_labels__mutmut_35': x_map_events_to_motor_labels__mutmut_35, 
    'x_map_events_to_motor_labels__mutmut_36': x_map_events_to_motor_labels__mutmut_36, 
    'x_map_events_to_motor_labels__mutmut_37': x_map_events_to_motor_labels__mutmut_37, 
    'x_map_events_to_motor_labels__mutmut_38': x_map_events_to_motor_labels__mutmut_38, 
    'x_map_events_to_motor_labels__mutmut_39': x_map_events_to_motor_labels__mutmut_39, 
    'x_map_events_to_motor_labels__mutmut_40': x_map_events_to_motor_labels__mutmut_40, 
    'x_map_events_to_motor_labels__mutmut_41': x_map_events_to_motor_labels__mutmut_41, 
    'x_map_events_to_motor_labels__mutmut_42': x_map_events_to_motor_labels__mutmut_42, 
    'x_map_events_to_motor_labels__mutmut_43': x_map_events_to_motor_labels__mutmut_43, 
    'x_map_events_to_motor_labels__mutmut_44': x_map_events_to_motor_labels__mutmut_44, 
    'x_map_events_to_motor_labels__mutmut_45': x_map_events_to_motor_labels__mutmut_45, 
    'x_map_events_to_motor_labels__mutmut_46': x_map_events_to_motor_labels__mutmut_46, 
    'x_map_events_to_motor_labels__mutmut_47': x_map_events_to_motor_labels__mutmut_47, 
    'x_map_events_to_motor_labels__mutmut_48': x_map_events_to_motor_labels__mutmut_48, 
    'x_map_events_to_motor_labels__mutmut_49': x_map_events_to_motor_labels__mutmut_49, 
    'x_map_events_to_motor_labels__mutmut_50': x_map_events_to_motor_labels__mutmut_50, 
    'x_map_events_to_motor_labels__mutmut_51': x_map_events_to_motor_labels__mutmut_51, 
    'x_map_events_to_motor_labels__mutmut_52': x_map_events_to_motor_labels__mutmut_52
}

def map_events_to_motor_labels(*args, **kwargs):
    result = _mutmut_trampoline(x_map_events_to_motor_labels__mutmut_orig, x_map_events_to_motor_labels__mutmut_mutants, args, kwargs)
    return result 

map_events_to_motor_labels.__signature__ = _mutmut_signature(x_map_events_to_motor_labels__mutmut_orig)
x_map_events_to_motor_labels__mutmut_orig.__name__ = 'x_map_events_to_motor_labels'


def x__validate_annotation_labels__mutmut_orig(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_1(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = None
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_2(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(None)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_3(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = None
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_4(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map or not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_5(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_6(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_7(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(None)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_8(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            None
        )


def x__validate_annotation_labels__mutmut_9(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                None
            )
        )


def x__validate_annotation_labels__mutmut_10(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_11(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "Unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_12(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "XXUnknown annotation labelsXX",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_13(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "unknown annotation labels",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_14(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "UNKNOWN ANNOTATION LABELS",
                    "unknown_labels": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_15(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "XXunknown_labelsXX": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_16(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "UNKNOWN_LABELS": sorted(unknown_labels),
                }
            )
        )


def x__validate_annotation_labels__mutmut_17(
    raw: mne.io.BaseRaw, effective_label_map: Mapping[str, int]
) -> None:
    """Ensure annotations only include labels present in the mapping."""

    # Inspect annotations to ensure only expected labels remain
    present_labels = set(raw.annotations.description)
    # Identify labels that would break the supervised mapping stage
    unknown_labels = {
        lab
        for lab in present_labels
        if lab not in effective_label_map and not _is_bad_description(lab)
    }
    # Stop early when unknown labels are detected to prevent silent errors
    if unknown_labels:
        # Raise a descriptive error to support dataset hygiene during setup
        raise ValueError(
            json.dumps(
                {
                    "error": "Unknown annotation labels",
                    "unknown_labels": sorted(None),
                }
            )
        )

x__validate_annotation_labels__mutmut_mutants : ClassVar[MutantDict] = {
'x__validate_annotation_labels__mutmut_1': x__validate_annotation_labels__mutmut_1, 
    'x__validate_annotation_labels__mutmut_2': x__validate_annotation_labels__mutmut_2, 
    'x__validate_annotation_labels__mutmut_3': x__validate_annotation_labels__mutmut_3, 
    'x__validate_annotation_labels__mutmut_4': x__validate_annotation_labels__mutmut_4, 
    'x__validate_annotation_labels__mutmut_5': x__validate_annotation_labels__mutmut_5, 
    'x__validate_annotation_labels__mutmut_6': x__validate_annotation_labels__mutmut_6, 
    'x__validate_annotation_labels__mutmut_7': x__validate_annotation_labels__mutmut_7, 
    'x__validate_annotation_labels__mutmut_8': x__validate_annotation_labels__mutmut_8, 
    'x__validate_annotation_labels__mutmut_9': x__validate_annotation_labels__mutmut_9, 
    'x__validate_annotation_labels__mutmut_10': x__validate_annotation_labels__mutmut_10, 
    'x__validate_annotation_labels__mutmut_11': x__validate_annotation_labels__mutmut_11, 
    'x__validate_annotation_labels__mutmut_12': x__validate_annotation_labels__mutmut_12, 
    'x__validate_annotation_labels__mutmut_13': x__validate_annotation_labels__mutmut_13, 
    'x__validate_annotation_labels__mutmut_14': x__validate_annotation_labels__mutmut_14, 
    'x__validate_annotation_labels__mutmut_15': x__validate_annotation_labels__mutmut_15, 
    'x__validate_annotation_labels__mutmut_16': x__validate_annotation_labels__mutmut_16, 
    'x__validate_annotation_labels__mutmut_17': x__validate_annotation_labels__mutmut_17
}

def _validate_annotation_labels(*args, **kwargs):
    result = _mutmut_trampoline(x__validate_annotation_labels__mutmut_orig, x__validate_annotation_labels__mutmut_mutants, args, kwargs)
    return result 

_validate_annotation_labels.__signature__ = _mutmut_signature(x__validate_annotation_labels__mutmut_orig)
x__validate_annotation_labels__mutmut_orig.__name__ = 'x__validate_annotation_labels'


def x__validate_motor_mapping__mutmut_orig(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_1(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = None
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_2(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(None)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_3(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = None
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_4(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"XXAXX", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_5(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"a", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_6(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "XXBXX"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_7(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "b"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_8(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = None
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_9(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) + allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_10(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(None) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_11(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            None
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_12(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(None)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_13(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(None)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_14(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = None
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_15(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_16(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(None)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_17(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = None
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_18(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc not in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_19(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = None
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_20(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates + set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_21(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(None)
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_22(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            None
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_23(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(None)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_24(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = None
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_25(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels + set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_26(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(None)
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_27(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            None
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_28(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(None)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_29(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(None)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_30(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = None
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_31(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) + set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_32(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(None) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_33(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(None)
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(unknown_keys)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_34(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            None
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map


def x__validate_motor_mapping__mutmut_35(
    raw: mne.io.BaseRaw,
    effective_label_map: Mapping[str, int],
    motor_label_map: Mapping[str, str],
) -> Dict[str, str]:
    """Validate motor mapping covers all events with A/B labels."""

    # Copy the mapping to avoid mutating caller dictionaries during validation
    effective_motor_map = dict(motor_label_map)
    # Restrict allowed motor labels to the binary A/B tasks defined by the project
    allowed_motor_labels = {"A", "B"}
    # Detect invalid motor labels that would break downstream training splits
    invalid_motor_labels = set(effective_motor_map.values()) - allowed_motor_labels
    # Raise a clear error when unsupported motor labels are provided
    if invalid_motor_labels:
        # Surface which labels are invalid to guide mapping corrections
        raise ValueError(
            f"Motor labels must be within {sorted(allowed_motor_labels)}: "
            f"found {sorted(invalid_motor_labels)}"
        )
    # Collect all annotation labels excluding BAD markers for completeness checks
    observed_labels = {
        desc for desc in raw.annotations.description if not _is_bad_description(desc)
    }
    # Restrict completeness checks to motor-related annotation labels
    motor_label_candidates = {
        desc for desc in observed_labels if desc in MOTOR_EVENT_LABELS
    }
    # Identify observed motor labels not covered by the motor mapping
    missing_motor_keys = motor_label_candidates - set(effective_motor_map.keys())
    # Raise a descriptive error when observed labels lack motor interpretations
    if missing_motor_keys:
        # Include unknown label names to speed up dataset adjustments
        raise ValueError(
            f"Motor mapping missing labels for events: {sorted(missing_motor_keys)}"
        )
    # Identify motor labels that are expected but absent from the mapping outputs
    missing_targets = allowed_motor_labels - set(effective_motor_map.values())
    # Raise when A or B is not reachable from the mapping configuration
    if missing_targets:
        # Provide actionable feedback by listing missing motor targets explicitly
        raise ValueError(
            f"Motor mapping must include targets {sorted(allowed_motor_labels)}: "
            f"missing {sorted(missing_targets)}"
        )
    # Identify motor keys that are not part of the annotation label map
    unknown_keys = set(effective_motor_map.keys()) - set(effective_label_map.keys())
    # Stop when the motor mapping references labels outside the event ID map
    if unknown_keys:
        # Include stray keys in the error to steer label alignment quickly
        raise ValueError(
            f"Motor mapping references unknown events: {sorted(None)}"
        )
    # Return the validated motor mapping for optional downstream logging
    return effective_motor_map

x__validate_motor_mapping__mutmut_mutants : ClassVar[MutantDict] = {
'x__validate_motor_mapping__mutmut_1': x__validate_motor_mapping__mutmut_1, 
    'x__validate_motor_mapping__mutmut_2': x__validate_motor_mapping__mutmut_2, 
    'x__validate_motor_mapping__mutmut_3': x__validate_motor_mapping__mutmut_3, 
    'x__validate_motor_mapping__mutmut_4': x__validate_motor_mapping__mutmut_4, 
    'x__validate_motor_mapping__mutmut_5': x__validate_motor_mapping__mutmut_5, 
    'x__validate_motor_mapping__mutmut_6': x__validate_motor_mapping__mutmut_6, 
    'x__validate_motor_mapping__mutmut_7': x__validate_motor_mapping__mutmut_7, 
    'x__validate_motor_mapping__mutmut_8': x__validate_motor_mapping__mutmut_8, 
    'x__validate_motor_mapping__mutmut_9': x__validate_motor_mapping__mutmut_9, 
    'x__validate_motor_mapping__mutmut_10': x__validate_motor_mapping__mutmut_10, 
    'x__validate_motor_mapping__mutmut_11': x__validate_motor_mapping__mutmut_11, 
    'x__validate_motor_mapping__mutmut_12': x__validate_motor_mapping__mutmut_12, 
    'x__validate_motor_mapping__mutmut_13': x__validate_motor_mapping__mutmut_13, 
    'x__validate_motor_mapping__mutmut_14': x__validate_motor_mapping__mutmut_14, 
    'x__validate_motor_mapping__mutmut_15': x__validate_motor_mapping__mutmut_15, 
    'x__validate_motor_mapping__mutmut_16': x__validate_motor_mapping__mutmut_16, 
    'x__validate_motor_mapping__mutmut_17': x__validate_motor_mapping__mutmut_17, 
    'x__validate_motor_mapping__mutmut_18': x__validate_motor_mapping__mutmut_18, 
    'x__validate_motor_mapping__mutmut_19': x__validate_motor_mapping__mutmut_19, 
    'x__validate_motor_mapping__mutmut_20': x__validate_motor_mapping__mutmut_20, 
    'x__validate_motor_mapping__mutmut_21': x__validate_motor_mapping__mutmut_21, 
    'x__validate_motor_mapping__mutmut_22': x__validate_motor_mapping__mutmut_22, 
    'x__validate_motor_mapping__mutmut_23': x__validate_motor_mapping__mutmut_23, 
    'x__validate_motor_mapping__mutmut_24': x__validate_motor_mapping__mutmut_24, 
    'x__validate_motor_mapping__mutmut_25': x__validate_motor_mapping__mutmut_25, 
    'x__validate_motor_mapping__mutmut_26': x__validate_motor_mapping__mutmut_26, 
    'x__validate_motor_mapping__mutmut_27': x__validate_motor_mapping__mutmut_27, 
    'x__validate_motor_mapping__mutmut_28': x__validate_motor_mapping__mutmut_28, 
    'x__validate_motor_mapping__mutmut_29': x__validate_motor_mapping__mutmut_29, 
    'x__validate_motor_mapping__mutmut_30': x__validate_motor_mapping__mutmut_30, 
    'x__validate_motor_mapping__mutmut_31': x__validate_motor_mapping__mutmut_31, 
    'x__validate_motor_mapping__mutmut_32': x__validate_motor_mapping__mutmut_32, 
    'x__validate_motor_mapping__mutmut_33': x__validate_motor_mapping__mutmut_33, 
    'x__validate_motor_mapping__mutmut_34': x__validate_motor_mapping__mutmut_34, 
    'x__validate_motor_mapping__mutmut_35': x__validate_motor_mapping__mutmut_35
}

def _validate_motor_mapping(*args, **kwargs):
    result = _mutmut_trampoline(x__validate_motor_mapping__mutmut_orig, x__validate_motor_mapping__mutmut_mutants, args, kwargs)
    return result 

_validate_motor_mapping.__signature__ = _mutmut_signature(x__validate_motor_mapping__mutmut_orig)
x__validate_motor_mapping__mutmut_orig.__name__ = 'x__validate_motor_mapping'


def x__build_keep_mask__mutmut_orig(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_1(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_2(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = None
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_3(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(None)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_4(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = None
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_5(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] / len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_6(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [False] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_7(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(None):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_8(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = None
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_9(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample * sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_10(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(None):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_11(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start < event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_12(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time < end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_13(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = None
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_14(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = True
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_15(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_16(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(None):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("Event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_17(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError(None)
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_18(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("XXEvent mask contained non-boolean valuesXX")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_19(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("event mask contained non-boolean values")
    # Return the vetted mask for downstream event filtering
    return keep_mask


def x__build_keep_mask__mutmut_20(
    events: np.ndarray,
    sampling_rate: float,
    bad_intervals: List[Tuple[float, float]],
    forced_mask: List[Any] | None = None,
) -> List[bool]:
    """Return a boolean mask that excludes events overlapping BAD spans."""

    # Declare the mask variable once to maintain consistent typing across branches
    keep_mask: List[Any]
    # Accept an externally supplied mask to validate defensive branches explicitly
    if forced_mask is not None:
        # Copy the forced mask to avoid caller-side mutations affecting validation
        keep_mask = list(forced_mask)
    else:
        # Initialize a boolean mask list to track valid events explicitly
        keep_mask = [True] * len(events)
        # Iterate over events to check whether they overlap a BAD interval
        for idx, (sample, _, _) in enumerate(events):
            # Convert sample index to seconds to compare against annotation times
            event_time = sample / sampling_rate
            # Mark the event for removal when it lies within a BAD interval
            if any(start <= event_time <= end for start, end in bad_intervals):
                # Update the mask to drop contaminated events from the dataset
                keep_mask[idx] = False
    # Enforce boolean typing on the mask to avoid silent drops from bad values
    if not all(isinstance(flag, bool) for flag in keep_mask):
        # Raise when the mask contains non-boolean entries to surface errors early
        raise TypeError("EVENT MASK CONTAINED NON-BOOLEAN VALUES")
    # Return the vetted mask for downstream event filtering
    return keep_mask

x__build_keep_mask__mutmut_mutants : ClassVar[MutantDict] = {
'x__build_keep_mask__mutmut_1': x__build_keep_mask__mutmut_1, 
    'x__build_keep_mask__mutmut_2': x__build_keep_mask__mutmut_2, 
    'x__build_keep_mask__mutmut_3': x__build_keep_mask__mutmut_3, 
    'x__build_keep_mask__mutmut_4': x__build_keep_mask__mutmut_4, 
    'x__build_keep_mask__mutmut_5': x__build_keep_mask__mutmut_5, 
    'x__build_keep_mask__mutmut_6': x__build_keep_mask__mutmut_6, 
    'x__build_keep_mask__mutmut_7': x__build_keep_mask__mutmut_7, 
    'x__build_keep_mask__mutmut_8': x__build_keep_mask__mutmut_8, 
    'x__build_keep_mask__mutmut_9': x__build_keep_mask__mutmut_9, 
    'x__build_keep_mask__mutmut_10': x__build_keep_mask__mutmut_10, 
    'x__build_keep_mask__mutmut_11': x__build_keep_mask__mutmut_11, 
    'x__build_keep_mask__mutmut_12': x__build_keep_mask__mutmut_12, 
    'x__build_keep_mask__mutmut_13': x__build_keep_mask__mutmut_13, 
    'x__build_keep_mask__mutmut_14': x__build_keep_mask__mutmut_14, 
    'x__build_keep_mask__mutmut_15': x__build_keep_mask__mutmut_15, 
    'x__build_keep_mask__mutmut_16': x__build_keep_mask__mutmut_16, 
    'x__build_keep_mask__mutmut_17': x__build_keep_mask__mutmut_17, 
    'x__build_keep_mask__mutmut_18': x__build_keep_mask__mutmut_18, 
    'x__build_keep_mask__mutmut_19': x__build_keep_mask__mutmut_19, 
    'x__build_keep_mask__mutmut_20': x__build_keep_mask__mutmut_20
}

def _build_keep_mask(*args, **kwargs):
    result = _mutmut_trampoline(x__build_keep_mask__mutmut_orig, x__build_keep_mask__mutmut_mutants, args, kwargs)
    return result 

_build_keep_mask.__signature__ = _mutmut_signature(x__build_keep_mask__mutmut_orig)
x__build_keep_mask__mutmut_orig.__name__ = 'x__build_keep_mask'


def x_create_epochs_from_raw__mutmut_orig(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_1(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 1.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_2(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = None
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_3(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(None)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_4(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_5(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(None, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_6(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, None):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_7(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_8(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, ):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_9(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError(None)
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_10(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("XXevents must contain integer-coded sample indicesXX")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_11(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("EVENTS MUST CONTAIN INTEGER-CODED SAMPLE INDICES")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_12(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = None
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_13(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(None, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_14(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=None)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_15(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_16(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, )
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_17(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=True)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_18(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = None
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_19(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        None,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_20(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=None,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_21(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=None,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_22(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=None,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_23(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=None,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_24(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=None,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_25(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=None,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_26(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing=None,
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_27(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=None,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_28(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_29(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_30(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_31(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_32(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_33(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_34(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_35(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_36(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_37(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_38(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=False,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_39(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=False,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_40(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="XXignoreXX",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_41(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="IGNORE",
        verbose=False,
    )
    # Return epochs ready for feature extraction and model training
    return epochs


def x_create_epochs_from_raw__mutmut_42(
    raw: mne.io.BaseRaw,
    events: np.ndarray,
    event_id: Mapping[str, int],
    tmin: float = -0.2,
    tmax: float = 0.8,
) -> mne.Epochs:
    """Construct epochs with annotation-aware rejection."""

    # Convert the events array to enforce integer sample indices
    safe_events = np.asarray(events)
    # Validate that all event indices are integers to satisfy MNE expectations
    if not np.issubdtype(safe_events.dtype, np.integer):
        # Raise an explicit error when indices are not numeric to avoid MNE crashes
        raise ValueError("events must contain integer-coded sample indices")
    # Reuse the typed array without copying when already integer
    typed_events = safe_events.astype(int, copy=False)
    # Build epochs while honoring BAD annotations to avoid contaminating data
    epochs = mne.Epochs(
        raw,
        events=typed_events,
        event_id=event_id,
        tmin=tmin,
        tmax=tmax,
        preload=True,
        reject_by_annotation=True,
        baseline=None,
        # Ignore labels missing from specific runs to keep epoching robust
        on_missing="ignore",
        verbose=True,
    )
    # Return epochs ready for feature extraction and model training
    return epochs

x_create_epochs_from_raw__mutmut_mutants : ClassVar[MutantDict] = {
'x_create_epochs_from_raw__mutmut_1': x_create_epochs_from_raw__mutmut_1, 
    'x_create_epochs_from_raw__mutmut_2': x_create_epochs_from_raw__mutmut_2, 
    'x_create_epochs_from_raw__mutmut_3': x_create_epochs_from_raw__mutmut_3, 
    'x_create_epochs_from_raw__mutmut_4': x_create_epochs_from_raw__mutmut_4, 
    'x_create_epochs_from_raw__mutmut_5': x_create_epochs_from_raw__mutmut_5, 
    'x_create_epochs_from_raw__mutmut_6': x_create_epochs_from_raw__mutmut_6, 
    'x_create_epochs_from_raw__mutmut_7': x_create_epochs_from_raw__mutmut_7, 
    'x_create_epochs_from_raw__mutmut_8': x_create_epochs_from_raw__mutmut_8, 
    'x_create_epochs_from_raw__mutmut_9': x_create_epochs_from_raw__mutmut_9, 
    'x_create_epochs_from_raw__mutmut_10': x_create_epochs_from_raw__mutmut_10, 
    'x_create_epochs_from_raw__mutmut_11': x_create_epochs_from_raw__mutmut_11, 
    'x_create_epochs_from_raw__mutmut_12': x_create_epochs_from_raw__mutmut_12, 
    'x_create_epochs_from_raw__mutmut_13': x_create_epochs_from_raw__mutmut_13, 
    'x_create_epochs_from_raw__mutmut_14': x_create_epochs_from_raw__mutmut_14, 
    'x_create_epochs_from_raw__mutmut_15': x_create_epochs_from_raw__mutmut_15, 
    'x_create_epochs_from_raw__mutmut_16': x_create_epochs_from_raw__mutmut_16, 
    'x_create_epochs_from_raw__mutmut_17': x_create_epochs_from_raw__mutmut_17, 
    'x_create_epochs_from_raw__mutmut_18': x_create_epochs_from_raw__mutmut_18, 
    'x_create_epochs_from_raw__mutmut_19': x_create_epochs_from_raw__mutmut_19, 
    'x_create_epochs_from_raw__mutmut_20': x_create_epochs_from_raw__mutmut_20, 
    'x_create_epochs_from_raw__mutmut_21': x_create_epochs_from_raw__mutmut_21, 
    'x_create_epochs_from_raw__mutmut_22': x_create_epochs_from_raw__mutmut_22, 
    'x_create_epochs_from_raw__mutmut_23': x_create_epochs_from_raw__mutmut_23, 
    'x_create_epochs_from_raw__mutmut_24': x_create_epochs_from_raw__mutmut_24, 
    'x_create_epochs_from_raw__mutmut_25': x_create_epochs_from_raw__mutmut_25, 
    'x_create_epochs_from_raw__mutmut_26': x_create_epochs_from_raw__mutmut_26, 
    'x_create_epochs_from_raw__mutmut_27': x_create_epochs_from_raw__mutmut_27, 
    'x_create_epochs_from_raw__mutmut_28': x_create_epochs_from_raw__mutmut_28, 
    'x_create_epochs_from_raw__mutmut_29': x_create_epochs_from_raw__mutmut_29, 
    'x_create_epochs_from_raw__mutmut_30': x_create_epochs_from_raw__mutmut_30, 
    'x_create_epochs_from_raw__mutmut_31': x_create_epochs_from_raw__mutmut_31, 
    'x_create_epochs_from_raw__mutmut_32': x_create_epochs_from_raw__mutmut_32, 
    'x_create_epochs_from_raw__mutmut_33': x_create_epochs_from_raw__mutmut_33, 
    'x_create_epochs_from_raw__mutmut_34': x_create_epochs_from_raw__mutmut_34, 
    'x_create_epochs_from_raw__mutmut_35': x_create_epochs_from_raw__mutmut_35, 
    'x_create_epochs_from_raw__mutmut_36': x_create_epochs_from_raw__mutmut_36, 
    'x_create_epochs_from_raw__mutmut_37': x_create_epochs_from_raw__mutmut_37, 
    'x_create_epochs_from_raw__mutmut_38': x_create_epochs_from_raw__mutmut_38, 
    'x_create_epochs_from_raw__mutmut_39': x_create_epochs_from_raw__mutmut_39, 
    'x_create_epochs_from_raw__mutmut_40': x_create_epochs_from_raw__mutmut_40, 
    'x_create_epochs_from_raw__mutmut_41': x_create_epochs_from_raw__mutmut_41, 
    'x_create_epochs_from_raw__mutmut_42': x_create_epochs_from_raw__mutmut_42
}

def create_epochs_from_raw(*args, **kwargs):
    result = _mutmut_trampoline(x_create_epochs_from_raw__mutmut_orig, x_create_epochs_from_raw__mutmut_mutants, args, kwargs)
    return result 

create_epochs_from_raw.__signature__ = _mutmut_signature(x_create_epochs_from_raw__mutmut_orig)
x_create_epochs_from_raw__mutmut_orig.__name__ = 'x_create_epochs_from_raw'


def x__expected_epoch_samples__mutmut_orig(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax - epochs.tmin) * epochs.info["sfreq"])) + 1


def x__expected_epoch_samples__mutmut_1(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax - epochs.tmin) * epochs.info["sfreq"])) - 1


def x__expected_epoch_samples__mutmut_2(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(None) + 1


def x__expected_epoch_samples__mutmut_3(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round(None)) + 1


def x__expected_epoch_samples__mutmut_4(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax - epochs.tmin) / epochs.info["sfreq"])) + 1


def x__expected_epoch_samples__mutmut_5(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax + epochs.tmin) * epochs.info["sfreq"])) + 1


def x__expected_epoch_samples__mutmut_6(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax - epochs.tmin) * epochs.info["XXsfreqXX"])) + 1


def x__expected_epoch_samples__mutmut_7(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax - epochs.tmin) * epochs.info["SFREQ"])) + 1


def x__expected_epoch_samples__mutmut_8(epochs: mne.Epochs) -> int:
    """Compute the expected number of samples per epoch."""

    # Derive duration-based sample count to catch truncated segments
    return int(round((epochs.tmax - epochs.tmin) * epochs.info["sfreq"])) + 2

x__expected_epoch_samples__mutmut_mutants : ClassVar[MutantDict] = {
'x__expected_epoch_samples__mutmut_1': x__expected_epoch_samples__mutmut_1, 
    'x__expected_epoch_samples__mutmut_2': x__expected_epoch_samples__mutmut_2, 
    'x__expected_epoch_samples__mutmut_3': x__expected_epoch_samples__mutmut_3, 
    'x__expected_epoch_samples__mutmut_4': x__expected_epoch_samples__mutmut_4, 
    'x__expected_epoch_samples__mutmut_5': x__expected_epoch_samples__mutmut_5, 
    'x__expected_epoch_samples__mutmut_6': x__expected_epoch_samples__mutmut_6, 
    'x__expected_epoch_samples__mutmut_7': x__expected_epoch_samples__mutmut_7, 
    'x__expected_epoch_samples__mutmut_8': x__expected_epoch_samples__mutmut_8
}

def _expected_epoch_samples(*args, **kwargs):
    result = _mutmut_trampoline(x__expected_epoch_samples__mutmut_orig, x__expected_epoch_samples__mutmut_mutants, args, kwargs)
    return result 

_expected_epoch_samples.__signature__ = _mutmut_signature(x__expected_epoch_samples__mutmut_orig)
x__expected_epoch_samples__mutmut_orig.__name__ = 'x__expected_epoch_samples'


def x__flag_epoch_quality__mutmut_orig(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_1(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = None
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_2(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = None
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_3(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(None)
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_4(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(None))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_5(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value >= max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_6(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append(None)
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_7(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("XXartifactXX")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_8(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("ARTIFACT")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_9(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples and np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_10(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[2] < expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_11(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] <= expected_samples or np.isnan(epoch).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_12(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(None).any():
        reasons.append("incomplete")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_13(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append(None)
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_14(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("XXincompleteXX")
    # Return accumulated flags for the caller to aggregate
    return reasons


def x__flag_epoch_quality__mutmut_15(
    epoch: np.ndarray, max_peak_to_peak: float, expected_samples: int
) -> List[str]:
    """Identify quality issues for a single epoch."""

    # Track reasons to support later reporting or masking
    reasons: List[str] = []
    # Measure peak-to-peak amplitude to reveal sharp artifacts
    ptp_value = float(np.ptp(epoch))
    # Record amplitude excursions beyond the threshold to protect models
    if ptp_value > max_peak_to_peak:
        reasons.append("artifact")
    # Detect incomplete epochs via shape or NaN inspection
    if epoch.shape[1] < expected_samples or np.isnan(epoch).any():
        reasons.append("INCOMPLETE")
    # Return accumulated flags for the caller to aggregate
    return reasons

x__flag_epoch_quality__mutmut_mutants : ClassVar[MutantDict] = {
'x__flag_epoch_quality__mutmut_1': x__flag_epoch_quality__mutmut_1, 
    'x__flag_epoch_quality__mutmut_2': x__flag_epoch_quality__mutmut_2, 
    'x__flag_epoch_quality__mutmut_3': x__flag_epoch_quality__mutmut_3, 
    'x__flag_epoch_quality__mutmut_4': x__flag_epoch_quality__mutmut_4, 
    'x__flag_epoch_quality__mutmut_5': x__flag_epoch_quality__mutmut_5, 
    'x__flag_epoch_quality__mutmut_6': x__flag_epoch_quality__mutmut_6, 
    'x__flag_epoch_quality__mutmut_7': x__flag_epoch_quality__mutmut_7, 
    'x__flag_epoch_quality__mutmut_8': x__flag_epoch_quality__mutmut_8, 
    'x__flag_epoch_quality__mutmut_9': x__flag_epoch_quality__mutmut_9, 
    'x__flag_epoch_quality__mutmut_10': x__flag_epoch_quality__mutmut_10, 
    'x__flag_epoch_quality__mutmut_11': x__flag_epoch_quality__mutmut_11, 
    'x__flag_epoch_quality__mutmut_12': x__flag_epoch_quality__mutmut_12, 
    'x__flag_epoch_quality__mutmut_13': x__flag_epoch_quality__mutmut_13, 
    'x__flag_epoch_quality__mutmut_14': x__flag_epoch_quality__mutmut_14, 
    'x__flag_epoch_quality__mutmut_15': x__flag_epoch_quality__mutmut_15
}

def _flag_epoch_quality(*args, **kwargs):
    result = _mutmut_trampoline(x__flag_epoch_quality__mutmut_orig, x__flag_epoch_quality__mutmut_mutants, args, kwargs)
    return result 

_flag_epoch_quality.__signature__ = _mutmut_signature(x__flag_epoch_quality__mutmut_orig)
x__flag_epoch_quality__mutmut_orig.__name__ = 'x__flag_epoch_quality'


def x__apply_marking__mutmut_orig(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_1(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is not None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_2(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = None
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_3(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame(None)
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_4(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"XXquality_flagXX": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_5(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"QUALITY_FLAG": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_6(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["ok"] / len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_7(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["XXokXX"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_8(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["OK"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_9(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "quality_flag"] = None
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_10(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "XXquality_flagXX"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged


def x__apply_marking__mutmut_11(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Mark flagged epochs in metadata and return the updated set."""

    # Initialize metadata so downstream code can track quality per epoch
    if safe_epochs.metadata is None:
        # Build a DataFrame with a single column matching epoch count
        safe_epochs.metadata = pd.DataFrame({"quality_flag": ["ok"] * len(safe_epochs)})
    # Iterate to update quality flags for each identified issue
    for reason, indices in flagged.items():
        # Apply the reason to all recorded indices for transparency
        for idx in indices:
            # Overwrite the quality flag to reflect the detected issue
            safe_epochs.metadata.at[idx, "QUALITY_FLAG"] = reason
    # Return the annotated epochs along with the indexed reasons
    return safe_epochs, flagged

x__apply_marking__mutmut_mutants : ClassVar[MutantDict] = {
'x__apply_marking__mutmut_1': x__apply_marking__mutmut_1, 
    'x__apply_marking__mutmut_2': x__apply_marking__mutmut_2, 
    'x__apply_marking__mutmut_3': x__apply_marking__mutmut_3, 
    'x__apply_marking__mutmut_4': x__apply_marking__mutmut_4, 
    'x__apply_marking__mutmut_5': x__apply_marking__mutmut_5, 
    'x__apply_marking__mutmut_6': x__apply_marking__mutmut_6, 
    'x__apply_marking__mutmut_7': x__apply_marking__mutmut_7, 
    'x__apply_marking__mutmut_8': x__apply_marking__mutmut_8, 
    'x__apply_marking__mutmut_9': x__apply_marking__mutmut_9, 
    'x__apply_marking__mutmut_10': x__apply_marking__mutmut_10, 
    'x__apply_marking__mutmut_11': x__apply_marking__mutmut_11
}

def _apply_marking(*args, **kwargs):
    result = _mutmut_trampoline(x__apply_marking__mutmut_orig, x__apply_marking__mutmut_mutants, args, kwargs)
    return result 

_apply_marking.__signature__ = _mutmut_signature(x__apply_marking__mutmut_orig)
x__apply_marking__mutmut_orig.__name__ = 'x__apply_marking'


def x__apply_rejection__mutmut_orig(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_1(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = None
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_2(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) & set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_3(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(None) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_4(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["XXartifactXX"]) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_5(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["ARTIFACT"]) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_6(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(None)
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_7(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(flagged["XXincompleteXX"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_8(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(flagged["INCOMPLETE"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_9(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = None
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_10(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx in removed_indices for idx in range(len(safe_epochs))]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged


def x__apply_rejection__mutmut_11(
    safe_epochs: mne.Epochs, flagged: Dict[str, List[int]]
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Drop flagged epochs and return the pruned set."""

    # Build a set of indices to remove for efficient membership tests
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construct a boolean mask that preserves only unflagged epochs
    keep_mask = [idx not in removed_indices for idx in range(None)]
    # Apply the mask to drop contaminated epochs before returning
    return safe_epochs[keep_mask], flagged

x__apply_rejection__mutmut_mutants : ClassVar[MutantDict] = {
'x__apply_rejection__mutmut_1': x__apply_rejection__mutmut_1, 
    'x__apply_rejection__mutmut_2': x__apply_rejection__mutmut_2, 
    'x__apply_rejection__mutmut_3': x__apply_rejection__mutmut_3, 
    'x__apply_rejection__mutmut_4': x__apply_rejection__mutmut_4, 
    'x__apply_rejection__mutmut_5': x__apply_rejection__mutmut_5, 
    'x__apply_rejection__mutmut_6': x__apply_rejection__mutmut_6, 
    'x__apply_rejection__mutmut_7': x__apply_rejection__mutmut_7, 
    'x__apply_rejection__mutmut_8': x__apply_rejection__mutmut_8, 
    'x__apply_rejection__mutmut_9': x__apply_rejection__mutmut_9, 
    'x__apply_rejection__mutmut_10': x__apply_rejection__mutmut_10, 
    'x__apply_rejection__mutmut_11': x__apply_rejection__mutmut_11
}

def _apply_rejection(*args, **kwargs):
    result = _mutmut_trampoline(x__apply_rejection__mutmut_orig, x__apply_rejection__mutmut_mutants, args, kwargs)
    return result 

_apply_rejection.__signature__ = _mutmut_signature(x__apply_rejection__mutmut_orig)
x__apply_rejection__mutmut_orig.__name__ = 'x__apply_rejection'


def x_quality_control_epochs__mutmut_orig(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_1(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "XXrejectXX",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_2(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "REJECT",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_3(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = None
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_4(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = None
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_5(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=None)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_6(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=False)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_7(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = None
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_8(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(None)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_9(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = None
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_10(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"XXartifactXX": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_11(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"ARTIFACT": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_12(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "XXincompleteXX": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_13(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "INCOMPLETE": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_14(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(None):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_15(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(None, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_16(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, None, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_17(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, None):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_18(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_19(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_20(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, ):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_21(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(None)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_22(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode != "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_23(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "XXrejectXX":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_24(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "REJECT":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_25(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(None, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_26(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, None)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_27(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_28(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, )
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_29(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode != "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_30(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "XXmarkXX":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_31(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "MARK":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_32(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(None, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_33(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, None)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_34(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_35(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, )
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("mode must be either 'reject' or 'mark'")


def x_quality_control_epochs__mutmut_36(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError(None)


def x_quality_control_epochs__mutmut_37(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("XXmode must be either 'reject' or 'mark'XX")


def x_quality_control_epochs__mutmut_38(
    epochs: mne.Epochs,
    max_peak_to_peak: float,
    mode: str = "reject",
) -> Tuple[mne.Epochs, Dict[str, List[int]]]:
    """Screen epochs for artifacts or incompleteness and flag or drop them."""

    # Copy the epochs to avoid mutating caller data during quality enforcement
    safe_epochs = epochs.copy()
    # Retrieve the data to inspect amplitude and completeness metrics
    data = safe_epochs.get_data(copy=True)
    # Compute expected sample count to detect truncated epoch shapes
    expected_samples = _expected_epoch_samples(safe_epochs)
    # Prepare buckets to track artifact and incomplete epoch indices
    flagged: Dict[str, List[int]] = {"artifact": [], "incomplete": []}
    # Iterate over epochs to check amplitude and completeness constraints
    for idx, epoch in enumerate(data):
        # Aggregate all reasons identified for the current epoch
        for reason in _flag_epoch_quality(epoch, max_peak_to_peak, expected_samples):
            # Record the reason to enable downstream removal or marking
            flagged[reason].append(idx)
    # When the mode is reject, remove flagged epochs to stabilize training
    if mode == "reject":
        # Delegate rejection to simplify complexity for linting and tests
        return _apply_rejection(safe_epochs, flagged)
    # When the mode is mark, annotate metadata instead of dropping epochs
    if mode == "mark":
        # Delegate marking to reuse the metadata update logic
        return _apply_marking(safe_epochs, flagged)
    # Raise when the mode is unsupported to avoid silent misuse
    raise ValueError("MODE MUST BE EITHER 'REJECT' OR 'MARK'")

x_quality_control_epochs__mutmut_mutants : ClassVar[MutantDict] = {
'x_quality_control_epochs__mutmut_1': x_quality_control_epochs__mutmut_1, 
    'x_quality_control_epochs__mutmut_2': x_quality_control_epochs__mutmut_2, 
    'x_quality_control_epochs__mutmut_3': x_quality_control_epochs__mutmut_3, 
    'x_quality_control_epochs__mutmut_4': x_quality_control_epochs__mutmut_4, 
    'x_quality_control_epochs__mutmut_5': x_quality_control_epochs__mutmut_5, 
    'x_quality_control_epochs__mutmut_6': x_quality_control_epochs__mutmut_6, 
    'x_quality_control_epochs__mutmut_7': x_quality_control_epochs__mutmut_7, 
    'x_quality_control_epochs__mutmut_8': x_quality_control_epochs__mutmut_8, 
    'x_quality_control_epochs__mutmut_9': x_quality_control_epochs__mutmut_9, 
    'x_quality_control_epochs__mutmut_10': x_quality_control_epochs__mutmut_10, 
    'x_quality_control_epochs__mutmut_11': x_quality_control_epochs__mutmut_11, 
    'x_quality_control_epochs__mutmut_12': x_quality_control_epochs__mutmut_12, 
    'x_quality_control_epochs__mutmut_13': x_quality_control_epochs__mutmut_13, 
    'x_quality_control_epochs__mutmut_14': x_quality_control_epochs__mutmut_14, 
    'x_quality_control_epochs__mutmut_15': x_quality_control_epochs__mutmut_15, 
    'x_quality_control_epochs__mutmut_16': x_quality_control_epochs__mutmut_16, 
    'x_quality_control_epochs__mutmut_17': x_quality_control_epochs__mutmut_17, 
    'x_quality_control_epochs__mutmut_18': x_quality_control_epochs__mutmut_18, 
    'x_quality_control_epochs__mutmut_19': x_quality_control_epochs__mutmut_19, 
    'x_quality_control_epochs__mutmut_20': x_quality_control_epochs__mutmut_20, 
    'x_quality_control_epochs__mutmut_21': x_quality_control_epochs__mutmut_21, 
    'x_quality_control_epochs__mutmut_22': x_quality_control_epochs__mutmut_22, 
    'x_quality_control_epochs__mutmut_23': x_quality_control_epochs__mutmut_23, 
    'x_quality_control_epochs__mutmut_24': x_quality_control_epochs__mutmut_24, 
    'x_quality_control_epochs__mutmut_25': x_quality_control_epochs__mutmut_25, 
    'x_quality_control_epochs__mutmut_26': x_quality_control_epochs__mutmut_26, 
    'x_quality_control_epochs__mutmut_27': x_quality_control_epochs__mutmut_27, 
    'x_quality_control_epochs__mutmut_28': x_quality_control_epochs__mutmut_28, 
    'x_quality_control_epochs__mutmut_29': x_quality_control_epochs__mutmut_29, 
    'x_quality_control_epochs__mutmut_30': x_quality_control_epochs__mutmut_30, 
    'x_quality_control_epochs__mutmut_31': x_quality_control_epochs__mutmut_31, 
    'x_quality_control_epochs__mutmut_32': x_quality_control_epochs__mutmut_32, 
    'x_quality_control_epochs__mutmut_33': x_quality_control_epochs__mutmut_33, 
    'x_quality_control_epochs__mutmut_34': x_quality_control_epochs__mutmut_34, 
    'x_quality_control_epochs__mutmut_35': x_quality_control_epochs__mutmut_35, 
    'x_quality_control_epochs__mutmut_36': x_quality_control_epochs__mutmut_36, 
    'x_quality_control_epochs__mutmut_37': x_quality_control_epochs__mutmut_37, 
    'x_quality_control_epochs__mutmut_38': x_quality_control_epochs__mutmut_38
}

def quality_control_epochs(*args, **kwargs):
    result = _mutmut_trampoline(x_quality_control_epochs__mutmut_orig, x_quality_control_epochs__mutmut_mutants, args, kwargs)
    return result 

quality_control_epochs.__signature__ = _mutmut_signature(x_quality_control_epochs__mutmut_orig)
x_quality_control_epochs__mutmut_orig.__name__ = 'x_quality_control_epochs'


def x_summarize_epoch_quality__mutmut_orig(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_1(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) == len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_2(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            None
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_3(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                None
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_4(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_5(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_6(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "XXLabel/event mismatchXX",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_7(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_8(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "LABEL/EVENT MISMATCH",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_9(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "XXexpected_eventsXX": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_10(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "EXPECTED_EVENTS": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_11(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "XXlabelsXX": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_12(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "LABELS": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_13(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = None
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_14(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        None, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_15(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=None, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_16(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode=None
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_17(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_18(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_19(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_20(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="XXrejectXX"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_21(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="REJECT"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_22(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = None
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_23(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) & set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_24(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(None) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_25(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["XXartifactXX"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_26(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["ARTIFACT"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_27(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(None)
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_28(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["XXincompleteXX"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_29(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["INCOMPLETE"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_30(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = None
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_31(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(None) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_32(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_33(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = None
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_34(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(None) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_35(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = None
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_36(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "XXsubjectXX": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_37(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "SUBJECT": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_38(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[1],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_39(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "XXrunXX": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_40(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "RUN": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_41(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[2],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_42(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "XXdroppedXX": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_43(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "DROPPED": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_44(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "XXcountsXX": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_45(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "COUNTS": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_46(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = None
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_47(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count != 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_48(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 1]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_49(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            None
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_50(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                None
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_51(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "XXerrorXX": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_52(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "ERROR": "Missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_53(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "XXMissing labelsXX", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_54(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "missing labels", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_55(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "MISSING LABELS", "missing_labels": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_56(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "XXmissing_labelsXX": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels


def x_summarize_epoch_quality__mutmut_57(
    epochs: mne.Epochs,
    motor_labels: List[str],
    session: Tuple[str, str],
    max_peak_to_peak: float,
    expected_labels: Tuple[str, str] = ("A", "B"),
) -> Tuple[mne.Epochs, Dict[str, Any], List[str]]:
    """Drop incomplete epochs then count valid labels per subject/run."""

    # Vérifie l'alignement entre événements et étiquettes transmises
    if len(motor_labels) != len(epochs):
        # Génère un rapport clair pour identifier le décalage détecté
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )
    # Applique le contrôle qualité pour supprimer les segments incomplets
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Calcule les indices supprimés afin de filtrer les étiquettes associées
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Construit la liste des labels conservés après suppression des segments
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Décompte les occurrences pour chaque étiquette attendue
    counts = {label: cleaned_labels.count(label) for label in expected_labels}
    # Prépare un rapport synthétique pour la surveillance par sujet et run
    report = {
        "subject": session[0],
        "run": session[1],
        "dropped": {key: len(value) for key, value in flagged.items()},
        "counts": counts,
    }
    # Identifie les classes absentes après nettoyage pour remonter une erreur
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Génère une erreur explicite lorsque des classes attendues manquent
    if missing_labels:
        # Insère le rapport de comptage pour faciliter le diagnostic utilisateur
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "MISSING_LABELS": missing_labels}
            )
        )
    # Retourne les epochs nettoyées, le rapport et les labels filtrés
    return cleaned_epochs, report, cleaned_labels

x_summarize_epoch_quality__mutmut_mutants : ClassVar[MutantDict] = {
'x_summarize_epoch_quality__mutmut_1': x_summarize_epoch_quality__mutmut_1, 
    'x_summarize_epoch_quality__mutmut_2': x_summarize_epoch_quality__mutmut_2, 
    'x_summarize_epoch_quality__mutmut_3': x_summarize_epoch_quality__mutmut_3, 
    'x_summarize_epoch_quality__mutmut_4': x_summarize_epoch_quality__mutmut_4, 
    'x_summarize_epoch_quality__mutmut_5': x_summarize_epoch_quality__mutmut_5, 
    'x_summarize_epoch_quality__mutmut_6': x_summarize_epoch_quality__mutmut_6, 
    'x_summarize_epoch_quality__mutmut_7': x_summarize_epoch_quality__mutmut_7, 
    'x_summarize_epoch_quality__mutmut_8': x_summarize_epoch_quality__mutmut_8, 
    'x_summarize_epoch_quality__mutmut_9': x_summarize_epoch_quality__mutmut_9, 
    'x_summarize_epoch_quality__mutmut_10': x_summarize_epoch_quality__mutmut_10, 
    'x_summarize_epoch_quality__mutmut_11': x_summarize_epoch_quality__mutmut_11, 
    'x_summarize_epoch_quality__mutmut_12': x_summarize_epoch_quality__mutmut_12, 
    'x_summarize_epoch_quality__mutmut_13': x_summarize_epoch_quality__mutmut_13, 
    'x_summarize_epoch_quality__mutmut_14': x_summarize_epoch_quality__mutmut_14, 
    'x_summarize_epoch_quality__mutmut_15': x_summarize_epoch_quality__mutmut_15, 
    'x_summarize_epoch_quality__mutmut_16': x_summarize_epoch_quality__mutmut_16, 
    'x_summarize_epoch_quality__mutmut_17': x_summarize_epoch_quality__mutmut_17, 
    'x_summarize_epoch_quality__mutmut_18': x_summarize_epoch_quality__mutmut_18, 
    'x_summarize_epoch_quality__mutmut_19': x_summarize_epoch_quality__mutmut_19, 
    'x_summarize_epoch_quality__mutmut_20': x_summarize_epoch_quality__mutmut_20, 
    'x_summarize_epoch_quality__mutmut_21': x_summarize_epoch_quality__mutmut_21, 
    'x_summarize_epoch_quality__mutmut_22': x_summarize_epoch_quality__mutmut_22, 
    'x_summarize_epoch_quality__mutmut_23': x_summarize_epoch_quality__mutmut_23, 
    'x_summarize_epoch_quality__mutmut_24': x_summarize_epoch_quality__mutmut_24, 
    'x_summarize_epoch_quality__mutmut_25': x_summarize_epoch_quality__mutmut_25, 
    'x_summarize_epoch_quality__mutmut_26': x_summarize_epoch_quality__mutmut_26, 
    'x_summarize_epoch_quality__mutmut_27': x_summarize_epoch_quality__mutmut_27, 
    'x_summarize_epoch_quality__mutmut_28': x_summarize_epoch_quality__mutmut_28, 
    'x_summarize_epoch_quality__mutmut_29': x_summarize_epoch_quality__mutmut_29, 
    'x_summarize_epoch_quality__mutmut_30': x_summarize_epoch_quality__mutmut_30, 
    'x_summarize_epoch_quality__mutmut_31': x_summarize_epoch_quality__mutmut_31, 
    'x_summarize_epoch_quality__mutmut_32': x_summarize_epoch_quality__mutmut_32, 
    'x_summarize_epoch_quality__mutmut_33': x_summarize_epoch_quality__mutmut_33, 
    'x_summarize_epoch_quality__mutmut_34': x_summarize_epoch_quality__mutmut_34, 
    'x_summarize_epoch_quality__mutmut_35': x_summarize_epoch_quality__mutmut_35, 
    'x_summarize_epoch_quality__mutmut_36': x_summarize_epoch_quality__mutmut_36, 
    'x_summarize_epoch_quality__mutmut_37': x_summarize_epoch_quality__mutmut_37, 
    'x_summarize_epoch_quality__mutmut_38': x_summarize_epoch_quality__mutmut_38, 
    'x_summarize_epoch_quality__mutmut_39': x_summarize_epoch_quality__mutmut_39, 
    'x_summarize_epoch_quality__mutmut_40': x_summarize_epoch_quality__mutmut_40, 
    'x_summarize_epoch_quality__mutmut_41': x_summarize_epoch_quality__mutmut_41, 
    'x_summarize_epoch_quality__mutmut_42': x_summarize_epoch_quality__mutmut_42, 
    'x_summarize_epoch_quality__mutmut_43': x_summarize_epoch_quality__mutmut_43, 
    'x_summarize_epoch_quality__mutmut_44': x_summarize_epoch_quality__mutmut_44, 
    'x_summarize_epoch_quality__mutmut_45': x_summarize_epoch_quality__mutmut_45, 
    'x_summarize_epoch_quality__mutmut_46': x_summarize_epoch_quality__mutmut_46, 
    'x_summarize_epoch_quality__mutmut_47': x_summarize_epoch_quality__mutmut_47, 
    'x_summarize_epoch_quality__mutmut_48': x_summarize_epoch_quality__mutmut_48, 
    'x_summarize_epoch_quality__mutmut_49': x_summarize_epoch_quality__mutmut_49, 
    'x_summarize_epoch_quality__mutmut_50': x_summarize_epoch_quality__mutmut_50, 
    'x_summarize_epoch_quality__mutmut_51': x_summarize_epoch_quality__mutmut_51, 
    'x_summarize_epoch_quality__mutmut_52': x_summarize_epoch_quality__mutmut_52, 
    'x_summarize_epoch_quality__mutmut_53': x_summarize_epoch_quality__mutmut_53, 
    'x_summarize_epoch_quality__mutmut_54': x_summarize_epoch_quality__mutmut_54, 
    'x_summarize_epoch_quality__mutmut_55': x_summarize_epoch_quality__mutmut_55, 
    'x_summarize_epoch_quality__mutmut_56': x_summarize_epoch_quality__mutmut_56, 
    'x_summarize_epoch_quality__mutmut_57': x_summarize_epoch_quality__mutmut_57
}

def summarize_epoch_quality(*args, **kwargs):
    result = _mutmut_trampoline(x_summarize_epoch_quality__mutmut_orig, x_summarize_epoch_quality__mutmut_mutants, args, kwargs)
    return result 

summarize_epoch_quality.__signature__ = _mutmut_signature(x_summarize_epoch_quality__mutmut_orig)
x_summarize_epoch_quality__mutmut_orig.__name__ = 'x_summarize_epoch_quality'


@dataclass
class ReportConfig:
    """Regroupe les paramètres nécessaires à la sérialisation du rapport."""

    # Stocke le chemin cible pour centraliser la sortie des rapports
    path: Path
    # Spécifie le format attendu pour contrôler la normalisation amont
    fmt: str = "json"


def x__ensure_label_alignment__mutmut_orig(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_1(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) == len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_2(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            None
        )


def x__ensure_label_alignment__mutmut_3(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                None
            )
        )


def x__ensure_label_alignment__mutmut_4(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "XXerrorXX": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_5(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "ERROR": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_6(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "XXLabel/event mismatchXX",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_7(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "label/event mismatch",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_8(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "LABEL/EVENT MISMATCH",
                    "expected_events": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_9(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "XXexpected_eventsXX": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_10(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "EXPECTED_EVENTS": len(epochs),
                    "labels": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_11(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "XXlabelsXX": len(motor_labels),
                }
            )
        )


def x__ensure_label_alignment__mutmut_12(epochs: mne.Epochs, motor_labels: List[str]) -> None:
    """Valide la correspondance entre événements MNE et labels utilisateur."""

    # Détecte immédiatement les décalages pour éviter des rapports incohérents
    if len(motor_labels) != len(epochs):
        # Fournit un rapport JSON pour aider à diagnostiquer le désalignement
        raise ValueError(
            json.dumps(
                {
                    "error": "Label/event mismatch",
                    "expected_events": len(epochs),
                    "LABELS": len(motor_labels),
                }
            )
        )

x__ensure_label_alignment__mutmut_mutants : ClassVar[MutantDict] = {
'x__ensure_label_alignment__mutmut_1': x__ensure_label_alignment__mutmut_1, 
    'x__ensure_label_alignment__mutmut_2': x__ensure_label_alignment__mutmut_2, 
    'x__ensure_label_alignment__mutmut_3': x__ensure_label_alignment__mutmut_3, 
    'x__ensure_label_alignment__mutmut_4': x__ensure_label_alignment__mutmut_4, 
    'x__ensure_label_alignment__mutmut_5': x__ensure_label_alignment__mutmut_5, 
    'x__ensure_label_alignment__mutmut_6': x__ensure_label_alignment__mutmut_6, 
    'x__ensure_label_alignment__mutmut_7': x__ensure_label_alignment__mutmut_7, 
    'x__ensure_label_alignment__mutmut_8': x__ensure_label_alignment__mutmut_8, 
    'x__ensure_label_alignment__mutmut_9': x__ensure_label_alignment__mutmut_9, 
    'x__ensure_label_alignment__mutmut_10': x__ensure_label_alignment__mutmut_10, 
    'x__ensure_label_alignment__mutmut_11': x__ensure_label_alignment__mutmut_11, 
    'x__ensure_label_alignment__mutmut_12': x__ensure_label_alignment__mutmut_12
}

def _ensure_label_alignment(*args, **kwargs):
    result = _mutmut_trampoline(x__ensure_label_alignment__mutmut_orig, x__ensure_label_alignment__mutmut_mutants, args, kwargs)
    return result 

_ensure_label_alignment.__signature__ = _mutmut_signature(x__ensure_label_alignment__mutmut_orig)
x__ensure_label_alignment__mutmut_orig.__name__ = 'x__ensure_label_alignment'


def x__apply_quality_control__mutmut_orig(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_1(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = None
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_2(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        None, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_3(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=None, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_4(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode=None
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_5(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_6(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_7(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_8(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="XXrejectXX"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_9(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="REJECT"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_10(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = None
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_11(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) & set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_12(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(None) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_13(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["XXartifactXX"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_14(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["ARTIFACT"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_15(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(None)
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_16(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["XXincompleteXX"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_17(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["INCOMPLETE"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_18(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = None
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_19(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(None) if idx not in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels


def x__apply_quality_control__mutmut_20(
    epochs: mne.Epochs,
    motor_labels: List[str],
    max_peak_to_peak: float,
) -> Tuple[mne.Epochs, Dict[str, List[int]], List[str]]:
    """Applique le rejet automatique et conserve les labels alignés."""

    # Filtre les artefacts et segments incomplets selon le seuil fourni
    cleaned_epochs, flagged = quality_control_epochs(
        epochs, max_peak_to_peak=max_peak_to_peak, mode="reject"
    )
    # Liste les indices rejetés pour synchroniser le filtrage des labels
    removed_indices = set(flagged["artifact"]) | set(flagged["incomplete"])
    # Retient les labels qui restent alignés avec les epochs conservées
    cleaned_labels = [
        label for idx, label in enumerate(motor_labels) if idx in removed_indices
    ]
    # Retourne les données nettoyées et les labels synchronisés
    return cleaned_epochs, flagged, cleaned_labels

x__apply_quality_control__mutmut_mutants : ClassVar[MutantDict] = {
'x__apply_quality_control__mutmut_1': x__apply_quality_control__mutmut_1, 
    'x__apply_quality_control__mutmut_2': x__apply_quality_control__mutmut_2, 
    'x__apply_quality_control__mutmut_3': x__apply_quality_control__mutmut_3, 
    'x__apply_quality_control__mutmut_4': x__apply_quality_control__mutmut_4, 
    'x__apply_quality_control__mutmut_5': x__apply_quality_control__mutmut_5, 
    'x__apply_quality_control__mutmut_6': x__apply_quality_control__mutmut_6, 
    'x__apply_quality_control__mutmut_7': x__apply_quality_control__mutmut_7, 
    'x__apply_quality_control__mutmut_8': x__apply_quality_control__mutmut_8, 
    'x__apply_quality_control__mutmut_9': x__apply_quality_control__mutmut_9, 
    'x__apply_quality_control__mutmut_10': x__apply_quality_control__mutmut_10, 
    'x__apply_quality_control__mutmut_11': x__apply_quality_control__mutmut_11, 
    'x__apply_quality_control__mutmut_12': x__apply_quality_control__mutmut_12, 
    'x__apply_quality_control__mutmut_13': x__apply_quality_control__mutmut_13, 
    'x__apply_quality_control__mutmut_14': x__apply_quality_control__mutmut_14, 
    'x__apply_quality_control__mutmut_15': x__apply_quality_control__mutmut_15, 
    'x__apply_quality_control__mutmut_16': x__apply_quality_control__mutmut_16, 
    'x__apply_quality_control__mutmut_17': x__apply_quality_control__mutmut_17, 
    'x__apply_quality_control__mutmut_18': x__apply_quality_control__mutmut_18, 
    'x__apply_quality_control__mutmut_19': x__apply_quality_control__mutmut_19, 
    'x__apply_quality_control__mutmut_20': x__apply_quality_control__mutmut_20
}

def _apply_quality_control(*args, **kwargs):
    result = _mutmut_trampoline(x__apply_quality_control__mutmut_orig, x__apply_quality_control__mutmut_mutants, args, kwargs)
    return result 

_apply_quality_control.__signature__ = _mutmut_signature(x__apply_quality_control__mutmut_orig)
x__apply_quality_control__mutmut_orig.__name__ = 'x__apply_quality_control'


def x__count_remaining_labels__mutmut_orig(cleaned_labels: List[str]) -> Dict[str, int]:
    """Calcule le nombre d'occurrences par classe attendue."""

    # Utilise les labels attendus pour assurer la cohérence des rapports
    return {label: cleaned_labels.count(label) for label in EXPECTED_LABELS}


def x__count_remaining_labels__mutmut_1(cleaned_labels: List[str]) -> Dict[str, int]:
    """Calcule le nombre d'occurrences par classe attendue."""

    # Utilise les labels attendus pour assurer la cohérence des rapports
    return {label: cleaned_labels.count(None) for label in EXPECTED_LABELS}

x__count_remaining_labels__mutmut_mutants : ClassVar[MutantDict] = {
'x__count_remaining_labels__mutmut_1': x__count_remaining_labels__mutmut_1
}

def _count_remaining_labels(*args, **kwargs):
    result = _mutmut_trampoline(x__count_remaining_labels__mutmut_orig, x__count_remaining_labels__mutmut_mutants, args, kwargs)
    return result 

_count_remaining_labels.__signature__ = _mutmut_signature(x__count_remaining_labels__mutmut_orig)
x__count_remaining_labels__mutmut_orig.__name__ = 'x__count_remaining_labels'


def x__assert_expected_labels_present__mutmut_orig(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_1(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = None
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_2(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count != 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_3(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 1]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_4(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            None
        )


def x__assert_expected_labels_present__mutmut_5(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                None
            )
        )


def x__assert_expected_labels_present__mutmut_6(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "XXerrorXX": "Missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_7(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "ERROR": "Missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_8(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "XXMissing labelsXX", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_9(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "missing labels", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_10(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "MISSING LABELS", "missing_labels": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_11(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "XXmissing_labelsXX": missing_labels}
            )
        )


def x__assert_expected_labels_present__mutmut_12(
    report: Dict[str, Any], counts: Dict[str, int]
) -> None:
    """Lève une erreur claire lorsque des classes manquent après nettoyage."""

    # Repère les classes dont le comptage tombe à zéro après filtrage
    missing_labels = [label for label, count in counts.items() if count == 0]
    # Remonte une erreur structurée pour faciliter le diagnostic utilisateur
    if missing_labels:
        # Injecte les labels manquants dans le rapport pour guider l'enquête
        raise ValueError(
            json.dumps(
                {**report, "error": "Missing labels", "MISSING_LABELS": missing_labels}
            )
        )

x__assert_expected_labels_present__mutmut_mutants : ClassVar[MutantDict] = {
'x__assert_expected_labels_present__mutmut_1': x__assert_expected_labels_present__mutmut_1, 
    'x__assert_expected_labels_present__mutmut_2': x__assert_expected_labels_present__mutmut_2, 
    'x__assert_expected_labels_present__mutmut_3': x__assert_expected_labels_present__mutmut_3, 
    'x__assert_expected_labels_present__mutmut_4': x__assert_expected_labels_present__mutmut_4, 
    'x__assert_expected_labels_present__mutmut_5': x__assert_expected_labels_present__mutmut_5, 
    'x__assert_expected_labels_present__mutmut_6': x__assert_expected_labels_present__mutmut_6, 
    'x__assert_expected_labels_present__mutmut_7': x__assert_expected_labels_present__mutmut_7, 
    'x__assert_expected_labels_present__mutmut_8': x__assert_expected_labels_present__mutmut_8, 
    'x__assert_expected_labels_present__mutmut_9': x__assert_expected_labels_present__mutmut_9, 
    'x__assert_expected_labels_present__mutmut_10': x__assert_expected_labels_present__mutmut_10, 
    'x__assert_expected_labels_present__mutmut_11': x__assert_expected_labels_present__mutmut_11, 
    'x__assert_expected_labels_present__mutmut_12': x__assert_expected_labels_present__mutmut_12
}

def _assert_expected_labels_present(*args, **kwargs):
    result = _mutmut_trampoline(x__assert_expected_labels_present__mutmut_orig, x__assert_expected_labels_present__mutmut_mutants, args, kwargs)
    return result 

_assert_expected_labels_present.__signature__ = _mutmut_signature(x__assert_expected_labels_present__mutmut_orig)
x__assert_expected_labels_present__mutmut_orig.__name__ = 'x__assert_expected_labels_present'


def x__build_epoch_report__mutmut_orig(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_1(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "XXsubjectXX": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_2(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "SUBJECT": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_3(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["XXsubjectXX"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_4(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["SUBJECT"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_5(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "XXrunXX": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_6(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "RUN": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_7(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["XXrunXX"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_8(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["RUN"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_9(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "XXtotal_epochs_beforeXX": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_10(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "TOTAL_EPOCHS_BEFORE": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_11(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "XXkept_epochsXX": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_12(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "KEPT_EPOCHS": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_13(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "XXcountsXX": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_14(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "COUNTS": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_15(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "XXanomaliesXX": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_16(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "ANOMALIES": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_17(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "XXartifactXX": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_18(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "ARTIFACT": flagged["artifact"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_19(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["XXartifactXX"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_20(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["ARTIFACT"],
            "incomplete": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_21(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "XXincompleteXX": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_22(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "INCOMPLETE": flagged["incomplete"],
        },
    }


def x__build_epoch_report__mutmut_23(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["XXincompleteXX"],
        },
    }


def x__build_epoch_report__mutmut_24(
    run_metadata: Mapping[str, str],
    total_epochs: int,
    kept_epochs: int,
    counts: Dict[str, int],
    flagged: Dict[str, List[int]],
) -> Dict[str, Any]:
    """Assemble les informations nécessaires au rapport qualité."""

    # Centralise les informations utiles pour le suivi par sujet et par run
    return {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs_before": total_epochs,
        "kept_epochs": kept_epochs,
        "counts": counts,
        "anomalies": {
            "artifact": flagged["artifact"],
            "incomplete": flagged["INCOMPLETE"],
        },
    }

x__build_epoch_report__mutmut_mutants : ClassVar[MutantDict] = {
'x__build_epoch_report__mutmut_1': x__build_epoch_report__mutmut_1, 
    'x__build_epoch_report__mutmut_2': x__build_epoch_report__mutmut_2, 
    'x__build_epoch_report__mutmut_3': x__build_epoch_report__mutmut_3, 
    'x__build_epoch_report__mutmut_4': x__build_epoch_report__mutmut_4, 
    'x__build_epoch_report__mutmut_5': x__build_epoch_report__mutmut_5, 
    'x__build_epoch_report__mutmut_6': x__build_epoch_report__mutmut_6, 
    'x__build_epoch_report__mutmut_7': x__build_epoch_report__mutmut_7, 
    'x__build_epoch_report__mutmut_8': x__build_epoch_report__mutmut_8, 
    'x__build_epoch_report__mutmut_9': x__build_epoch_report__mutmut_9, 
    'x__build_epoch_report__mutmut_10': x__build_epoch_report__mutmut_10, 
    'x__build_epoch_report__mutmut_11': x__build_epoch_report__mutmut_11, 
    'x__build_epoch_report__mutmut_12': x__build_epoch_report__mutmut_12, 
    'x__build_epoch_report__mutmut_13': x__build_epoch_report__mutmut_13, 
    'x__build_epoch_report__mutmut_14': x__build_epoch_report__mutmut_14, 
    'x__build_epoch_report__mutmut_15': x__build_epoch_report__mutmut_15, 
    'x__build_epoch_report__mutmut_16': x__build_epoch_report__mutmut_16, 
    'x__build_epoch_report__mutmut_17': x__build_epoch_report__mutmut_17, 
    'x__build_epoch_report__mutmut_18': x__build_epoch_report__mutmut_18, 
    'x__build_epoch_report__mutmut_19': x__build_epoch_report__mutmut_19, 
    'x__build_epoch_report__mutmut_20': x__build_epoch_report__mutmut_20, 
    'x__build_epoch_report__mutmut_21': x__build_epoch_report__mutmut_21, 
    'x__build_epoch_report__mutmut_22': x__build_epoch_report__mutmut_22, 
    'x__build_epoch_report__mutmut_23': x__build_epoch_report__mutmut_23, 
    'x__build_epoch_report__mutmut_24': x__build_epoch_report__mutmut_24
}

def _build_epoch_report(*args, **kwargs):
    result = _mutmut_trampoline(x__build_epoch_report__mutmut_orig, x__build_epoch_report__mutmut_mutants, args, kwargs)
    return result 

_build_epoch_report.__signature__ = _mutmut_signature(x__build_epoch_report__mutmut_orig)
x__build_epoch_report__mutmut_orig.__name__ = 'x__build_epoch_report'


def x__normalize_report_config__mutmut_orig(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_1(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = None
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_2(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.upper()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_3(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt == fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_4(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError(None)
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_5(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("XXfmt must be lowercaseXX")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_6(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("FMT MUST BE LOWERCASE")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_7(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_8(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"XXjsonXX", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_9(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"JSON", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_10(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "XXcsvXX"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_11(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "CSV"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_12(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError(None)
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_13(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("XXfmt must be either 'json' or 'csv'XX")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_14(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("FMT MUST BE EITHER 'JSON' OR 'CSV'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_15(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=None, fmt=fmt_normalized)


def x__normalize_report_config__mutmut_16(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, fmt=None)


def x__normalize_report_config__mutmut_17(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(fmt=fmt_normalized)


def x__normalize_report_config__mutmut_18(report_config: ReportConfig) -> ReportConfig:
    """Normalise la configuration pour sécuriser la sérialisation."""

    # Harmonise la casse pour éviter des variations inattendues dans les noms
    fmt_normalized = report_config.fmt.lower()
    # Vérifie que la casse initiale respecte la normalisation imposée
    if report_config.fmt != fmt_normalized:
        # Refuse une casse incohérente pour prévenir des collisions de fichiers
        raise ValueError("fmt must be lowercase")
    # Valide la liste des formats acceptés pour verrouiller l'API
    if fmt_normalized not in {"json", "csv"}:
        # Signale explicitement la liste des formats supportés par la pipeline
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Retourne la configuration avec un format uniformisé
    return ReportConfig(path=report_config.path, )

x__normalize_report_config__mutmut_mutants : ClassVar[MutantDict] = {
'x__normalize_report_config__mutmut_1': x__normalize_report_config__mutmut_1, 
    'x__normalize_report_config__mutmut_2': x__normalize_report_config__mutmut_2, 
    'x__normalize_report_config__mutmut_3': x__normalize_report_config__mutmut_3, 
    'x__normalize_report_config__mutmut_4': x__normalize_report_config__mutmut_4, 
    'x__normalize_report_config__mutmut_5': x__normalize_report_config__mutmut_5, 
    'x__normalize_report_config__mutmut_6': x__normalize_report_config__mutmut_6, 
    'x__normalize_report_config__mutmut_7': x__normalize_report_config__mutmut_7, 
    'x__normalize_report_config__mutmut_8': x__normalize_report_config__mutmut_8, 
    'x__normalize_report_config__mutmut_9': x__normalize_report_config__mutmut_9, 
    'x__normalize_report_config__mutmut_10': x__normalize_report_config__mutmut_10, 
    'x__normalize_report_config__mutmut_11': x__normalize_report_config__mutmut_11, 
    'x__normalize_report_config__mutmut_12': x__normalize_report_config__mutmut_12, 
    'x__normalize_report_config__mutmut_13': x__normalize_report_config__mutmut_13, 
    'x__normalize_report_config__mutmut_14': x__normalize_report_config__mutmut_14, 
    'x__normalize_report_config__mutmut_15': x__normalize_report_config__mutmut_15, 
    'x__normalize_report_config__mutmut_16': x__normalize_report_config__mutmut_16, 
    'x__normalize_report_config__mutmut_17': x__normalize_report_config__mutmut_17, 
    'x__normalize_report_config__mutmut_18': x__normalize_report_config__mutmut_18
}

def _normalize_report_config(*args, **kwargs):
    result = _mutmut_trampoline(x__normalize_report_config__mutmut_orig, x__normalize_report_config__mutmut_mutants, args, kwargs)
    return result 

_normalize_report_config.__signature__ = _mutmut_signature(x__normalize_report_config__mutmut_orig)
x__normalize_report_config__mutmut_orig.__name__ = 'x__normalize_report_config'


def x__write_json_report__mutmut_orig(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_1(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=None, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_2(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=None)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_3(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_4(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, )
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_5(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=False, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_6(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=False)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_7(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(None, encoding="utf-8")


def x__write_json_report__mutmut_8(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding=None)


def x__write_json_report__mutmut_9(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(encoding="utf-8")


def x__write_json_report__mutmut_10(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), )


def x__write_json_report__mutmut_11(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(None, indent=2), encoding="utf-8")


def x__write_json_report__mutmut_12(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=None), encoding="utf-8")


def x__write_json_report__mutmut_13(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(indent=2), encoding="utf-8")


def x__write_json_report__mutmut_14(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, ), encoding="utf-8")


def x__write_json_report__mutmut_15(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=3), encoding="utf-8")


def x__write_json_report__mutmut_16(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="XXutf-8XX")


def x__write_json_report__mutmut_17(report: Dict[str, Any], target: Path) -> None:
    """Écrit le rapport qualité au format JSON."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Sérialise le rapport avec indentation pour faciliter la lecture humaine
    target.write_text(json.dumps(report, indent=2), encoding="UTF-8")

x__write_json_report__mutmut_mutants : ClassVar[MutantDict] = {
'x__write_json_report__mutmut_1': x__write_json_report__mutmut_1, 
    'x__write_json_report__mutmut_2': x__write_json_report__mutmut_2, 
    'x__write_json_report__mutmut_3': x__write_json_report__mutmut_3, 
    'x__write_json_report__mutmut_4': x__write_json_report__mutmut_4, 
    'x__write_json_report__mutmut_5': x__write_json_report__mutmut_5, 
    'x__write_json_report__mutmut_6': x__write_json_report__mutmut_6, 
    'x__write_json_report__mutmut_7': x__write_json_report__mutmut_7, 
    'x__write_json_report__mutmut_8': x__write_json_report__mutmut_8, 
    'x__write_json_report__mutmut_9': x__write_json_report__mutmut_9, 
    'x__write_json_report__mutmut_10': x__write_json_report__mutmut_10, 
    'x__write_json_report__mutmut_11': x__write_json_report__mutmut_11, 
    'x__write_json_report__mutmut_12': x__write_json_report__mutmut_12, 
    'x__write_json_report__mutmut_13': x__write_json_report__mutmut_13, 
    'x__write_json_report__mutmut_14': x__write_json_report__mutmut_14, 
    'x__write_json_report__mutmut_15': x__write_json_report__mutmut_15, 
    'x__write_json_report__mutmut_16': x__write_json_report__mutmut_16, 
    'x__write_json_report__mutmut_17': x__write_json_report__mutmut_17
}

def _write_json_report(*args, **kwargs):
    result = _mutmut_trampoline(x__write_json_report__mutmut_orig, x__write_json_report__mutmut_mutants, args, kwargs)
    return result 

_write_json_report.__signature__ = _mutmut_signature(x__write_json_report__mutmut_orig)
x__write_json_report__mutmut_orig.__name__ = 'x__write_json_report'


def x__write_csv_report__mutmut_orig(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_1(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=None, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_2(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=None)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_3(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_4(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, )
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_5(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=False, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_6(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=False)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_7(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = None
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_8(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "XXsubject,run,total_epochs_before,kept_epochs,dropped_artifact,XX"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_9(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "SUBJECT,RUN,TOTAL_EPOCHS_BEFORE,KEPT_EPOCHS,DROPPED_ARTIFACT,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_10(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = None
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_11(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(None)
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_12(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = "XX;XX".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_13(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(None) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_14(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["XXartifactXX"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_15(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["ARTIFACT"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_16(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = None
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_17(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(None)
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_18(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = "XX;XX".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_19(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(None) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_20(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["XXincompleteXX"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_21(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["INCOMPLETE"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_22(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            None
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_23(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                None
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_24(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            "XX,XX".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_25(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["XXsubjectXX"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_26(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["SUBJECT"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_27(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["XXrunXX"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_28(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["RUN"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_29(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(None),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_30(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(None),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_31(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["XXkept_epochsXX"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_32(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["KEPT_EPOCHS"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_33(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(None),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_34(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text(None, encoding="utf-8")


def x__write_csv_report__mutmut_35(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding=None)


def x__write_csv_report__mutmut_36(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text(encoding="utf-8")


def x__write_csv_report__mutmut_37(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), )


def x__write_csv_report__mutmut_38(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(None), encoding="utf-8")


def x__write_csv_report__mutmut_39(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("XX\nXX".join(lines), encoding="utf-8")


def x__write_csv_report__mutmut_40(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="XXutf-8XX")


def x__write_csv_report__mutmut_41(
    report: Dict[str, Any],
    flagged: Dict[str, List[int]],
    counts: Dict[str, int],
    total_epochs: int,
    target: Path,
) -> None:
    """Écrit le rapport qualité au format CSV."""

    # Crée les dossiers parents pour éviter une erreur d'écriture
    target.parent.mkdir(parents=True, exist_ok=True)
    # Construit l'en-tête en exposant les colonnes critiques pour la QA
    lines = [
        "subject,run,total_epochs_before,kept_epochs,dropped_artifact,"
        "dropped_incomplete,label,count"
    ]
    # Génère une ligne par classe pour détailler les indices supprimés
    for label, count in counts.items():
        # Concatène les indices artefacts pour conserver la traçabilité
        artifact_indices = ";".join(str(idx) for idx in flagged["artifact"])
        # Concatène les indices incomplets pour un niveau de détail équivalent
        incomplete_indices = ";".join(str(idx) for idx in flagged["incomplete"])
        # Agrège la ligne finale pour la classe courante
        lines.append(
            ",".join(
                [
                    report["subject"],
                    report["run"],
                    str(total_epochs),
                    str(report["kept_epochs"]),
                    artifact_indices,
                    incomplete_indices,
                    label,
                    str(count),
                ]
            )
        )
    # Écrit le contenu complet pour permettre une inspection rapide
    target.write_text("\n".join(lines), encoding="UTF-8")

x__write_csv_report__mutmut_mutants : ClassVar[MutantDict] = {
'x__write_csv_report__mutmut_1': x__write_csv_report__mutmut_1, 
    'x__write_csv_report__mutmut_2': x__write_csv_report__mutmut_2, 
    'x__write_csv_report__mutmut_3': x__write_csv_report__mutmut_3, 
    'x__write_csv_report__mutmut_4': x__write_csv_report__mutmut_4, 
    'x__write_csv_report__mutmut_5': x__write_csv_report__mutmut_5, 
    'x__write_csv_report__mutmut_6': x__write_csv_report__mutmut_6, 
    'x__write_csv_report__mutmut_7': x__write_csv_report__mutmut_7, 
    'x__write_csv_report__mutmut_8': x__write_csv_report__mutmut_8, 
    'x__write_csv_report__mutmut_9': x__write_csv_report__mutmut_9, 
    'x__write_csv_report__mutmut_10': x__write_csv_report__mutmut_10, 
    'x__write_csv_report__mutmut_11': x__write_csv_report__mutmut_11, 
    'x__write_csv_report__mutmut_12': x__write_csv_report__mutmut_12, 
    'x__write_csv_report__mutmut_13': x__write_csv_report__mutmut_13, 
    'x__write_csv_report__mutmut_14': x__write_csv_report__mutmut_14, 
    'x__write_csv_report__mutmut_15': x__write_csv_report__mutmut_15, 
    'x__write_csv_report__mutmut_16': x__write_csv_report__mutmut_16, 
    'x__write_csv_report__mutmut_17': x__write_csv_report__mutmut_17, 
    'x__write_csv_report__mutmut_18': x__write_csv_report__mutmut_18, 
    'x__write_csv_report__mutmut_19': x__write_csv_report__mutmut_19, 
    'x__write_csv_report__mutmut_20': x__write_csv_report__mutmut_20, 
    'x__write_csv_report__mutmut_21': x__write_csv_report__mutmut_21, 
    'x__write_csv_report__mutmut_22': x__write_csv_report__mutmut_22, 
    'x__write_csv_report__mutmut_23': x__write_csv_report__mutmut_23, 
    'x__write_csv_report__mutmut_24': x__write_csv_report__mutmut_24, 
    'x__write_csv_report__mutmut_25': x__write_csv_report__mutmut_25, 
    'x__write_csv_report__mutmut_26': x__write_csv_report__mutmut_26, 
    'x__write_csv_report__mutmut_27': x__write_csv_report__mutmut_27, 
    'x__write_csv_report__mutmut_28': x__write_csv_report__mutmut_28, 
    'x__write_csv_report__mutmut_29': x__write_csv_report__mutmut_29, 
    'x__write_csv_report__mutmut_30': x__write_csv_report__mutmut_30, 
    'x__write_csv_report__mutmut_31': x__write_csv_report__mutmut_31, 
    'x__write_csv_report__mutmut_32': x__write_csv_report__mutmut_32, 
    'x__write_csv_report__mutmut_33': x__write_csv_report__mutmut_33, 
    'x__write_csv_report__mutmut_34': x__write_csv_report__mutmut_34, 
    'x__write_csv_report__mutmut_35': x__write_csv_report__mutmut_35, 
    'x__write_csv_report__mutmut_36': x__write_csv_report__mutmut_36, 
    'x__write_csv_report__mutmut_37': x__write_csv_report__mutmut_37, 
    'x__write_csv_report__mutmut_38': x__write_csv_report__mutmut_38, 
    'x__write_csv_report__mutmut_39': x__write_csv_report__mutmut_39, 
    'x__write_csv_report__mutmut_40': x__write_csv_report__mutmut_40, 
    'x__write_csv_report__mutmut_41': x__write_csv_report__mutmut_41
}

def _write_csv_report(*args, **kwargs):
    result = _mutmut_trampoline(x__write_csv_report__mutmut_orig, x__write_csv_report__mutmut_mutants, args, kwargs)
    return result 

_write_csv_report.__signature__ = _mutmut_signature(x__write_csv_report__mutmut_orig)
x__write_csv_report__mutmut_orig.__name__ = 'x__write_csv_report'


def x_report_epoch_anomalies__mutmut_orig(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_1(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(None, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_2(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, None)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_3(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_4(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, )
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_5(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = None
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_6(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        None, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_7(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, None, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_8(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, None
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_9(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_10(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_11(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_12(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = None
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_13(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(None)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_14(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = None
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_15(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        None, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_16(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, None, len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_17(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), None, counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_18(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), None, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_19(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, None
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_20(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_21(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_22(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_23(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_24(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_25(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(None, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_26(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, None)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_27(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_28(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, )
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_29(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = None
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_30(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(None)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_31(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt != "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_32(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "XXjsonXX":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_33(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "JSON":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_34(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(None, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_35(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, None)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_36(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_37(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, )
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_38(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(None, flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_39(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, None, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_40(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, None, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_41(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, None, normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_42(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), None)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_43(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(flagged, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_44(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, counts, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_45(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, len(epochs), normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_46(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, normalized_config.path)
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path


def x_report_epoch_anomalies__mutmut_47(
    epochs: mne.Epochs,
    motor_labels: List[str],
    run_metadata: Mapping[str, str],
    max_peak_to_peak: float,
    report_config: ReportConfig,
) -> Tuple[mne.Epochs, Dict[str, Any], Path]:
    """Reject corrupted epochs then persist a detailed quality report."""

    # Vérifie l'alignement entre événements et labels pour fiabiliser le rapport
    _ensure_label_alignment(epochs, motor_labels)
    # Applique le contrôle qualité afin de mesurer l'impact des anomalies
    cleaned_epochs, flagged, cleaned_labels = _apply_quality_control(
        epochs, motor_labels, max_peak_to_peak
    )
    # Calcule le décompte par classe après filtrage
    counts = _count_remaining_labels(cleaned_labels)
    # Assemble le rapport avec les métadonnées de run
    report = _build_epoch_report(
        run_metadata, len(epochs), len(cleaned_epochs), counts, flagged
    )
    # Valide la présence de toutes les classes attendues
    _assert_expected_labels_present(report, counts)
    # Normalise la configuration pour sécuriser le format
    normalized_config = _normalize_report_config(report_config)
    # Sérialise en JSON lorsque demandé
    if normalized_config.fmt == "json":
        # Écrit le rapport JSON prêt à l'usage
        _write_json_report(report, normalized_config.path)
        # Retourne les résultats accompagnés du chemin généré
        return cleaned_epochs, report, normalized_config.path
    # Sérialise en CSV lorsque demandé
    _write_csv_report(report, flagged, counts, len(epochs), )
    # Retourne les résultats accompagnés du chemin généré
    return cleaned_epochs, report, normalized_config.path

x_report_epoch_anomalies__mutmut_mutants : ClassVar[MutantDict] = {
'x_report_epoch_anomalies__mutmut_1': x_report_epoch_anomalies__mutmut_1, 
    'x_report_epoch_anomalies__mutmut_2': x_report_epoch_anomalies__mutmut_2, 
    'x_report_epoch_anomalies__mutmut_3': x_report_epoch_anomalies__mutmut_3, 
    'x_report_epoch_anomalies__mutmut_4': x_report_epoch_anomalies__mutmut_4, 
    'x_report_epoch_anomalies__mutmut_5': x_report_epoch_anomalies__mutmut_5, 
    'x_report_epoch_anomalies__mutmut_6': x_report_epoch_anomalies__mutmut_6, 
    'x_report_epoch_anomalies__mutmut_7': x_report_epoch_anomalies__mutmut_7, 
    'x_report_epoch_anomalies__mutmut_8': x_report_epoch_anomalies__mutmut_8, 
    'x_report_epoch_anomalies__mutmut_9': x_report_epoch_anomalies__mutmut_9, 
    'x_report_epoch_anomalies__mutmut_10': x_report_epoch_anomalies__mutmut_10, 
    'x_report_epoch_anomalies__mutmut_11': x_report_epoch_anomalies__mutmut_11, 
    'x_report_epoch_anomalies__mutmut_12': x_report_epoch_anomalies__mutmut_12, 
    'x_report_epoch_anomalies__mutmut_13': x_report_epoch_anomalies__mutmut_13, 
    'x_report_epoch_anomalies__mutmut_14': x_report_epoch_anomalies__mutmut_14, 
    'x_report_epoch_anomalies__mutmut_15': x_report_epoch_anomalies__mutmut_15, 
    'x_report_epoch_anomalies__mutmut_16': x_report_epoch_anomalies__mutmut_16, 
    'x_report_epoch_anomalies__mutmut_17': x_report_epoch_anomalies__mutmut_17, 
    'x_report_epoch_anomalies__mutmut_18': x_report_epoch_anomalies__mutmut_18, 
    'x_report_epoch_anomalies__mutmut_19': x_report_epoch_anomalies__mutmut_19, 
    'x_report_epoch_anomalies__mutmut_20': x_report_epoch_anomalies__mutmut_20, 
    'x_report_epoch_anomalies__mutmut_21': x_report_epoch_anomalies__mutmut_21, 
    'x_report_epoch_anomalies__mutmut_22': x_report_epoch_anomalies__mutmut_22, 
    'x_report_epoch_anomalies__mutmut_23': x_report_epoch_anomalies__mutmut_23, 
    'x_report_epoch_anomalies__mutmut_24': x_report_epoch_anomalies__mutmut_24, 
    'x_report_epoch_anomalies__mutmut_25': x_report_epoch_anomalies__mutmut_25, 
    'x_report_epoch_anomalies__mutmut_26': x_report_epoch_anomalies__mutmut_26, 
    'x_report_epoch_anomalies__mutmut_27': x_report_epoch_anomalies__mutmut_27, 
    'x_report_epoch_anomalies__mutmut_28': x_report_epoch_anomalies__mutmut_28, 
    'x_report_epoch_anomalies__mutmut_29': x_report_epoch_anomalies__mutmut_29, 
    'x_report_epoch_anomalies__mutmut_30': x_report_epoch_anomalies__mutmut_30, 
    'x_report_epoch_anomalies__mutmut_31': x_report_epoch_anomalies__mutmut_31, 
    'x_report_epoch_anomalies__mutmut_32': x_report_epoch_anomalies__mutmut_32, 
    'x_report_epoch_anomalies__mutmut_33': x_report_epoch_anomalies__mutmut_33, 
    'x_report_epoch_anomalies__mutmut_34': x_report_epoch_anomalies__mutmut_34, 
    'x_report_epoch_anomalies__mutmut_35': x_report_epoch_anomalies__mutmut_35, 
    'x_report_epoch_anomalies__mutmut_36': x_report_epoch_anomalies__mutmut_36, 
    'x_report_epoch_anomalies__mutmut_37': x_report_epoch_anomalies__mutmut_37, 
    'x_report_epoch_anomalies__mutmut_38': x_report_epoch_anomalies__mutmut_38, 
    'x_report_epoch_anomalies__mutmut_39': x_report_epoch_anomalies__mutmut_39, 
    'x_report_epoch_anomalies__mutmut_40': x_report_epoch_anomalies__mutmut_40, 
    'x_report_epoch_anomalies__mutmut_41': x_report_epoch_anomalies__mutmut_41, 
    'x_report_epoch_anomalies__mutmut_42': x_report_epoch_anomalies__mutmut_42, 
    'x_report_epoch_anomalies__mutmut_43': x_report_epoch_anomalies__mutmut_43, 
    'x_report_epoch_anomalies__mutmut_44': x_report_epoch_anomalies__mutmut_44, 
    'x_report_epoch_anomalies__mutmut_45': x_report_epoch_anomalies__mutmut_45, 
    'x_report_epoch_anomalies__mutmut_46': x_report_epoch_anomalies__mutmut_46, 
    'x_report_epoch_anomalies__mutmut_47': x_report_epoch_anomalies__mutmut_47
}

def report_epoch_anomalies(*args, **kwargs):
    result = _mutmut_trampoline(x_report_epoch_anomalies__mutmut_orig, x_report_epoch_anomalies__mutmut_mutants, args, kwargs)
    return result 

report_epoch_anomalies.__signature__ = _mutmut_signature(x_report_epoch_anomalies__mutmut_orig)
x_report_epoch_anomalies__mutmut_orig.__name__ = 'x_report_epoch_anomalies'


def x_detect_artifacts__mutmut_orig(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_1(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "XXrejectXX",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_2(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "REJECT",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_3(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = None
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_4(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(None, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_5(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=None)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_6(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_7(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, )
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_8(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = None
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_9(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(None) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_10(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) >= amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_11(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = None
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_12(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(None, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_13(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=None)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_14(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_15(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, )
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_16(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=1)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_17(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = None
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_18(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample >= variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_19(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = None
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_20(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask & variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_21(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = None
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_22(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=None)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_23(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=1)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_24(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode != "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_25(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "XXrejectXX":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_26(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "REJECT":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_27(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = None
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_28(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_29(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode != "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_30(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "XXinterpolateXX":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_31(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "INTERPOLATE":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_32(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = None
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_33(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(None)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_34(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_35(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) != 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_36(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 1:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_37(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = None
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_38(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(None)
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_39(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[2])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_40(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(None):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_41(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[1]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_42(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = None
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_43(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = None
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_44(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                None, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_45(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, None, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_46(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, None
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_47(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_48(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_49(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("mode must be either 'reject' or 'interpolate'")


def x_detect_artifacts__mutmut_50(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError(None)


def x_detect_artifacts__mutmut_51(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("XXmode must be either 'reject' or 'interpolate'XX")


def x_detect_artifacts__mutmut_52(
    signal: np.ndarray,
    amplitude_threshold: float,
    variance_threshold: float,
    mode: str = "reject",
) -> Tuple[np.ndarray, np.ndarray]:
    """Detect amplitude or variance artifacts and reject or interpolate."""

    # Copie le signal en flottants pour uniformiser la suite du traitement
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Calcule l'amplitude absolue pour repérer les excursions extrêmes
    amplitude_mask = np.abs(safe_signal) > amplitude_threshold
    # Calcule la variance par échantillon pour capturer les déviations croisées
    variance_per_sample = np.var(safe_signal, axis=0)
    # Étend le masque de variance à toutes les voies pour uniformiser le traitement
    variance_mask = variance_per_sample > variance_threshold
    # Combine les critères pour identifier chaque échantillon contaminé
    combined_mask = amplitude_mask | variance_mask
    # Déduit un masque global indiquant les colonnes à exclure ou corriger
    sample_mask = combined_mask.any(axis=0)
    # Traite la branche de rejet pour supprimer les échantillons fautifs
    if mode == "reject":
        # Construit un masque de conservation pour filtrer les colonnes sûres
        keep_mask = ~sample_mask
        # Supprime les colonnes contaminées afin de stabiliser l'apprentissage
        return safe_signal[:, keep_mask], sample_mask
    # Traite la branche d'interpolation pour conserver la structure temporelle
    if mode == "interpolate":
        # Localise les indices sûrs pour guider l'interpolation linéaire
        valid_indices = np.flatnonzero(~sample_mask)
        # Traite l'absence totale de points fiables en conservant le signal brut
        if len(valid_indices) == 0:
            # Retourne le signal initial lorsque l'interpolation est impossible
            return safe_signal, sample_mask
        # Prépare un vecteur d'indices cible pour reconstituer chaque colonne
        target_indices = np.arange(safe_signal.shape[1])
        # Itère sur chaque canal pour appliquer une interpolation indépendante
        for channel in range(safe_signal.shape[0]):
            # Extrait les valeurs sûres du canal courant pour alimenter l'interpolation
            valid_values = safe_signal[channel, valid_indices]
            # Remplace les échantillons fautifs par l'interpolation linéaire
            safe_signal[channel] = np.interp(
                target_indices, valid_indices, valid_values
            )
        # Retourne le signal interpolé pour préserver la longueur temporelle
        return safe_signal, sample_mask
    # Lève une erreur explicite pour les modes non supportés
    raise ValueError("MODE MUST BE EITHER 'REJECT' OR 'INTERPOLATE'")

x_detect_artifacts__mutmut_mutants : ClassVar[MutantDict] = {
'x_detect_artifacts__mutmut_1': x_detect_artifacts__mutmut_1, 
    'x_detect_artifacts__mutmut_2': x_detect_artifacts__mutmut_2, 
    'x_detect_artifacts__mutmut_3': x_detect_artifacts__mutmut_3, 
    'x_detect_artifacts__mutmut_4': x_detect_artifacts__mutmut_4, 
    'x_detect_artifacts__mutmut_5': x_detect_artifacts__mutmut_5, 
    'x_detect_artifacts__mutmut_6': x_detect_artifacts__mutmut_6, 
    'x_detect_artifacts__mutmut_7': x_detect_artifacts__mutmut_7, 
    'x_detect_artifacts__mutmut_8': x_detect_artifacts__mutmut_8, 
    'x_detect_artifacts__mutmut_9': x_detect_artifacts__mutmut_9, 
    'x_detect_artifacts__mutmut_10': x_detect_artifacts__mutmut_10, 
    'x_detect_artifacts__mutmut_11': x_detect_artifacts__mutmut_11, 
    'x_detect_artifacts__mutmut_12': x_detect_artifacts__mutmut_12, 
    'x_detect_artifacts__mutmut_13': x_detect_artifacts__mutmut_13, 
    'x_detect_artifacts__mutmut_14': x_detect_artifacts__mutmut_14, 
    'x_detect_artifacts__mutmut_15': x_detect_artifacts__mutmut_15, 
    'x_detect_artifacts__mutmut_16': x_detect_artifacts__mutmut_16, 
    'x_detect_artifacts__mutmut_17': x_detect_artifacts__mutmut_17, 
    'x_detect_artifacts__mutmut_18': x_detect_artifacts__mutmut_18, 
    'x_detect_artifacts__mutmut_19': x_detect_artifacts__mutmut_19, 
    'x_detect_artifacts__mutmut_20': x_detect_artifacts__mutmut_20, 
    'x_detect_artifacts__mutmut_21': x_detect_artifacts__mutmut_21, 
    'x_detect_artifacts__mutmut_22': x_detect_artifacts__mutmut_22, 
    'x_detect_artifacts__mutmut_23': x_detect_artifacts__mutmut_23, 
    'x_detect_artifacts__mutmut_24': x_detect_artifacts__mutmut_24, 
    'x_detect_artifacts__mutmut_25': x_detect_artifacts__mutmut_25, 
    'x_detect_artifacts__mutmut_26': x_detect_artifacts__mutmut_26, 
    'x_detect_artifacts__mutmut_27': x_detect_artifacts__mutmut_27, 
    'x_detect_artifacts__mutmut_28': x_detect_artifacts__mutmut_28, 
    'x_detect_artifacts__mutmut_29': x_detect_artifacts__mutmut_29, 
    'x_detect_artifacts__mutmut_30': x_detect_artifacts__mutmut_30, 
    'x_detect_artifacts__mutmut_31': x_detect_artifacts__mutmut_31, 
    'x_detect_artifacts__mutmut_32': x_detect_artifacts__mutmut_32, 
    'x_detect_artifacts__mutmut_33': x_detect_artifacts__mutmut_33, 
    'x_detect_artifacts__mutmut_34': x_detect_artifacts__mutmut_34, 
    'x_detect_artifacts__mutmut_35': x_detect_artifacts__mutmut_35, 
    'x_detect_artifacts__mutmut_36': x_detect_artifacts__mutmut_36, 
    'x_detect_artifacts__mutmut_37': x_detect_artifacts__mutmut_37, 
    'x_detect_artifacts__mutmut_38': x_detect_artifacts__mutmut_38, 
    'x_detect_artifacts__mutmut_39': x_detect_artifacts__mutmut_39, 
    'x_detect_artifacts__mutmut_40': x_detect_artifacts__mutmut_40, 
    'x_detect_artifacts__mutmut_41': x_detect_artifacts__mutmut_41, 
    'x_detect_artifacts__mutmut_42': x_detect_artifacts__mutmut_42, 
    'x_detect_artifacts__mutmut_43': x_detect_artifacts__mutmut_43, 
    'x_detect_artifacts__mutmut_44': x_detect_artifacts__mutmut_44, 
    'x_detect_artifacts__mutmut_45': x_detect_artifacts__mutmut_45, 
    'x_detect_artifacts__mutmut_46': x_detect_artifacts__mutmut_46, 
    'x_detect_artifacts__mutmut_47': x_detect_artifacts__mutmut_47, 
    'x_detect_artifacts__mutmut_48': x_detect_artifacts__mutmut_48, 
    'x_detect_artifacts__mutmut_49': x_detect_artifacts__mutmut_49, 
    'x_detect_artifacts__mutmut_50': x_detect_artifacts__mutmut_50, 
    'x_detect_artifacts__mutmut_51': x_detect_artifacts__mutmut_51, 
    'x_detect_artifacts__mutmut_52': x_detect_artifacts__mutmut_52
}

def detect_artifacts(*args, **kwargs):
    result = _mutmut_trampoline(x_detect_artifacts__mutmut_orig, x_detect_artifacts__mutmut_mutants, args, kwargs)
    return result 

detect_artifacts.__signature__ = _mutmut_signature(x_detect_artifacts__mutmut_orig)
x_detect_artifacts__mutmut_orig.__name__ = 'x_detect_artifacts'


def x_normalize_channels__mutmut_orig(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_1(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = None
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_2(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(None, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_3(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=None)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_4(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_5(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, )
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_6(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = None
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_7(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.upper()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_8(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method != "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_9(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "XXzscoreXX":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_10(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "ZSCORE":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_11(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = None
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_12(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(None, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_13(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=None, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_14(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=None)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_15(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_16(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_17(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, )
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_18(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=2, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_19(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=False)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_20(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = None
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_21(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) - epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_22(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(None, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_23(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=None, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_24(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=None) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_25(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_26(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_27(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, ) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_28(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=2, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_29(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=False) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_30(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = None
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_31(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            None, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_32(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=None
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_33(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_34(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_35(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) * std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_36(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal + mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_37(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method != "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_38(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "XXrobustXX":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_39(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "ROBUST":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_40(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = None
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_41(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(None, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_42(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=None, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_43(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=None)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_44(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_45(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_46(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, )
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_47(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=2, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_48(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=False)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_49(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = None
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_50(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True) - epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_51(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True) + np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_52(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(None, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_53(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, None, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_54(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=None, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_55(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=None)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_56(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_57(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_58(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_59(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, )
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_60(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 76, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_61(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=2, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_62(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=False)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_63(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(None, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_64(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, None, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_65(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=None, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_66(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=None)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_67(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_68(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_69(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_70(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, )
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_71(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 26, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_72(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=2, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_73(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=False)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_74(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = None
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_75(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            None, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_76(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=None
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_77(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_78(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_79(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) * iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_80(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal + median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("method must be either 'zscore' or 'robust'")


def x_normalize_channels__mutmut_81(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError(None)


def x_normalize_channels__mutmut_82(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("XXmethod must be either 'zscore' or 'robust'XX")


def x_normalize_channels__mutmut_83(
    signal: NDArray[np.floating[Any]],
    method: str = DEFAULT_NORMALIZE_METHOD,
    epsilon: float = DEFAULT_NORMALIZE_EPSILON,
) -> NDArray[np.floating[Any]]:
    """Normalize each channel using z-score or robust statistics."""

    # Copie le signal en flottants pour garantir des sorties typées
    safe_signal: NDArray[np.floating[Any]] = np.asarray(signal, dtype=float)
    # Uniformise le nom de méthode pour éviter les confusions de casse
    normalized_method = method.lower()
    # Applique une normalisation z-score basée sur moyenne et écart-type
    if normalized_method == "zscore":
        # Calcule la moyenne par canal pour centrer la distribution
        mean_per_channel = np.mean(safe_signal, axis=1, keepdims=True)
        # Calcule l'écart-type par canal et ajoute epsilon pour la stabilité
        std_per_channel = np.std(safe_signal, axis=1, keepdims=True) + epsilon
        # Centre et réduit chaque canal pour homogénéiser les amplitudes
        result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - mean_per_channel) / std_per_channel, dtype=float
        )
        # Retourne l'étalonnage z-score avec un type numpy explicite
        return result
    # Applique une normalisation robuste basée sur médiane et IQR
    if normalized_method == "robust":
        # Calcule la médiane par canal pour neutraliser les valeurs extrêmes
        median_per_channel = np.median(safe_signal, axis=1, keepdims=True)
        # Calcule l'IQR par canal et ajoute epsilon pour éviter les divisions nulles
        iqr_per_channel = (
            np.percentile(safe_signal, 75, axis=1, keepdims=True)
            - np.percentile(safe_signal, 25, axis=1, keepdims=True)
            + epsilon
        )
        # Centre et met à l'échelle chaque canal selon les statistiques robustes
        robust_result: NDArray[np.floating[Any]] = np.asarray(
            (safe_signal - median_per_channel) / iqr_per_channel, dtype=float
        )
        # Retourne la version robuste typée pour mypy et les tests
        return robust_result
    # Lève une erreur explicite pour les méthodes non supportées
    raise ValueError("METHOD MUST BE EITHER 'ZSCORE' OR 'ROBUST'")

x_normalize_channels__mutmut_mutants : ClassVar[MutantDict] = {
'x_normalize_channels__mutmut_1': x_normalize_channels__mutmut_1, 
    'x_normalize_channels__mutmut_2': x_normalize_channels__mutmut_2, 
    'x_normalize_channels__mutmut_3': x_normalize_channels__mutmut_3, 
    'x_normalize_channels__mutmut_4': x_normalize_channels__mutmut_4, 
    'x_normalize_channels__mutmut_5': x_normalize_channels__mutmut_5, 
    'x_normalize_channels__mutmut_6': x_normalize_channels__mutmut_6, 
    'x_normalize_channels__mutmut_7': x_normalize_channels__mutmut_7, 
    'x_normalize_channels__mutmut_8': x_normalize_channels__mutmut_8, 
    'x_normalize_channels__mutmut_9': x_normalize_channels__mutmut_9, 
    'x_normalize_channels__mutmut_10': x_normalize_channels__mutmut_10, 
    'x_normalize_channels__mutmut_11': x_normalize_channels__mutmut_11, 
    'x_normalize_channels__mutmut_12': x_normalize_channels__mutmut_12, 
    'x_normalize_channels__mutmut_13': x_normalize_channels__mutmut_13, 
    'x_normalize_channels__mutmut_14': x_normalize_channels__mutmut_14, 
    'x_normalize_channels__mutmut_15': x_normalize_channels__mutmut_15, 
    'x_normalize_channels__mutmut_16': x_normalize_channels__mutmut_16, 
    'x_normalize_channels__mutmut_17': x_normalize_channels__mutmut_17, 
    'x_normalize_channels__mutmut_18': x_normalize_channels__mutmut_18, 
    'x_normalize_channels__mutmut_19': x_normalize_channels__mutmut_19, 
    'x_normalize_channels__mutmut_20': x_normalize_channels__mutmut_20, 
    'x_normalize_channels__mutmut_21': x_normalize_channels__mutmut_21, 
    'x_normalize_channels__mutmut_22': x_normalize_channels__mutmut_22, 
    'x_normalize_channels__mutmut_23': x_normalize_channels__mutmut_23, 
    'x_normalize_channels__mutmut_24': x_normalize_channels__mutmut_24, 
    'x_normalize_channels__mutmut_25': x_normalize_channels__mutmut_25, 
    'x_normalize_channels__mutmut_26': x_normalize_channels__mutmut_26, 
    'x_normalize_channels__mutmut_27': x_normalize_channels__mutmut_27, 
    'x_normalize_channels__mutmut_28': x_normalize_channels__mutmut_28, 
    'x_normalize_channels__mutmut_29': x_normalize_channels__mutmut_29, 
    'x_normalize_channels__mutmut_30': x_normalize_channels__mutmut_30, 
    'x_normalize_channels__mutmut_31': x_normalize_channels__mutmut_31, 
    'x_normalize_channels__mutmut_32': x_normalize_channels__mutmut_32, 
    'x_normalize_channels__mutmut_33': x_normalize_channels__mutmut_33, 
    'x_normalize_channels__mutmut_34': x_normalize_channels__mutmut_34, 
    'x_normalize_channels__mutmut_35': x_normalize_channels__mutmut_35, 
    'x_normalize_channels__mutmut_36': x_normalize_channels__mutmut_36, 
    'x_normalize_channels__mutmut_37': x_normalize_channels__mutmut_37, 
    'x_normalize_channels__mutmut_38': x_normalize_channels__mutmut_38, 
    'x_normalize_channels__mutmut_39': x_normalize_channels__mutmut_39, 
    'x_normalize_channels__mutmut_40': x_normalize_channels__mutmut_40, 
    'x_normalize_channels__mutmut_41': x_normalize_channels__mutmut_41, 
    'x_normalize_channels__mutmut_42': x_normalize_channels__mutmut_42, 
    'x_normalize_channels__mutmut_43': x_normalize_channels__mutmut_43, 
    'x_normalize_channels__mutmut_44': x_normalize_channels__mutmut_44, 
    'x_normalize_channels__mutmut_45': x_normalize_channels__mutmut_45, 
    'x_normalize_channels__mutmut_46': x_normalize_channels__mutmut_46, 
    'x_normalize_channels__mutmut_47': x_normalize_channels__mutmut_47, 
    'x_normalize_channels__mutmut_48': x_normalize_channels__mutmut_48, 
    'x_normalize_channels__mutmut_49': x_normalize_channels__mutmut_49, 
    'x_normalize_channels__mutmut_50': x_normalize_channels__mutmut_50, 
    'x_normalize_channels__mutmut_51': x_normalize_channels__mutmut_51, 
    'x_normalize_channels__mutmut_52': x_normalize_channels__mutmut_52, 
    'x_normalize_channels__mutmut_53': x_normalize_channels__mutmut_53, 
    'x_normalize_channels__mutmut_54': x_normalize_channels__mutmut_54, 
    'x_normalize_channels__mutmut_55': x_normalize_channels__mutmut_55, 
    'x_normalize_channels__mutmut_56': x_normalize_channels__mutmut_56, 
    'x_normalize_channels__mutmut_57': x_normalize_channels__mutmut_57, 
    'x_normalize_channels__mutmut_58': x_normalize_channels__mutmut_58, 
    'x_normalize_channels__mutmut_59': x_normalize_channels__mutmut_59, 
    'x_normalize_channels__mutmut_60': x_normalize_channels__mutmut_60, 
    'x_normalize_channels__mutmut_61': x_normalize_channels__mutmut_61, 
    'x_normalize_channels__mutmut_62': x_normalize_channels__mutmut_62, 
    'x_normalize_channels__mutmut_63': x_normalize_channels__mutmut_63, 
    'x_normalize_channels__mutmut_64': x_normalize_channels__mutmut_64, 
    'x_normalize_channels__mutmut_65': x_normalize_channels__mutmut_65, 
    'x_normalize_channels__mutmut_66': x_normalize_channels__mutmut_66, 
    'x_normalize_channels__mutmut_67': x_normalize_channels__mutmut_67, 
    'x_normalize_channels__mutmut_68': x_normalize_channels__mutmut_68, 
    'x_normalize_channels__mutmut_69': x_normalize_channels__mutmut_69, 
    'x_normalize_channels__mutmut_70': x_normalize_channels__mutmut_70, 
    'x_normalize_channels__mutmut_71': x_normalize_channels__mutmut_71, 
    'x_normalize_channels__mutmut_72': x_normalize_channels__mutmut_72, 
    'x_normalize_channels__mutmut_73': x_normalize_channels__mutmut_73, 
    'x_normalize_channels__mutmut_74': x_normalize_channels__mutmut_74, 
    'x_normalize_channels__mutmut_75': x_normalize_channels__mutmut_75, 
    'x_normalize_channels__mutmut_76': x_normalize_channels__mutmut_76, 
    'x_normalize_channels__mutmut_77': x_normalize_channels__mutmut_77, 
    'x_normalize_channels__mutmut_78': x_normalize_channels__mutmut_78, 
    'x_normalize_channels__mutmut_79': x_normalize_channels__mutmut_79, 
    'x_normalize_channels__mutmut_80': x_normalize_channels__mutmut_80, 
    'x_normalize_channels__mutmut_81': x_normalize_channels__mutmut_81, 
    'x_normalize_channels__mutmut_82': x_normalize_channels__mutmut_82, 
    'x_normalize_channels__mutmut_83': x_normalize_channels__mutmut_83
}

def normalize_channels(*args, **kwargs):
    result = _mutmut_trampoline(x_normalize_channels__mutmut_orig, x_normalize_channels__mutmut_mutants, args, kwargs)
    return result 

normalize_channels.__signature__ = _mutmut_signature(x_normalize_channels__mutmut_orig)
x_normalize_channels__mutmut_orig.__name__ = 'x_normalize_channels'


def x__build_file_entry__mutmut_orig(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_1(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = None
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_2(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = None
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_3(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(None).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_4(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = None
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_5(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(None)
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_6(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(None))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_7(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = None
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_8(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None and expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_9(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is not None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_10(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(None) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_11(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) != file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_12(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "XXpathXX": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_13(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "PATH": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_14(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "XXsizeXX": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_15(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "SIZE": size_bytes,
        "sha256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_16(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "XXsha256XX": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_17(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "SHA256": file_hash,
        "hash_ok": hash_match,
    }


def x__build_file_entry__mutmut_18(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "XXhash_okXX": hash_match,
    }


def x__build_file_entry__mutmut_19(
    data_root: Path,
    file_path: Path,
    expected_hashes: Mapping[str, str] | None,
) -> Dict[str, object]:
    """Compose a report entry for a single EDF file."""

    # Record the file size to detect incomplete downloads
    size_bytes = file_path.stat().st_size
    # Compute SHA256 only when reference hashes are provided for comparison
    file_hash = (
        hashlib.sha256(file_path.read_bytes()).hexdigest() if expected_hashes else None
    )
    # Build a stable relative key to align with expected hashes mapping
    rel_key = str(file_path.relative_to(data_root))
    # Evaluate hash parity when expectations exist to surface corruption
    hash_match = expected_hashes is None or expected_hashes.get(rel_key) == file_hash
    # Return a structured entry consumable by integrity reports
    return {
        "path": rel_key,
        "size": size_bytes,
        "sha256": file_hash,
        "HASH_OK": hash_match,
    }

x__build_file_entry__mutmut_mutants : ClassVar[MutantDict] = {
'x__build_file_entry__mutmut_1': x__build_file_entry__mutmut_1, 
    'x__build_file_entry__mutmut_2': x__build_file_entry__mutmut_2, 
    'x__build_file_entry__mutmut_3': x__build_file_entry__mutmut_3, 
    'x__build_file_entry__mutmut_4': x__build_file_entry__mutmut_4, 
    'x__build_file_entry__mutmut_5': x__build_file_entry__mutmut_5, 
    'x__build_file_entry__mutmut_6': x__build_file_entry__mutmut_6, 
    'x__build_file_entry__mutmut_7': x__build_file_entry__mutmut_7, 
    'x__build_file_entry__mutmut_8': x__build_file_entry__mutmut_8, 
    'x__build_file_entry__mutmut_9': x__build_file_entry__mutmut_9, 
    'x__build_file_entry__mutmut_10': x__build_file_entry__mutmut_10, 
    'x__build_file_entry__mutmut_11': x__build_file_entry__mutmut_11, 
    'x__build_file_entry__mutmut_12': x__build_file_entry__mutmut_12, 
    'x__build_file_entry__mutmut_13': x__build_file_entry__mutmut_13, 
    'x__build_file_entry__mutmut_14': x__build_file_entry__mutmut_14, 
    'x__build_file_entry__mutmut_15': x__build_file_entry__mutmut_15, 
    'x__build_file_entry__mutmut_16': x__build_file_entry__mutmut_16, 
    'x__build_file_entry__mutmut_17': x__build_file_entry__mutmut_17, 
    'x__build_file_entry__mutmut_18': x__build_file_entry__mutmut_18, 
    'x__build_file_entry__mutmut_19': x__build_file_entry__mutmut_19
}

def _build_file_entry(*args, **kwargs):
    result = _mutmut_trampoline(x__build_file_entry__mutmut_orig, x__build_file_entry__mutmut_mutants, args, kwargs)
    return result 

_build_file_entry.__signature__ = _mutmut_signature(x__build_file_entry__mutmut_orig)
x__build_file_entry__mutmut_orig.__name__ = 'x__build_file_entry'


def x__collect_run_counts__mutmut_orig(data_root: Path) -> Dict[str, int]:
    """Count EDF runs per subject directory."""

    # Initialize dictionary to aggregate run totals by subject
    subject_counts: Dict[str, int] = {}
    # Iterate over immediate child directories representing subjects
    for subject_dir in data_root.iterdir():
        # Ignore non-directories to focus exclusively on subject folders
        if not subject_dir.is_dir():
            # Continue scanning when encountering stray files at the root
            continue
        # Count EDF files within the subject directory to quantify runs
        run_count = len(list(subject_dir.glob("*.edf")))
        # Persist the count for downstream comparison against expectations
        subject_counts[subject_dir.name] = run_count
    # Return all computed run counts for further validation steps
    return subject_counts


def x__collect_run_counts__mutmut_1(data_root: Path) -> Dict[str, int]:
    """Count EDF runs per subject directory."""

    # Initialize dictionary to aggregate run totals by subject
    subject_counts: Dict[str, int] = None
    # Iterate over immediate child directories representing subjects
    for subject_dir in data_root.iterdir():
        # Ignore non-directories to focus exclusively on subject folders
        if not subject_dir.is_dir():
            # Continue scanning when encountering stray files at the root
            continue
        # Count EDF files within the subject directory to quantify runs
        run_count = len(list(subject_dir.glob("*.edf")))
        # Persist the count for downstream comparison against expectations
        subject_counts[subject_dir.name] = run_count
    # Return all computed run counts for further validation steps
    return subject_counts


def x__collect_run_counts__mutmut_2(data_root: Path) -> Dict[str, int]:
    """Count EDF runs per subject directory."""

    # Initialize dictionary to aggregate run totals by subject
    subject_counts: Dict[str, int] = {}
    # Iterate over immediate child directories representing subjects
    for subject_dir in data_root.iterdir():
        # Ignore non-directories to focus exclusively on subject folders
        if subject_dir.is_dir():
            # Continue scanning when encountering stray files at the root
            continue
        # Count EDF files within the subject directory to quantify runs
        run_count = len(list(subject_dir.glob("*.edf")))
        # Persist the count for downstream comparison against expectations
        subject_counts[subject_dir.name] = run_count
    # Return all computed run counts for further validation steps
    return subject_counts


def x__collect_run_counts__mutmut_3(data_root: Path) -> Dict[str, int]:
    """Count EDF runs per subject directory."""

    # Initialize dictionary to aggregate run totals by subject
    subject_counts: Dict[str, int] = {}
    # Iterate over immediate child directories representing subjects
    for subject_dir in data_root.iterdir():
        # Ignore non-directories to focus exclusively on subject folders
        if not subject_dir.is_dir():
            # Continue scanning when encountering stray files at the root
            break
        # Count EDF files within the subject directory to quantify runs
        run_count = len(list(subject_dir.glob("*.edf")))
        # Persist the count for downstream comparison against expectations
        subject_counts[subject_dir.name] = run_count
    # Return all computed run counts for further validation steps
    return subject_counts


def x__collect_run_counts__mutmut_4(data_root: Path) -> Dict[str, int]:
    """Count EDF runs per subject directory."""

    # Initialize dictionary to aggregate run totals by subject
    subject_counts: Dict[str, int] = {}
    # Iterate over immediate child directories representing subjects
    for subject_dir in data_root.iterdir():
        # Ignore non-directories to focus exclusively on subject folders
        if not subject_dir.is_dir():
            # Continue scanning when encountering stray files at the root
            continue
        # Count EDF files within the subject directory to quantify runs
        run_count = None
        # Persist the count for downstream comparison against expectations
        subject_counts[subject_dir.name] = run_count
    # Return all computed run counts for further validation steps
    return subject_counts


def x__collect_run_counts__mutmut_5(data_root: Path) -> Dict[str, int]:
    """Count EDF runs per subject directory."""

    # Initialize dictionary to aggregate run totals by subject
    subject_counts: Dict[str, int] = {}
    # Iterate over immediate child directories representing subjects
    for subject_dir in data_root.iterdir():
        # Ignore non-directories to focus exclusively on subject folders
        if not subject_dir.is_dir():
            # Continue scanning when encountering stray files at the root
            continue
        # Count EDF files within the subject directory to quantify runs
        run_count = len(list(subject_dir.glob("*.edf")))
        # Persist the count for downstream comparison against expectations
        subject_counts[subject_dir.name] = None
    # Return all computed run counts for further validation steps
    return subject_counts

x__collect_run_counts__mutmut_mutants : ClassVar[MutantDict] = {
'x__collect_run_counts__mutmut_1': x__collect_run_counts__mutmut_1, 
    'x__collect_run_counts__mutmut_2': x__collect_run_counts__mutmut_2, 
    'x__collect_run_counts__mutmut_3': x__collect_run_counts__mutmut_3, 
    'x__collect_run_counts__mutmut_4': x__collect_run_counts__mutmut_4, 
    'x__collect_run_counts__mutmut_5': x__collect_run_counts__mutmut_5
}

def _collect_run_counts(*args, **kwargs):
    result = _mutmut_trampoline(x__collect_run_counts__mutmut_orig, x__collect_run_counts__mutmut_mutants, args, kwargs)
    return result 

_collect_run_counts.__signature__ = _mutmut_signature(x__collect_run_counts__mutmut_orig)
x__collect_run_counts__mutmut_orig.__name__ = 'x__collect_run_counts'


def x_verify_dataset_integrity__mutmut_orig(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_1(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = None
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_2(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(None).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_3(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = None
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_4(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = None
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_5(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"XXrootXX": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_6(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"ROOT": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_7(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(None), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_8(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "XXfilesXX": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_9(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "FILES": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_10(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_11(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(None)
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_12(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob(None):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_13(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("XX*.edfXX"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_14(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.EDF"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_15(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(None)
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_16(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(None, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_17(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, None, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_18(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, None))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_19(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_20(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_21(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, ))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_22(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = None
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_23(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(None)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_24(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = None
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_25(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["XXsubject_run_countsXX"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_26(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["SUBJECT_RUN_COUNTS"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_27(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = None
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_28(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(None) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_29(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(missing_runs)}")
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_30(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(None)
    # Return the report to enable higher-level monitoring or logging
    return report


def x_verify_dataset_integrity__mutmut_31(
    base_path: Path,
    expected_hashes: Mapping[str, str] | None = None,
    expected_runs_per_subject: Mapping[str, int] | None = None,
) -> Dict[str, Any]:
    """Verify presence, size, and optional hashes for Physionet data."""

    # Resolve dataset root to ensure comparisons use absolute locations
    data_root = Path(base_path).expanduser().resolve()
    # Prepare a container for per-file reports to keep typing explicit
    file_entries: List[Dict[str, object]] = []
    # Prepare a report structure to feed monitoring or logging systems
    report: Dict[str, Any] = {"root": str(data_root), "files": file_entries}
    # Fail fast if the dataset directory is missing to avoid silent skips
    if not data_root.exists():
        # Raise an explicit error when the dataset root cannot be found
        raise FileNotFoundError(f"Dataset directory not found: {data_root}")
    # Walk through EDF files to build a detailed integrity report
    for file_path in data_root.rglob("*.edf"):
        # Append structured entry for each discovered EDF recording
        file_entries.append(_build_file_entry(data_root, file_path, expected_hashes))
    # Validate expected run counts when provided by the caller
    if expected_runs_per_subject:
        # Collect run totals per subject to compare against expectations
        subject_counts = _collect_run_counts(data_root)
        # Attach run counts to the report for external visibility
        report["subject_run_counts"] = subject_counts
        # Identify subjects whose run counts deviate from expectations
        missing_runs = {
            subject: count
            for subject, count in subject_counts.items()
            if expected_runs_per_subject.get(subject) not in (None, count)
        }
        # Raise when any subject is incomplete to protect model validity
        if missing_runs:
            # Raise a clear error so dataset preparation can be fixed early
            raise ValueError(f"Run count mismatch: {json.dumps(None)}")
    # Return the report to enable higher-level monitoring or logging
    return report

x_verify_dataset_integrity__mutmut_mutants : ClassVar[MutantDict] = {
'x_verify_dataset_integrity__mutmut_1': x_verify_dataset_integrity__mutmut_1, 
    'x_verify_dataset_integrity__mutmut_2': x_verify_dataset_integrity__mutmut_2, 
    'x_verify_dataset_integrity__mutmut_3': x_verify_dataset_integrity__mutmut_3, 
    'x_verify_dataset_integrity__mutmut_4': x_verify_dataset_integrity__mutmut_4, 
    'x_verify_dataset_integrity__mutmut_5': x_verify_dataset_integrity__mutmut_5, 
    'x_verify_dataset_integrity__mutmut_6': x_verify_dataset_integrity__mutmut_6, 
    'x_verify_dataset_integrity__mutmut_7': x_verify_dataset_integrity__mutmut_7, 
    'x_verify_dataset_integrity__mutmut_8': x_verify_dataset_integrity__mutmut_8, 
    'x_verify_dataset_integrity__mutmut_9': x_verify_dataset_integrity__mutmut_9, 
    'x_verify_dataset_integrity__mutmut_10': x_verify_dataset_integrity__mutmut_10, 
    'x_verify_dataset_integrity__mutmut_11': x_verify_dataset_integrity__mutmut_11, 
    'x_verify_dataset_integrity__mutmut_12': x_verify_dataset_integrity__mutmut_12, 
    'x_verify_dataset_integrity__mutmut_13': x_verify_dataset_integrity__mutmut_13, 
    'x_verify_dataset_integrity__mutmut_14': x_verify_dataset_integrity__mutmut_14, 
    'x_verify_dataset_integrity__mutmut_15': x_verify_dataset_integrity__mutmut_15, 
    'x_verify_dataset_integrity__mutmut_16': x_verify_dataset_integrity__mutmut_16, 
    'x_verify_dataset_integrity__mutmut_17': x_verify_dataset_integrity__mutmut_17, 
    'x_verify_dataset_integrity__mutmut_18': x_verify_dataset_integrity__mutmut_18, 
    'x_verify_dataset_integrity__mutmut_19': x_verify_dataset_integrity__mutmut_19, 
    'x_verify_dataset_integrity__mutmut_20': x_verify_dataset_integrity__mutmut_20, 
    'x_verify_dataset_integrity__mutmut_21': x_verify_dataset_integrity__mutmut_21, 
    'x_verify_dataset_integrity__mutmut_22': x_verify_dataset_integrity__mutmut_22, 
    'x_verify_dataset_integrity__mutmut_23': x_verify_dataset_integrity__mutmut_23, 
    'x_verify_dataset_integrity__mutmut_24': x_verify_dataset_integrity__mutmut_24, 
    'x_verify_dataset_integrity__mutmut_25': x_verify_dataset_integrity__mutmut_25, 
    'x_verify_dataset_integrity__mutmut_26': x_verify_dataset_integrity__mutmut_26, 
    'x_verify_dataset_integrity__mutmut_27': x_verify_dataset_integrity__mutmut_27, 
    'x_verify_dataset_integrity__mutmut_28': x_verify_dataset_integrity__mutmut_28, 
    'x_verify_dataset_integrity__mutmut_29': x_verify_dataset_integrity__mutmut_29, 
    'x_verify_dataset_integrity__mutmut_30': x_verify_dataset_integrity__mutmut_30, 
    'x_verify_dataset_integrity__mutmut_31': x_verify_dataset_integrity__mutmut_31
}

def verify_dataset_integrity(*args, **kwargs):
    result = _mutmut_trampoline(x_verify_dataset_integrity__mutmut_orig, x_verify_dataset_integrity__mutmut_mutants, args, kwargs)
    return result 

verify_dataset_integrity.__signature__ = _mutmut_signature(x_verify_dataset_integrity__mutmut_orig)
x_verify_dataset_integrity__mutmut_orig.__name__ = 'x_verify_dataset_integrity'


def x_generate_epoch_report__mutmut_orig(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_1(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "XXjsonXX",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_2(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "JSON",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_3(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = None
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_4(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.upper()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_5(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt == fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_6(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError(None)
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_7(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("XXfmt must be lowercaseXX")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_8(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("FMT MUST BE LOWERCASE")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_9(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_10(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"XXjsonXX", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_11(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"JSON", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_12(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "XXcsvXX"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_13(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "CSV"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_14(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError(None)
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_15(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("XXfmt must be either 'json' or 'csv'XX")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_16(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("FMT MUST BE EITHER 'JSON' OR 'CSV'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_17(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = None
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_18(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(None)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_19(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=None, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_20(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=None)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_21(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_22(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, )
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_23(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=False, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_24(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=False)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_25(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = None
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_26(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = None
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_27(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(None)
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_28(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(None))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_29(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 3] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_30(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] != code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_31(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = None
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_32(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "XXsubjectXX": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_33(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "SUBJECT": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_34(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["XXsubjectXX"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_35(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["SUBJECT"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_36(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "XXrunXX": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_37(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "RUN": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_38(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["XXrunXX"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_39(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["RUN"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_40(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "XXtotal_epochsXX": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_41(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "TOTAL_EPOCHS": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_42(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(None),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_43(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "XXcountsXX": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_44(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "COUNTS": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_45(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized != "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_46(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "XXjsonXX":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_47(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "JSON":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_48(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(None, encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_49(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding=None)
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_50(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_51(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), )
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_52(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(None, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_53(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=None), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_54(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_55(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, ), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_56(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=3), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_57(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="XXutf-8XX")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_58(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="UTF-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_59(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = None
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_60(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["XXsubject,run,label,countXX"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_61(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["SUBJECT,RUN,LABEL,COUNT"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_62(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(None)
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_63(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['XXsubjectXX']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_64(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['SUBJECT']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_65(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['XXrunXX']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_66(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['RUN']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_67(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text(None, encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_68(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding=None)
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_69(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text(encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_70(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), )
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_71(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(None), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_72(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("XX\nXX".join(lines), encoding="utf-8")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_73(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="XXutf-8XX")
    # Return the path so downstream processes can load the CSV
    return output_path


def x_generate_epoch_report__mutmut_74(
    epochs: mne.Epochs,
    event_id: Mapping[str, int],
    run_metadata: Mapping[str, str],
    output_path: Path,
    fmt: str = "json",
) -> Path:
    """Persist epoch counts per class, subject, and run in JSON or CSV."""

    # Convertit le format en minuscules pour uniformiser les comparaisons
    fmt_normalized = fmt.lower()
    # Refuse les formats non minuscules pour éviter les ambiguïtés silencieuses
    if fmt != fmt_normalized:
        # Arrête l'exécution pour imposer une convention de nommage explicite
        raise ValueError("fmt must be lowercase")
    # Vérifie que le format fourni est limité aux options minuscules supportées
    if fmt_normalized not in {"json", "csv"}:
        # Interrompt tôt pour éviter d'écrire un rapport avec un format ambigu
        raise ValueError("fmt must be either 'json' or 'csv'")
    # Normalise le chemin pour garantir des écritures cohérentes sur disque
    output_path = Path(output_path)
    # Create parent directories so the report can be written without errors
    output_path.parent.mkdir(parents=True, exist_ok=True)
    # Build a reverse lookup to translate event codes into label names
    reverse_map = {code: label for label, code in event_id.items()}
    # Count epochs for every label present in the event mapping
    label_counts = {
        label: int(np.sum(epochs.events[:, 2] == code))
        for code, label in reverse_map.items()
    }
    # Compose a structured payload describing the run content
    payload: Dict[str, Any] = {
        "subject": run_metadata["subject"],
        "run": run_metadata["run"],
        "total_epochs": int(len(epochs)),
        "counts": label_counts,
    }
    # Serialize the payload to JSON when requested by the caller
    if fmt_normalized == "json":
        # Write the JSON content with indentation for human readability
        output_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        # Return the path so callers can locate the generated report
        return output_path
    # Serialize the payload to CSV rows when CSV is requested
    lines = ["subject,run,label,count"]
    # Iterate over label counts to materialize per-class entries
    for label, count in label_counts.items():
        # Append a CSV line detailing subject, run, label, and count
        lines.append(f"{run_metadata['subject']},{run_metadata['run']},{label},{count}")
    # Write all lines with newline separation to the output path
    output_path.write_text("\n".join(lines), encoding="UTF-8")
    # Return the path so downstream processes can load the CSV
    return output_path

x_generate_epoch_report__mutmut_mutants : ClassVar[MutantDict] = {
'x_generate_epoch_report__mutmut_1': x_generate_epoch_report__mutmut_1, 
    'x_generate_epoch_report__mutmut_2': x_generate_epoch_report__mutmut_2, 
    'x_generate_epoch_report__mutmut_3': x_generate_epoch_report__mutmut_3, 
    'x_generate_epoch_report__mutmut_4': x_generate_epoch_report__mutmut_4, 
    'x_generate_epoch_report__mutmut_5': x_generate_epoch_report__mutmut_5, 
    'x_generate_epoch_report__mutmut_6': x_generate_epoch_report__mutmut_6, 
    'x_generate_epoch_report__mutmut_7': x_generate_epoch_report__mutmut_7, 
    'x_generate_epoch_report__mutmut_8': x_generate_epoch_report__mutmut_8, 
    'x_generate_epoch_report__mutmut_9': x_generate_epoch_report__mutmut_9, 
    'x_generate_epoch_report__mutmut_10': x_generate_epoch_report__mutmut_10, 
    'x_generate_epoch_report__mutmut_11': x_generate_epoch_report__mutmut_11, 
    'x_generate_epoch_report__mutmut_12': x_generate_epoch_report__mutmut_12, 
    'x_generate_epoch_report__mutmut_13': x_generate_epoch_report__mutmut_13, 
    'x_generate_epoch_report__mutmut_14': x_generate_epoch_report__mutmut_14, 
    'x_generate_epoch_report__mutmut_15': x_generate_epoch_report__mutmut_15, 
    'x_generate_epoch_report__mutmut_16': x_generate_epoch_report__mutmut_16, 
    'x_generate_epoch_report__mutmut_17': x_generate_epoch_report__mutmut_17, 
    'x_generate_epoch_report__mutmut_18': x_generate_epoch_report__mutmut_18, 
    'x_generate_epoch_report__mutmut_19': x_generate_epoch_report__mutmut_19, 
    'x_generate_epoch_report__mutmut_20': x_generate_epoch_report__mutmut_20, 
    'x_generate_epoch_report__mutmut_21': x_generate_epoch_report__mutmut_21, 
    'x_generate_epoch_report__mutmut_22': x_generate_epoch_report__mutmut_22, 
    'x_generate_epoch_report__mutmut_23': x_generate_epoch_report__mutmut_23, 
    'x_generate_epoch_report__mutmut_24': x_generate_epoch_report__mutmut_24, 
    'x_generate_epoch_report__mutmut_25': x_generate_epoch_report__mutmut_25, 
    'x_generate_epoch_report__mutmut_26': x_generate_epoch_report__mutmut_26, 
    'x_generate_epoch_report__mutmut_27': x_generate_epoch_report__mutmut_27, 
    'x_generate_epoch_report__mutmut_28': x_generate_epoch_report__mutmut_28, 
    'x_generate_epoch_report__mutmut_29': x_generate_epoch_report__mutmut_29, 
    'x_generate_epoch_report__mutmut_30': x_generate_epoch_report__mutmut_30, 
    'x_generate_epoch_report__mutmut_31': x_generate_epoch_report__mutmut_31, 
    'x_generate_epoch_report__mutmut_32': x_generate_epoch_report__mutmut_32, 
    'x_generate_epoch_report__mutmut_33': x_generate_epoch_report__mutmut_33, 
    'x_generate_epoch_report__mutmut_34': x_generate_epoch_report__mutmut_34, 
    'x_generate_epoch_report__mutmut_35': x_generate_epoch_report__mutmut_35, 
    'x_generate_epoch_report__mutmut_36': x_generate_epoch_report__mutmut_36, 
    'x_generate_epoch_report__mutmut_37': x_generate_epoch_report__mutmut_37, 
    'x_generate_epoch_report__mutmut_38': x_generate_epoch_report__mutmut_38, 
    'x_generate_epoch_report__mutmut_39': x_generate_epoch_report__mutmut_39, 
    'x_generate_epoch_report__mutmut_40': x_generate_epoch_report__mutmut_40, 
    'x_generate_epoch_report__mutmut_41': x_generate_epoch_report__mutmut_41, 
    'x_generate_epoch_report__mutmut_42': x_generate_epoch_report__mutmut_42, 
    'x_generate_epoch_report__mutmut_43': x_generate_epoch_report__mutmut_43, 
    'x_generate_epoch_report__mutmut_44': x_generate_epoch_report__mutmut_44, 
    'x_generate_epoch_report__mutmut_45': x_generate_epoch_report__mutmut_45, 
    'x_generate_epoch_report__mutmut_46': x_generate_epoch_report__mutmut_46, 
    'x_generate_epoch_report__mutmut_47': x_generate_epoch_report__mutmut_47, 
    'x_generate_epoch_report__mutmut_48': x_generate_epoch_report__mutmut_48, 
    'x_generate_epoch_report__mutmut_49': x_generate_epoch_report__mutmut_49, 
    'x_generate_epoch_report__mutmut_50': x_generate_epoch_report__mutmut_50, 
    'x_generate_epoch_report__mutmut_51': x_generate_epoch_report__mutmut_51, 
    'x_generate_epoch_report__mutmut_52': x_generate_epoch_report__mutmut_52, 
    'x_generate_epoch_report__mutmut_53': x_generate_epoch_report__mutmut_53, 
    'x_generate_epoch_report__mutmut_54': x_generate_epoch_report__mutmut_54, 
    'x_generate_epoch_report__mutmut_55': x_generate_epoch_report__mutmut_55, 
    'x_generate_epoch_report__mutmut_56': x_generate_epoch_report__mutmut_56, 
    'x_generate_epoch_report__mutmut_57': x_generate_epoch_report__mutmut_57, 
    'x_generate_epoch_report__mutmut_58': x_generate_epoch_report__mutmut_58, 
    'x_generate_epoch_report__mutmut_59': x_generate_epoch_report__mutmut_59, 
    'x_generate_epoch_report__mutmut_60': x_generate_epoch_report__mutmut_60, 
    'x_generate_epoch_report__mutmut_61': x_generate_epoch_report__mutmut_61, 
    'x_generate_epoch_report__mutmut_62': x_generate_epoch_report__mutmut_62, 
    'x_generate_epoch_report__mutmut_63': x_generate_epoch_report__mutmut_63, 
    'x_generate_epoch_report__mutmut_64': x_generate_epoch_report__mutmut_64, 
    'x_generate_epoch_report__mutmut_65': x_generate_epoch_report__mutmut_65, 
    'x_generate_epoch_report__mutmut_66': x_generate_epoch_report__mutmut_66, 
    'x_generate_epoch_report__mutmut_67': x_generate_epoch_report__mutmut_67, 
    'x_generate_epoch_report__mutmut_68': x_generate_epoch_report__mutmut_68, 
    'x_generate_epoch_report__mutmut_69': x_generate_epoch_report__mutmut_69, 
    'x_generate_epoch_report__mutmut_70': x_generate_epoch_report__mutmut_70, 
    'x_generate_epoch_report__mutmut_71': x_generate_epoch_report__mutmut_71, 
    'x_generate_epoch_report__mutmut_72': x_generate_epoch_report__mutmut_72, 
    'x_generate_epoch_report__mutmut_73': x_generate_epoch_report__mutmut_73, 
    'x_generate_epoch_report__mutmut_74': x_generate_epoch_report__mutmut_74
}

def generate_epoch_report(*args, **kwargs):
    result = _mutmut_trampoline(x_generate_epoch_report__mutmut_orig, x_generate_epoch_report__mutmut_mutants, args, kwargs)
    return result 

generate_epoch_report.__signature__ = _mutmut_signature(x_generate_epoch_report__mutmut_orig)
x_generate_epoch_report__mutmut_orig.__name__ = 'x_generate_epoch_report'
