"""CLI de prédiction pour le pipeline TPV."""

# Préserve argparse pour exposer une interface CLI homogène avec mybci
# Garantit l'accès aux chemins portables pour données et artefacts
# Fournit le parsing CLI pour aligner la signature mybci
import argparse

# Fournit l'écriture CSV pour exposer les prédictions individuelles
import csv

# Fournit la sérialisation JSON pour tracer les rapports générés
import json

# Garantit l'accès aux chemins portables pour données et artefacts
from pathlib import Path

# Centralise l'accès aux tableaux numpy pour l'évaluation
import numpy as np

# Calcule les métriques de classification pour le rapport
from sklearn.metrics import confusion_matrix

# Expose l'entraînement programmatique pour générer un modèle manquant
from scripts.train import DEFAULT_SAMPLING_RATE, TrainingRequest, run_training

# Centralise le parsing et le contrôle qualité des fichiers EDF
from tpv import preprocessing

# Permet de restaurer la matrice W pour des usages temps-réel
from tpv.dimensionality import TPVDimReducer

# Expose la configuration de pipeline pour déclencher un auto-train
from tpv.pipeline import PipelineConfig, load_pipeline

# Définit le volume attendu des données EEG brutes (trials, canaux, temps)
EXPECTED_FEATURES_DIMENSIONS = 3

# Définit le répertoire par défaut où chercher les enregistrements
DEFAULT_DATA_DIR = Path("data")

# Définit le répertoire par défaut pour récupérer les artefacts
DEFAULT_ARTIFACTS_DIR = Path("artifacts")

# Définit le répertoire par défaut pour les fichiers EDF bruts
DEFAULT_RAW_DIR = Path("data")
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_orig() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_1() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = None
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_2() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description=None,
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_3() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="XXCharge une pipeline TPV entraînée et produit un rapportXX",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_4() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="charge une pipeline tpv entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_5() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="CHARGE UNE PIPELINE TPV ENTRAÎNÉE ET PRODUIT UN RAPPORT",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_6() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument(None, help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_7() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help=None)
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_8() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument(help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_9() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", )
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_10() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("XXsubjectXX", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_11() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("SUBJECT", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_12() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="XXIdentifiant du sujet (ex: S001)XX")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_13() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="identifiant du sujet (ex: s001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_14() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="IDENTIFIANT DU SUJET (EX: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_15() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument(None, help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_16() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help=None)

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_17() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument(help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_18() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", )

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_19() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("XXrunXX", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_20() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("RUN", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_21() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="XXIdentifiant du run (ex: R01)XX")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_22() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="identifiant du run (ex: r01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_23() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="IDENTIFIANT DU RUN (EX: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_24() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        None,
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_25() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=None,
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_26() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default=None,
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_27() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help=None,
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_28() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_29() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_30() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_31() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_32() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "XX--classifierXX",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_33() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--CLASSIFIER",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_34() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("XXldaXX", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_35() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("LDA", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_36() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "XXlogisticXX", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_37() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "LOGISTIC", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_38() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "XXsvmXX", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_39() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "SVM", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_40() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "XXcentroidXX"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_41() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "CENTROID"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_42() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="XXldaXX",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_43() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="LDA",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_44() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="XXClassifieur final (ignoré en prédiction, pour compatibilité CLI)XX",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_45() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="classifieur final (ignoré en prédiction, pour compatibilité cli)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_46() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="CLASSIFIEUR FINAL (IGNORÉ EN PRÉDICTION, POUR COMPATIBILITÉ CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_47() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        None,
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_48() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=None,
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_49() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default=None,
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_50() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help=None,
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_51() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_52() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_53() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_54() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_55() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "XX--scalerXX",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_56() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--SCALER",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_57() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("XXstandardXX", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_58() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("STANDARD", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_59() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "XXrobustXX", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_60() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "ROBUST", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_61() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "XXnoneXX"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_62() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "NONE"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_63() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="XXnoneXX",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_64() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="NONE",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_65() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="XXScaler appliqué en entraînement (ignoré en prédiction)XX",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_66() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_67() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="SCALER APPLIQUÉ EN ENTRAÎNEMENT (IGNORÉ EN PRÉDICTION)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_68() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        None,
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_69() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=None,
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_70() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default=None,
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_71() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help=None,
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_72() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_73() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_74() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_75() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_76() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "XX--feature-strategyXX",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_77() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--FEATURE-STRATEGY",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_78() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("XXfftXX", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_79() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("FFT", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_80() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "XXwaveletXX"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_81() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "WAVELET"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_82() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="XXfftXX",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_83() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="FFT",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_84() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="XXStratégie de features utilisée à l'entraînement (ignorée ici)XX",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_85() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_86() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="STRATÉGIE DE FEATURES UTILISÉE À L'ENTRAÎNEMENT (IGNORÉE ICI)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_87() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        None,
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_88() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=None,
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_89() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default=None,
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_90() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help=None,
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_91() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_92() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_93() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_94() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_95() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "XX--dim-methodXX",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_96() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--DIM-METHOD",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_97() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("XXpcaXX", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_98() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("PCA", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_99() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "XXcspXX"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_100() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "CSP"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_101() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="XXpcaXX",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_102() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="PCA",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_103() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="XXMéthode de réduction de dimension (ignorée en prédiction)XX",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_104() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_105() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="MÉTHODE DE RÉDUCTION DE DIMENSION (IGNORÉE EN PRÉDICTION)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_106() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        None,
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_107() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=None,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_108() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=None,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_109() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help=None,
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_110() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_111() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_112() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_113() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_114() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "XX--n-componentsXX",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_115() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--N-COMPONENTS",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_116() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="XXNombre de composantes (ignoré en prédiction)XX",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_117() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_118() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="NOMBRE DE COMPOSANTES (IGNORÉ EN PRÉDICTION)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_119() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        None,
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_120() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action=None,
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_121() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help=None,
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_122() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_123() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_124() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_125() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "XX--no-normalize-featuresXX",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_126() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--NO-NORMALIZE-FEATURES",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_127() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="XXstore_trueXX",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_128() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="STORE_TRUE",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_129() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="XXFlag de normalisation (ignoré en prédiction)XX",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_130() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_131() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="FLAG DE NORMALISATION (IGNORÉ EN PRÉDICTION)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_132() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        None,
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_133() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=None,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_134() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=None,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_135() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help=None,
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_136() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_137() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_138() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_139() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_140() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "XX--sfreqXX",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_141() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--SFREQ",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_142() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=51.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_143() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="XXFréquence utilisée en features (ignorée ici)XX",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_144() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_145() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="FRÉQUENCE UTILISÉE EN FEATURES (IGNORÉE ICI)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_146() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        None,
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_147() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=None,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_148() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=None,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_149() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help=None,
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_150() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_151() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_152() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_153() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_154() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "XX--data-dirXX",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_155() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--DATA-DIR",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_156() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="XXRépertoire racine contenant les fichiers numpyXX",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_157() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_158() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="RÉPERTOIRE RACINE CONTENANT LES FICHIERS NUMPY",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_159() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        None,
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_160() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=None,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_161() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=None,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_162() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help=None,
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_163() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_164() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_165() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_166() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_167() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "XX--artifacts-dirXX",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_168() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--ARTIFACTS-DIR",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_169() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="XXRépertoire racine où lire le modèleXX",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_170() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_171() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="RÉPERTOIRE RACINE OÙ LIRE LE MODÈLE",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_172() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        None,
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_173() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=None,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_174() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=None,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_175() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help=None,
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_176() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_177() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_178() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_179() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_180() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "XX--raw-dirXX",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_181() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--RAW-DIR",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_182() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="XXRépertoire racine contenant les fichiers EDF brutsXX",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_183() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="répertoire racine contenant les fichiers edf bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur l'appel mybci
def x_build_parser__mutmut_184() -> argparse.ArgumentParser:
    """Construit le parser CLI pour la prédiction TPV."""

    # Crée le parser avec description explicite pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Charge une pipeline TPV entraînée et produit un rapport",
    )
    # Ajoute l'argument positionnel du sujet pour cibler les artefacts
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")

    # ------------------------------------------------------------------
    # Options de compatibilité avec la CLI mybci (train/predict)
    # Ces options sont acceptées mais *ignorées* côté prédiction, car
    # le modèle déjà entraîné porte la vraie configuration.
    # ------------------------------------------------------------------
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final (ignoré en prédiction, pour compatibilité CLI)",
    )
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler appliqué en entraînement (ignoré en prédiction)",
    )
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Stratégie de features utilisée à l'entraînement (ignorée ici)",
    )
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension (ignorée en prédiction)",
    )
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes (ignoré en prédiction)",
    )
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Flag de normalisation (ignoré en prédiction)",
    )
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence utilisée en features (ignorée ici)",
    )

    # ------------------------------------------------------------------
    # Options réellement utilisées par scripts/predict
    # ------------------------------------------------------------------
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où lire le modèle",
    )
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="RÉPERTOIRE RACINE CONTENANT LES FICHIERS EDF BRUTS",
    )
    # Retourne le parser configuré
    return parser

x_build_parser__mutmut_mutants : ClassVar[MutantDict] = {
'x_build_parser__mutmut_1': x_build_parser__mutmut_1, 
    'x_build_parser__mutmut_2': x_build_parser__mutmut_2, 
    'x_build_parser__mutmut_3': x_build_parser__mutmut_3, 
    'x_build_parser__mutmut_4': x_build_parser__mutmut_4, 
    'x_build_parser__mutmut_5': x_build_parser__mutmut_5, 
    'x_build_parser__mutmut_6': x_build_parser__mutmut_6, 
    'x_build_parser__mutmut_7': x_build_parser__mutmut_7, 
    'x_build_parser__mutmut_8': x_build_parser__mutmut_8, 
    'x_build_parser__mutmut_9': x_build_parser__mutmut_9, 
    'x_build_parser__mutmut_10': x_build_parser__mutmut_10, 
    'x_build_parser__mutmut_11': x_build_parser__mutmut_11, 
    'x_build_parser__mutmut_12': x_build_parser__mutmut_12, 
    'x_build_parser__mutmut_13': x_build_parser__mutmut_13, 
    'x_build_parser__mutmut_14': x_build_parser__mutmut_14, 
    'x_build_parser__mutmut_15': x_build_parser__mutmut_15, 
    'x_build_parser__mutmut_16': x_build_parser__mutmut_16, 
    'x_build_parser__mutmut_17': x_build_parser__mutmut_17, 
    'x_build_parser__mutmut_18': x_build_parser__mutmut_18, 
    'x_build_parser__mutmut_19': x_build_parser__mutmut_19, 
    'x_build_parser__mutmut_20': x_build_parser__mutmut_20, 
    'x_build_parser__mutmut_21': x_build_parser__mutmut_21, 
    'x_build_parser__mutmut_22': x_build_parser__mutmut_22, 
    'x_build_parser__mutmut_23': x_build_parser__mutmut_23, 
    'x_build_parser__mutmut_24': x_build_parser__mutmut_24, 
    'x_build_parser__mutmut_25': x_build_parser__mutmut_25, 
    'x_build_parser__mutmut_26': x_build_parser__mutmut_26, 
    'x_build_parser__mutmut_27': x_build_parser__mutmut_27, 
    'x_build_parser__mutmut_28': x_build_parser__mutmut_28, 
    'x_build_parser__mutmut_29': x_build_parser__mutmut_29, 
    'x_build_parser__mutmut_30': x_build_parser__mutmut_30, 
    'x_build_parser__mutmut_31': x_build_parser__mutmut_31, 
    'x_build_parser__mutmut_32': x_build_parser__mutmut_32, 
    'x_build_parser__mutmut_33': x_build_parser__mutmut_33, 
    'x_build_parser__mutmut_34': x_build_parser__mutmut_34, 
    'x_build_parser__mutmut_35': x_build_parser__mutmut_35, 
    'x_build_parser__mutmut_36': x_build_parser__mutmut_36, 
    'x_build_parser__mutmut_37': x_build_parser__mutmut_37, 
    'x_build_parser__mutmut_38': x_build_parser__mutmut_38, 
    'x_build_parser__mutmut_39': x_build_parser__mutmut_39, 
    'x_build_parser__mutmut_40': x_build_parser__mutmut_40, 
    'x_build_parser__mutmut_41': x_build_parser__mutmut_41, 
    'x_build_parser__mutmut_42': x_build_parser__mutmut_42, 
    'x_build_parser__mutmut_43': x_build_parser__mutmut_43, 
    'x_build_parser__mutmut_44': x_build_parser__mutmut_44, 
    'x_build_parser__mutmut_45': x_build_parser__mutmut_45, 
    'x_build_parser__mutmut_46': x_build_parser__mutmut_46, 
    'x_build_parser__mutmut_47': x_build_parser__mutmut_47, 
    'x_build_parser__mutmut_48': x_build_parser__mutmut_48, 
    'x_build_parser__mutmut_49': x_build_parser__mutmut_49, 
    'x_build_parser__mutmut_50': x_build_parser__mutmut_50, 
    'x_build_parser__mutmut_51': x_build_parser__mutmut_51, 
    'x_build_parser__mutmut_52': x_build_parser__mutmut_52, 
    'x_build_parser__mutmut_53': x_build_parser__mutmut_53, 
    'x_build_parser__mutmut_54': x_build_parser__mutmut_54, 
    'x_build_parser__mutmut_55': x_build_parser__mutmut_55, 
    'x_build_parser__mutmut_56': x_build_parser__mutmut_56, 
    'x_build_parser__mutmut_57': x_build_parser__mutmut_57, 
    'x_build_parser__mutmut_58': x_build_parser__mutmut_58, 
    'x_build_parser__mutmut_59': x_build_parser__mutmut_59, 
    'x_build_parser__mutmut_60': x_build_parser__mutmut_60, 
    'x_build_parser__mutmut_61': x_build_parser__mutmut_61, 
    'x_build_parser__mutmut_62': x_build_parser__mutmut_62, 
    'x_build_parser__mutmut_63': x_build_parser__mutmut_63, 
    'x_build_parser__mutmut_64': x_build_parser__mutmut_64, 
    'x_build_parser__mutmut_65': x_build_parser__mutmut_65, 
    'x_build_parser__mutmut_66': x_build_parser__mutmut_66, 
    'x_build_parser__mutmut_67': x_build_parser__mutmut_67, 
    'x_build_parser__mutmut_68': x_build_parser__mutmut_68, 
    'x_build_parser__mutmut_69': x_build_parser__mutmut_69, 
    'x_build_parser__mutmut_70': x_build_parser__mutmut_70, 
    'x_build_parser__mutmut_71': x_build_parser__mutmut_71, 
    'x_build_parser__mutmut_72': x_build_parser__mutmut_72, 
    'x_build_parser__mutmut_73': x_build_parser__mutmut_73, 
    'x_build_parser__mutmut_74': x_build_parser__mutmut_74, 
    'x_build_parser__mutmut_75': x_build_parser__mutmut_75, 
    'x_build_parser__mutmut_76': x_build_parser__mutmut_76, 
    'x_build_parser__mutmut_77': x_build_parser__mutmut_77, 
    'x_build_parser__mutmut_78': x_build_parser__mutmut_78, 
    'x_build_parser__mutmut_79': x_build_parser__mutmut_79, 
    'x_build_parser__mutmut_80': x_build_parser__mutmut_80, 
    'x_build_parser__mutmut_81': x_build_parser__mutmut_81, 
    'x_build_parser__mutmut_82': x_build_parser__mutmut_82, 
    'x_build_parser__mutmut_83': x_build_parser__mutmut_83, 
    'x_build_parser__mutmut_84': x_build_parser__mutmut_84, 
    'x_build_parser__mutmut_85': x_build_parser__mutmut_85, 
    'x_build_parser__mutmut_86': x_build_parser__mutmut_86, 
    'x_build_parser__mutmut_87': x_build_parser__mutmut_87, 
    'x_build_parser__mutmut_88': x_build_parser__mutmut_88, 
    'x_build_parser__mutmut_89': x_build_parser__mutmut_89, 
    'x_build_parser__mutmut_90': x_build_parser__mutmut_90, 
    'x_build_parser__mutmut_91': x_build_parser__mutmut_91, 
    'x_build_parser__mutmut_92': x_build_parser__mutmut_92, 
    'x_build_parser__mutmut_93': x_build_parser__mutmut_93, 
    'x_build_parser__mutmut_94': x_build_parser__mutmut_94, 
    'x_build_parser__mutmut_95': x_build_parser__mutmut_95, 
    'x_build_parser__mutmut_96': x_build_parser__mutmut_96, 
    'x_build_parser__mutmut_97': x_build_parser__mutmut_97, 
    'x_build_parser__mutmut_98': x_build_parser__mutmut_98, 
    'x_build_parser__mutmut_99': x_build_parser__mutmut_99, 
    'x_build_parser__mutmut_100': x_build_parser__mutmut_100, 
    'x_build_parser__mutmut_101': x_build_parser__mutmut_101, 
    'x_build_parser__mutmut_102': x_build_parser__mutmut_102, 
    'x_build_parser__mutmut_103': x_build_parser__mutmut_103, 
    'x_build_parser__mutmut_104': x_build_parser__mutmut_104, 
    'x_build_parser__mutmut_105': x_build_parser__mutmut_105, 
    'x_build_parser__mutmut_106': x_build_parser__mutmut_106, 
    'x_build_parser__mutmut_107': x_build_parser__mutmut_107, 
    'x_build_parser__mutmut_108': x_build_parser__mutmut_108, 
    'x_build_parser__mutmut_109': x_build_parser__mutmut_109, 
    'x_build_parser__mutmut_110': x_build_parser__mutmut_110, 
    'x_build_parser__mutmut_111': x_build_parser__mutmut_111, 
    'x_build_parser__mutmut_112': x_build_parser__mutmut_112, 
    'x_build_parser__mutmut_113': x_build_parser__mutmut_113, 
    'x_build_parser__mutmut_114': x_build_parser__mutmut_114, 
    'x_build_parser__mutmut_115': x_build_parser__mutmut_115, 
    'x_build_parser__mutmut_116': x_build_parser__mutmut_116, 
    'x_build_parser__mutmut_117': x_build_parser__mutmut_117, 
    'x_build_parser__mutmut_118': x_build_parser__mutmut_118, 
    'x_build_parser__mutmut_119': x_build_parser__mutmut_119, 
    'x_build_parser__mutmut_120': x_build_parser__mutmut_120, 
    'x_build_parser__mutmut_121': x_build_parser__mutmut_121, 
    'x_build_parser__mutmut_122': x_build_parser__mutmut_122, 
    'x_build_parser__mutmut_123': x_build_parser__mutmut_123, 
    'x_build_parser__mutmut_124': x_build_parser__mutmut_124, 
    'x_build_parser__mutmut_125': x_build_parser__mutmut_125, 
    'x_build_parser__mutmut_126': x_build_parser__mutmut_126, 
    'x_build_parser__mutmut_127': x_build_parser__mutmut_127, 
    'x_build_parser__mutmut_128': x_build_parser__mutmut_128, 
    'x_build_parser__mutmut_129': x_build_parser__mutmut_129, 
    'x_build_parser__mutmut_130': x_build_parser__mutmut_130, 
    'x_build_parser__mutmut_131': x_build_parser__mutmut_131, 
    'x_build_parser__mutmut_132': x_build_parser__mutmut_132, 
    'x_build_parser__mutmut_133': x_build_parser__mutmut_133, 
    'x_build_parser__mutmut_134': x_build_parser__mutmut_134, 
    'x_build_parser__mutmut_135': x_build_parser__mutmut_135, 
    'x_build_parser__mutmut_136': x_build_parser__mutmut_136, 
    'x_build_parser__mutmut_137': x_build_parser__mutmut_137, 
    'x_build_parser__mutmut_138': x_build_parser__mutmut_138, 
    'x_build_parser__mutmut_139': x_build_parser__mutmut_139, 
    'x_build_parser__mutmut_140': x_build_parser__mutmut_140, 
    'x_build_parser__mutmut_141': x_build_parser__mutmut_141, 
    'x_build_parser__mutmut_142': x_build_parser__mutmut_142, 
    'x_build_parser__mutmut_143': x_build_parser__mutmut_143, 
    'x_build_parser__mutmut_144': x_build_parser__mutmut_144, 
    'x_build_parser__mutmut_145': x_build_parser__mutmut_145, 
    'x_build_parser__mutmut_146': x_build_parser__mutmut_146, 
    'x_build_parser__mutmut_147': x_build_parser__mutmut_147, 
    'x_build_parser__mutmut_148': x_build_parser__mutmut_148, 
    'x_build_parser__mutmut_149': x_build_parser__mutmut_149, 
    'x_build_parser__mutmut_150': x_build_parser__mutmut_150, 
    'x_build_parser__mutmut_151': x_build_parser__mutmut_151, 
    'x_build_parser__mutmut_152': x_build_parser__mutmut_152, 
    'x_build_parser__mutmut_153': x_build_parser__mutmut_153, 
    'x_build_parser__mutmut_154': x_build_parser__mutmut_154, 
    'x_build_parser__mutmut_155': x_build_parser__mutmut_155, 
    'x_build_parser__mutmut_156': x_build_parser__mutmut_156, 
    'x_build_parser__mutmut_157': x_build_parser__mutmut_157, 
    'x_build_parser__mutmut_158': x_build_parser__mutmut_158, 
    'x_build_parser__mutmut_159': x_build_parser__mutmut_159, 
    'x_build_parser__mutmut_160': x_build_parser__mutmut_160, 
    'x_build_parser__mutmut_161': x_build_parser__mutmut_161, 
    'x_build_parser__mutmut_162': x_build_parser__mutmut_162, 
    'x_build_parser__mutmut_163': x_build_parser__mutmut_163, 
    'x_build_parser__mutmut_164': x_build_parser__mutmut_164, 
    'x_build_parser__mutmut_165': x_build_parser__mutmut_165, 
    'x_build_parser__mutmut_166': x_build_parser__mutmut_166, 
    'x_build_parser__mutmut_167': x_build_parser__mutmut_167, 
    'x_build_parser__mutmut_168': x_build_parser__mutmut_168, 
    'x_build_parser__mutmut_169': x_build_parser__mutmut_169, 
    'x_build_parser__mutmut_170': x_build_parser__mutmut_170, 
    'x_build_parser__mutmut_171': x_build_parser__mutmut_171, 
    'x_build_parser__mutmut_172': x_build_parser__mutmut_172, 
    'x_build_parser__mutmut_173': x_build_parser__mutmut_173, 
    'x_build_parser__mutmut_174': x_build_parser__mutmut_174, 
    'x_build_parser__mutmut_175': x_build_parser__mutmut_175, 
    'x_build_parser__mutmut_176': x_build_parser__mutmut_176, 
    'x_build_parser__mutmut_177': x_build_parser__mutmut_177, 
    'x_build_parser__mutmut_178': x_build_parser__mutmut_178, 
    'x_build_parser__mutmut_179': x_build_parser__mutmut_179, 
    'x_build_parser__mutmut_180': x_build_parser__mutmut_180, 
    'x_build_parser__mutmut_181': x_build_parser__mutmut_181, 
    'x_build_parser__mutmut_182': x_build_parser__mutmut_182, 
    'x_build_parser__mutmut_183': x_build_parser__mutmut_183, 
    'x_build_parser__mutmut_184': x_build_parser__mutmut_184
}

def build_parser(*args, **kwargs):
    result = _mutmut_trampoline(x_build_parser__mutmut_orig, x_build_parser__mutmut_mutants, args, kwargs)
    return result 

build_parser.__signature__ = _mutmut_signature(x_build_parser__mutmut_orig)
x_build_parser__mutmut_orig.__name__ = 'x_build_parser'


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_orig(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_1(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = None
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_2(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir * subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_3(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = None
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_4(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir * f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_5(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = None
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_6(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir * f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path

x__resolve_data_paths__mutmut_mutants : ClassVar[MutantDict] = {
'x__resolve_data_paths__mutmut_1': x__resolve_data_paths__mutmut_1, 
    'x__resolve_data_paths__mutmut_2': x__resolve_data_paths__mutmut_2, 
    'x__resolve_data_paths__mutmut_3': x__resolve_data_paths__mutmut_3, 
    'x__resolve_data_paths__mutmut_4': x__resolve_data_paths__mutmut_4, 
    'x__resolve_data_paths__mutmut_5': x__resolve_data_paths__mutmut_5, 
    'x__resolve_data_paths__mutmut_6': x__resolve_data_paths__mutmut_6
}

def _resolve_data_paths(*args, **kwargs):
    result = _mutmut_trampoline(x__resolve_data_paths__mutmut_orig, x__resolve_data_paths__mutmut_mutants, args, kwargs)
    return result 

_resolve_data_paths.__signature__ = _mutmut_signature(x__resolve_data_paths__mutmut_orig)
x__resolve_data_paths__mutmut_orig.__name__ = 'x__resolve_data_paths'


# Construit des matrices numpy à partir d'un EDF lorsque nécessaire
def _build_npy_from_edf(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[Path, Path]:
    """Génère X (epochs brutes) et y depuis un fichier EDF Physionet."""

    # Calcule les chemins cibles pour les fichiers numpy
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Calcule le chemin attendu du fichier EDF brut
    raw_path = raw_dir / subject / f"{subject}{run}.edf"
    # Arrête l'exécution si l'EDF est introuvable
    if not raw_path.exists():
        # Signale explicitement le chemin absent pour guider l'utilisateur
        raise FileNotFoundError(f"EDF introuvable pour {subject} {run}: {raw_path}")
    # Crée l'arborescence cible pour déposer les .npy
    features_path.parent.mkdir(parents=True, exist_ok=True)
    # Charge l'EDF en conservant les métadonnées essentielles
    raw, _ = preprocessing.load_physionet_raw(raw_path)
    # Mappe les annotations en événements moteurs
    events, event_id, motor_labels = preprocessing.map_events_to_motor_labels(raw)
    # Découpe le signal en epochs exploitables
    epochs = preprocessing.create_epochs_from_raw(raw, events, event_id)
    # Récupère les données brutes des epochs (n_trials, n_channels, n_times)
    epochs_data = epochs.get_data(copy=True)
    # Définit un mapping stable label → entier
    label_mapping = {label: idx for idx, label in enumerate(sorted(set(motor_labels)))}
    # Convertit les labels symboliques en entiers
    numeric_labels = np.array([label_mapping[label] for label in motor_labels])
    # Persiste les epochs brutes pour déléguer l'extraction des features
    np.save(features_path, epochs_data)
    # Persiste les labels alignés
    np.save(labels_path, numeric_labels)
    # Retourne les chemins nouvellement générés
    return features_path, labels_path


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_orig(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_1(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = None
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_2(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(None, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_3(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, None, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_4(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, None)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_5(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_6(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_7(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, )
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_8(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = None

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_9(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = True

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_10(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() and not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_11(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_12(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_13(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = None
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_14(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = False
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_15(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = None
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_16(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(None, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_17(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode=None)
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_18(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_19(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, )
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_20(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="XXrXX")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_21(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="R")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_22(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = None

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_23(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(None, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_24(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode=None)

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_25(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_26(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, )

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_27(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="XXrXX")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_28(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="R")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_29(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim == EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_30(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = None
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_31(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = False
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_32(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[1] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_33(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] == candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_34(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[1]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_35(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = None

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_36(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_37(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = None

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_38(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            None, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_39(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, None, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_40(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, None, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_41(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, None
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_42(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_43(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_44(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_45(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_46(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = None
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_47(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(None)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(labels_path)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_48(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = None
    # Retourne les deux tableaux prêts pour le scoring
    return X, y


# Charge ou génère les matrices numpy attendues pour la prédiction
def x__load_data__mutmut_49(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run."""

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Indique si nous devons régénérer les .npy
    needs_rebuild = False

    # Construit les .npy depuis l'EDF si l'un d'eux manque
    if not features_path.exists() or not labels_path.exists():
        # Force une reconstruction complète pour retrouver les tensors bruts
        needs_rebuild = True
    else:
        # Charge X en mmap pour inspecter la forme sans tout charger
        candidate_X = np.load(features_path, mmap_mode="r")
        # Charge y en mmap pour inspecter la longueur
        candidate_y = np.load(labels_path, mmap_mode="r")

        # Vérifie que X est bien un tenseur 3D attendu par la pipeline
        if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
            # Relance la génération si l'ancien format tabulaire est détecté
            needs_rebuild = True
        # Vérifie l'alignement entre le nombre d'epochs et de labels
        elif candidate_X.shape[0] != candidate_y.shape[0]:
            # Relance la génération pour réaligner les données et labels
            needs_rebuild = True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        # Convertit l'EDF associé en fichiers numpy persistés
        features_path, labels_path = _build_npy_from_edf(
            subject, run, data_dir, raw_dir
        )

    # Utilise numpy.load pour récupérer les features en mémoire
    X = np.load(features_path)
    # Utilise numpy.load pour récupérer les labels associés
    y = np.load(None)
    # Retourne les deux tableaux prêts pour le scoring
    return X, y

x__load_data__mutmut_mutants : ClassVar[MutantDict] = {
'x__load_data__mutmut_1': x__load_data__mutmut_1, 
    'x__load_data__mutmut_2': x__load_data__mutmut_2, 
    'x__load_data__mutmut_3': x__load_data__mutmut_3, 
    'x__load_data__mutmut_4': x__load_data__mutmut_4, 
    'x__load_data__mutmut_5': x__load_data__mutmut_5, 
    'x__load_data__mutmut_6': x__load_data__mutmut_6, 
    'x__load_data__mutmut_7': x__load_data__mutmut_7, 
    'x__load_data__mutmut_8': x__load_data__mutmut_8, 
    'x__load_data__mutmut_9': x__load_data__mutmut_9, 
    'x__load_data__mutmut_10': x__load_data__mutmut_10, 
    'x__load_data__mutmut_11': x__load_data__mutmut_11, 
    'x__load_data__mutmut_12': x__load_data__mutmut_12, 
    'x__load_data__mutmut_13': x__load_data__mutmut_13, 
    'x__load_data__mutmut_14': x__load_data__mutmut_14, 
    'x__load_data__mutmut_15': x__load_data__mutmut_15, 
    'x__load_data__mutmut_16': x__load_data__mutmut_16, 
    'x__load_data__mutmut_17': x__load_data__mutmut_17, 
    'x__load_data__mutmut_18': x__load_data__mutmut_18, 
    'x__load_data__mutmut_19': x__load_data__mutmut_19, 
    'x__load_data__mutmut_20': x__load_data__mutmut_20, 
    'x__load_data__mutmut_21': x__load_data__mutmut_21, 
    'x__load_data__mutmut_22': x__load_data__mutmut_22, 
    'x__load_data__mutmut_23': x__load_data__mutmut_23, 
    'x__load_data__mutmut_24': x__load_data__mutmut_24, 
    'x__load_data__mutmut_25': x__load_data__mutmut_25, 
    'x__load_data__mutmut_26': x__load_data__mutmut_26, 
    'x__load_data__mutmut_27': x__load_data__mutmut_27, 
    'x__load_data__mutmut_28': x__load_data__mutmut_28, 
    'x__load_data__mutmut_29': x__load_data__mutmut_29, 
    'x__load_data__mutmut_30': x__load_data__mutmut_30, 
    'x__load_data__mutmut_31': x__load_data__mutmut_31, 
    'x__load_data__mutmut_32': x__load_data__mutmut_32, 
    'x__load_data__mutmut_33': x__load_data__mutmut_33, 
    'x__load_data__mutmut_34': x__load_data__mutmut_34, 
    'x__load_data__mutmut_35': x__load_data__mutmut_35, 
    'x__load_data__mutmut_36': x__load_data__mutmut_36, 
    'x__load_data__mutmut_37': x__load_data__mutmut_37, 
    'x__load_data__mutmut_38': x__load_data__mutmut_38, 
    'x__load_data__mutmut_39': x__load_data__mutmut_39, 
    'x__load_data__mutmut_40': x__load_data__mutmut_40, 
    'x__load_data__mutmut_41': x__load_data__mutmut_41, 
    'x__load_data__mutmut_42': x__load_data__mutmut_42, 
    'x__load_data__mutmut_43': x__load_data__mutmut_43, 
    'x__load_data__mutmut_44': x__load_data__mutmut_44, 
    'x__load_data__mutmut_45': x__load_data__mutmut_45, 
    'x__load_data__mutmut_46': x__load_data__mutmut_46, 
    'x__load_data__mutmut_47': x__load_data__mutmut_47, 
    'x__load_data__mutmut_48': x__load_data__mutmut_48, 
    'x__load_data__mutmut_49': x__load_data__mutmut_49
}

def _load_data(*args, **kwargs):
    result = _mutmut_trampoline(x__load_data__mutmut_orig, x__load_data__mutmut_mutants, args, kwargs)
    return result 

_load_data.__signature__ = _mutmut_signature(x__load_data__mutmut_orig)
x__load_data__mutmut_orig.__name__ = 'x__load_data'


# Restaure le réducteur de dimension pour valider l'artefact W
def x__load_w_matrix__mutmut_orig(path: Path) -> TPVDimReducer:
    """Recharge la matrice W persistée lors de l'entraînement."""

    # Instancie un réducteur de dimension vierge pour recharger la matrice
    reducer = TPVDimReducer()
    # Charge le contenu sérialisé pour restaurer les attributs internes
    reducer.load(path)
    # Retourne le réducteur prêt pour une projection éventuelle
    return reducer


# Restaure le réducteur de dimension pour valider l'artefact W
def x__load_w_matrix__mutmut_1(path: Path) -> TPVDimReducer:
    """Recharge la matrice W persistée lors de l'entraînement."""

    # Instancie un réducteur de dimension vierge pour recharger la matrice
    reducer = None
    # Charge le contenu sérialisé pour restaurer les attributs internes
    reducer.load(path)
    # Retourne le réducteur prêt pour une projection éventuelle
    return reducer


# Restaure le réducteur de dimension pour valider l'artefact W
def x__load_w_matrix__mutmut_2(path: Path) -> TPVDimReducer:
    """Recharge la matrice W persistée lors de l'entraînement."""

    # Instancie un réducteur de dimension vierge pour recharger la matrice
    reducer = TPVDimReducer()
    # Charge le contenu sérialisé pour restaurer les attributs internes
    reducer.load(None)
    # Retourne le réducteur prêt pour une projection éventuelle
    return reducer

x__load_w_matrix__mutmut_mutants : ClassVar[MutantDict] = {
'x__load_w_matrix__mutmut_1': x__load_w_matrix__mutmut_1, 
    'x__load_w_matrix__mutmut_2': x__load_w_matrix__mutmut_2
}

def _load_w_matrix(*args, **kwargs):
    result = _mutmut_trampoline(x__load_w_matrix__mutmut_orig, x__load_w_matrix__mutmut_mutants, args, kwargs)
    return result 

_load_w_matrix.__signature__ = _mutmut_signature(x__load_w_matrix__mutmut_orig)
x__load_w_matrix__mutmut_orig.__name__ = 'x__load_w_matrix'


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_orig(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_1(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = None
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_2(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(None)
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_3(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(None).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_4(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate(None)).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_5(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = None
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_6(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(None, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_7(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, None, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_8(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=None)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_9(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_10(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_11(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, )
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_12(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = None
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_13(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = None
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_14(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(None):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_15(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = None
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_16(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(None)
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_17(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = None
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_18(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(None)
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_19(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = None
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_20(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(None)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_21(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct * class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_22(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 1.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_23(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = None
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_24(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "XXsubjectXX": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_25(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "SUBJECT": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_26(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["XXsubjectXX"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_27(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["SUBJECT"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_28(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "XXrunXX": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_29(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "RUN": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_30(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["XXrunXX"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_31(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["RUN"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_32(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "XXaccuracyXX": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_33(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "ACCURACY": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_34(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "XXconfusion_matrixXX": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_35(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "CONFUSION_MATRIX": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_36(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "XXper_class_accuracyXX": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_37(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "PER_CLASS_ACCURACY": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_38(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "XXsamplesXX": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_39(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "SAMPLES": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_40(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = None
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_41(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir * "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_42(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "XXreport.jsonXX"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_43(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "REPORT.JSON"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_44(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(None)
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_45(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(None, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_46(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=None, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_47(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=None))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_48(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_49(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_50(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, ))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_51(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=True, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_52(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=3))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_53(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = None
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_54(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir * "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_55(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "XXclass_report.csvXX"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_56(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "CLASS_REPORT.CSV"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_57(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open(None, newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_58(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline=None) as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_59(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open(newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_60(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", ) as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_61(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("XXwXX", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_62(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("W", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_63(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="XXXX") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_64(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = None
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_65(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["XXclassXX", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_66(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["CLASS", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_67(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "XXaccuracyXX", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_68(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "ACCURACY", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_69(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "XXsupportXX"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_70(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "SUPPORT"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_71(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = None
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_72(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(None, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_73(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=None)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_74(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_75(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, )
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_76(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = None
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_77(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(None)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_78(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = None
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_79(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(None)
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_80(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(None)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_81(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.rindex(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_82(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                None
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_83(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "XXclassXX": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_84(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "CLASS": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_85(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "XXaccuracyXX": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_86(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "ACCURACY": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_87(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "XXsupportXX": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_88(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "SUPPORT": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_89(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = None
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_90(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir * "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_91(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "XXpredictions.csvXX"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_92(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "PREDICTIONS.CSV"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_93(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open(None, newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_94(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline=None) as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_95(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open(newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_96(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", ) as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_97(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("XXwXX", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_98(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("W", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_99(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="XXXX") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_100(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = None
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_101(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["XXsubjectXX", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_102(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["SUBJECT", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_103(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "XXrunXX", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_104(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "RUN", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_105(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "XXindexXX", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_106(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "INDEX", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_107(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "XXy_trueXX", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_108(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "Y_TRUE", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_109(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "XXy_predXX"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_110(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "Y_PRED"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_111(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = None
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_112(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(None, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_113(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=None)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_114(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_115(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, )
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_116(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            None
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_117(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(None, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_118(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, None, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_119(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=None)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_120(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_121(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_122(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, )
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_123(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=True)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_124(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                None
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_125(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "XXsubjectXX": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_126(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "SUBJECT": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_127(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["XXsubjectXX"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_128(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["SUBJECT"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_129(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "XXrunXX": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_130(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "RUN": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_131(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["XXrunXX"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_132(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["RUN"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_133(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "XXindexXX": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_134(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "INDEX": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_135(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "XXy_trueXX": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_136(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "Y_TRUE": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_137(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(None),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_138(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "XXy_predXX": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_139(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "Y_PRED": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_140(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(None),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_141(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "XXjson_reportXX": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_142(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "JSON_REPORT": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_143(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "XXcsv_reportXX": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_144(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "CSV_REPORT": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_145(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "XXclass_reportXX": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_146(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "CLASS_REPORT": class_report_path,
        "confusion": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_147(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "XXconfusionXX": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_148(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "CONFUSION": confusion,
        "per_class_accuracy": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_149(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "XXper_class_accuracyXX": per_class_accuracy,
    }


# Sérialise les rapports JSON et CSV pour un run donné
def x__write_reports__mutmut_150(
    target_dir: Path,
    identifiers: dict[str, str],
    y_true: np.ndarray,
    y_pred: np.ndarray,
    accuracy: float,
) -> dict:
    """Écrit les rapports de prédiction et retourne les chemins créés."""

    # Agrège les classes vues et prédites pour éviter les labels manquants
    labels = sorted(np.unique(np.concatenate((y_true, y_pred))).tolist())
    # Calcule la matrice de confusion en préservant l'ordre des labels
    confusion_array = confusion_matrix(y_true, y_pred, labels=labels)
    # Convertit la matrice en liste pour la sérialisation JSON
    confusion = confusion_array.tolist()
    # Prépare la structure d'accuracy par classe pour la CLI
    per_class_accuracy: dict[str, float] = {}
    # Calcule l'accuracy pour chaque classe en utilisant la diagonale
    for index, label in enumerate(labels):
        # Calcule le nombre total d'échantillons pour la classe courante
        class_total = int(confusion_array[index].sum())
        # Calcule le nombre de prédictions correctes pour la classe courante
        correct = int(confusion_array[index][index])
        # Enregistre l'accuracy en évitant la division par zéro
        per_class_accuracy[str(label)] = correct / class_total if class_total else 0.0
    # Prépare un rapport JSON synthétique pour la CLI et la CI
    report = {
        "subject": identifiers["subject"],
        "run": identifiers["run"],
        "accuracy": accuracy,
        "confusion_matrix": confusion,
        "per_class_accuracy": per_class_accuracy,
        "samples": len(y_true),
    }
    # Définit le chemin du rapport JSON dans les artefacts
    report_path = target_dir / "report.json"
    # Écrit le rapport JSON en UTF-8 avec indentation pour inspection
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    # Définit le chemin du rapport CSV par classe pour les diagnostics
    class_report_path = target_dir / "class_report.csv"
    # Ouvre le fichier CSV pour enregistrer accuracy et support
    with class_report_path.open("w", newline="") as handle:
        # Définit les en-têtes pour l'accuracy par classe
        fieldnames = ["class", "accuracy", "support"]
        # Construit le writer CSV prêt à écrire chaque classe
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit les en-têtes pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque classe pour détailler les métriques associées
        for label in labels:
            # Récupère l'accuracy calculée pour la classe ciblée
            class_accuracy = per_class_accuracy[str(label)]
            # Calcule le support en comptant les occurrences de la classe
            support = int(confusion_array[labels.index(label)].sum())
            # Écrit la ligne de métriques dédiée à la classe
            writer.writerow(
                {
                    "class": label,
                    "accuracy": class_accuracy,
                    "support": support,
                }
            )
    # Définit le chemin du CSV listant chaque prédiction
    csv_path = target_dir / "predictions.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with csv_path.open("w", newline="") as handle:
        # Prépare les en-têtes pour aligner vérité terrain et prédiction
        fieldnames = ["subject", "run", "index", "y_true", "y_pred"]
        # Crée un writer dict pour simplifier l'écriture ligne par ligne
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        # Inscrit l'en-tête pour faciliter la lecture
        writer.writeheader()
        # Parcourt chaque échantillon pour exposer la prédiction individuelle
        for idx, (true_label, pred_label) in enumerate(
            zip(y_true, y_pred, strict=False)
        ):
            # Écrit la ligne CSV pour l'index courant
            writer.writerow(
                {
                    "subject": identifiers["subject"],
                    "run": identifiers["run"],
                    "index": idx,
                    "y_true": int(true_label),
                    "y_pred": int(pred_label),
                }
            )
    # Retourne les chemins créés pour validation amont
    return {
        "json_report": report_path,
        "csv_report": csv_path,
        "class_report": class_report_path,
        "confusion": confusion,
        "PER_CLASS_ACCURACY": per_class_accuracy,
    }

x__write_reports__mutmut_mutants : ClassVar[MutantDict] = {
'x__write_reports__mutmut_1': x__write_reports__mutmut_1, 
    'x__write_reports__mutmut_2': x__write_reports__mutmut_2, 
    'x__write_reports__mutmut_3': x__write_reports__mutmut_3, 
    'x__write_reports__mutmut_4': x__write_reports__mutmut_4, 
    'x__write_reports__mutmut_5': x__write_reports__mutmut_5, 
    'x__write_reports__mutmut_6': x__write_reports__mutmut_6, 
    'x__write_reports__mutmut_7': x__write_reports__mutmut_7, 
    'x__write_reports__mutmut_8': x__write_reports__mutmut_8, 
    'x__write_reports__mutmut_9': x__write_reports__mutmut_9, 
    'x__write_reports__mutmut_10': x__write_reports__mutmut_10, 
    'x__write_reports__mutmut_11': x__write_reports__mutmut_11, 
    'x__write_reports__mutmut_12': x__write_reports__mutmut_12, 
    'x__write_reports__mutmut_13': x__write_reports__mutmut_13, 
    'x__write_reports__mutmut_14': x__write_reports__mutmut_14, 
    'x__write_reports__mutmut_15': x__write_reports__mutmut_15, 
    'x__write_reports__mutmut_16': x__write_reports__mutmut_16, 
    'x__write_reports__mutmut_17': x__write_reports__mutmut_17, 
    'x__write_reports__mutmut_18': x__write_reports__mutmut_18, 
    'x__write_reports__mutmut_19': x__write_reports__mutmut_19, 
    'x__write_reports__mutmut_20': x__write_reports__mutmut_20, 
    'x__write_reports__mutmut_21': x__write_reports__mutmut_21, 
    'x__write_reports__mutmut_22': x__write_reports__mutmut_22, 
    'x__write_reports__mutmut_23': x__write_reports__mutmut_23, 
    'x__write_reports__mutmut_24': x__write_reports__mutmut_24, 
    'x__write_reports__mutmut_25': x__write_reports__mutmut_25, 
    'x__write_reports__mutmut_26': x__write_reports__mutmut_26, 
    'x__write_reports__mutmut_27': x__write_reports__mutmut_27, 
    'x__write_reports__mutmut_28': x__write_reports__mutmut_28, 
    'x__write_reports__mutmut_29': x__write_reports__mutmut_29, 
    'x__write_reports__mutmut_30': x__write_reports__mutmut_30, 
    'x__write_reports__mutmut_31': x__write_reports__mutmut_31, 
    'x__write_reports__mutmut_32': x__write_reports__mutmut_32, 
    'x__write_reports__mutmut_33': x__write_reports__mutmut_33, 
    'x__write_reports__mutmut_34': x__write_reports__mutmut_34, 
    'x__write_reports__mutmut_35': x__write_reports__mutmut_35, 
    'x__write_reports__mutmut_36': x__write_reports__mutmut_36, 
    'x__write_reports__mutmut_37': x__write_reports__mutmut_37, 
    'x__write_reports__mutmut_38': x__write_reports__mutmut_38, 
    'x__write_reports__mutmut_39': x__write_reports__mutmut_39, 
    'x__write_reports__mutmut_40': x__write_reports__mutmut_40, 
    'x__write_reports__mutmut_41': x__write_reports__mutmut_41, 
    'x__write_reports__mutmut_42': x__write_reports__mutmut_42, 
    'x__write_reports__mutmut_43': x__write_reports__mutmut_43, 
    'x__write_reports__mutmut_44': x__write_reports__mutmut_44, 
    'x__write_reports__mutmut_45': x__write_reports__mutmut_45, 
    'x__write_reports__mutmut_46': x__write_reports__mutmut_46, 
    'x__write_reports__mutmut_47': x__write_reports__mutmut_47, 
    'x__write_reports__mutmut_48': x__write_reports__mutmut_48, 
    'x__write_reports__mutmut_49': x__write_reports__mutmut_49, 
    'x__write_reports__mutmut_50': x__write_reports__mutmut_50, 
    'x__write_reports__mutmut_51': x__write_reports__mutmut_51, 
    'x__write_reports__mutmut_52': x__write_reports__mutmut_52, 
    'x__write_reports__mutmut_53': x__write_reports__mutmut_53, 
    'x__write_reports__mutmut_54': x__write_reports__mutmut_54, 
    'x__write_reports__mutmut_55': x__write_reports__mutmut_55, 
    'x__write_reports__mutmut_56': x__write_reports__mutmut_56, 
    'x__write_reports__mutmut_57': x__write_reports__mutmut_57, 
    'x__write_reports__mutmut_58': x__write_reports__mutmut_58, 
    'x__write_reports__mutmut_59': x__write_reports__mutmut_59, 
    'x__write_reports__mutmut_60': x__write_reports__mutmut_60, 
    'x__write_reports__mutmut_61': x__write_reports__mutmut_61, 
    'x__write_reports__mutmut_62': x__write_reports__mutmut_62, 
    'x__write_reports__mutmut_63': x__write_reports__mutmut_63, 
    'x__write_reports__mutmut_64': x__write_reports__mutmut_64, 
    'x__write_reports__mutmut_65': x__write_reports__mutmut_65, 
    'x__write_reports__mutmut_66': x__write_reports__mutmut_66, 
    'x__write_reports__mutmut_67': x__write_reports__mutmut_67, 
    'x__write_reports__mutmut_68': x__write_reports__mutmut_68, 
    'x__write_reports__mutmut_69': x__write_reports__mutmut_69, 
    'x__write_reports__mutmut_70': x__write_reports__mutmut_70, 
    'x__write_reports__mutmut_71': x__write_reports__mutmut_71, 
    'x__write_reports__mutmut_72': x__write_reports__mutmut_72, 
    'x__write_reports__mutmut_73': x__write_reports__mutmut_73, 
    'x__write_reports__mutmut_74': x__write_reports__mutmut_74, 
    'x__write_reports__mutmut_75': x__write_reports__mutmut_75, 
    'x__write_reports__mutmut_76': x__write_reports__mutmut_76, 
    'x__write_reports__mutmut_77': x__write_reports__mutmut_77, 
    'x__write_reports__mutmut_78': x__write_reports__mutmut_78, 
    'x__write_reports__mutmut_79': x__write_reports__mutmut_79, 
    'x__write_reports__mutmut_80': x__write_reports__mutmut_80, 
    'x__write_reports__mutmut_81': x__write_reports__mutmut_81, 
    'x__write_reports__mutmut_82': x__write_reports__mutmut_82, 
    'x__write_reports__mutmut_83': x__write_reports__mutmut_83, 
    'x__write_reports__mutmut_84': x__write_reports__mutmut_84, 
    'x__write_reports__mutmut_85': x__write_reports__mutmut_85, 
    'x__write_reports__mutmut_86': x__write_reports__mutmut_86, 
    'x__write_reports__mutmut_87': x__write_reports__mutmut_87, 
    'x__write_reports__mutmut_88': x__write_reports__mutmut_88, 
    'x__write_reports__mutmut_89': x__write_reports__mutmut_89, 
    'x__write_reports__mutmut_90': x__write_reports__mutmut_90, 
    'x__write_reports__mutmut_91': x__write_reports__mutmut_91, 
    'x__write_reports__mutmut_92': x__write_reports__mutmut_92, 
    'x__write_reports__mutmut_93': x__write_reports__mutmut_93, 
    'x__write_reports__mutmut_94': x__write_reports__mutmut_94, 
    'x__write_reports__mutmut_95': x__write_reports__mutmut_95, 
    'x__write_reports__mutmut_96': x__write_reports__mutmut_96, 
    'x__write_reports__mutmut_97': x__write_reports__mutmut_97, 
    'x__write_reports__mutmut_98': x__write_reports__mutmut_98, 
    'x__write_reports__mutmut_99': x__write_reports__mutmut_99, 
    'x__write_reports__mutmut_100': x__write_reports__mutmut_100, 
    'x__write_reports__mutmut_101': x__write_reports__mutmut_101, 
    'x__write_reports__mutmut_102': x__write_reports__mutmut_102, 
    'x__write_reports__mutmut_103': x__write_reports__mutmut_103, 
    'x__write_reports__mutmut_104': x__write_reports__mutmut_104, 
    'x__write_reports__mutmut_105': x__write_reports__mutmut_105, 
    'x__write_reports__mutmut_106': x__write_reports__mutmut_106, 
    'x__write_reports__mutmut_107': x__write_reports__mutmut_107, 
    'x__write_reports__mutmut_108': x__write_reports__mutmut_108, 
    'x__write_reports__mutmut_109': x__write_reports__mutmut_109, 
    'x__write_reports__mutmut_110': x__write_reports__mutmut_110, 
    'x__write_reports__mutmut_111': x__write_reports__mutmut_111, 
    'x__write_reports__mutmut_112': x__write_reports__mutmut_112, 
    'x__write_reports__mutmut_113': x__write_reports__mutmut_113, 
    'x__write_reports__mutmut_114': x__write_reports__mutmut_114, 
    'x__write_reports__mutmut_115': x__write_reports__mutmut_115, 
    'x__write_reports__mutmut_116': x__write_reports__mutmut_116, 
    'x__write_reports__mutmut_117': x__write_reports__mutmut_117, 
    'x__write_reports__mutmut_118': x__write_reports__mutmut_118, 
    'x__write_reports__mutmut_119': x__write_reports__mutmut_119, 
    'x__write_reports__mutmut_120': x__write_reports__mutmut_120, 
    'x__write_reports__mutmut_121': x__write_reports__mutmut_121, 
    'x__write_reports__mutmut_122': x__write_reports__mutmut_122, 
    'x__write_reports__mutmut_123': x__write_reports__mutmut_123, 
    'x__write_reports__mutmut_124': x__write_reports__mutmut_124, 
    'x__write_reports__mutmut_125': x__write_reports__mutmut_125, 
    'x__write_reports__mutmut_126': x__write_reports__mutmut_126, 
    'x__write_reports__mutmut_127': x__write_reports__mutmut_127, 
    'x__write_reports__mutmut_128': x__write_reports__mutmut_128, 
    'x__write_reports__mutmut_129': x__write_reports__mutmut_129, 
    'x__write_reports__mutmut_130': x__write_reports__mutmut_130, 
    'x__write_reports__mutmut_131': x__write_reports__mutmut_131, 
    'x__write_reports__mutmut_132': x__write_reports__mutmut_132, 
    'x__write_reports__mutmut_133': x__write_reports__mutmut_133, 
    'x__write_reports__mutmut_134': x__write_reports__mutmut_134, 
    'x__write_reports__mutmut_135': x__write_reports__mutmut_135, 
    'x__write_reports__mutmut_136': x__write_reports__mutmut_136, 
    'x__write_reports__mutmut_137': x__write_reports__mutmut_137, 
    'x__write_reports__mutmut_138': x__write_reports__mutmut_138, 
    'x__write_reports__mutmut_139': x__write_reports__mutmut_139, 
    'x__write_reports__mutmut_140': x__write_reports__mutmut_140, 
    'x__write_reports__mutmut_141': x__write_reports__mutmut_141, 
    'x__write_reports__mutmut_142': x__write_reports__mutmut_142, 
    'x__write_reports__mutmut_143': x__write_reports__mutmut_143, 
    'x__write_reports__mutmut_144': x__write_reports__mutmut_144, 
    'x__write_reports__mutmut_145': x__write_reports__mutmut_145, 
    'x__write_reports__mutmut_146': x__write_reports__mutmut_146, 
    'x__write_reports__mutmut_147': x__write_reports__mutmut_147, 
    'x__write_reports__mutmut_148': x__write_reports__mutmut_148, 
    'x__write_reports__mutmut_149': x__write_reports__mutmut_149, 
    'x__write_reports__mutmut_150': x__write_reports__mutmut_150
}

def _write_reports(*args, **kwargs):
    result = _mutmut_trampoline(x__write_reports__mutmut_orig, x__write_reports__mutmut_mutants, args, kwargs)
    return result 

_write_reports.__signature__ = _mutmut_signature(x__write_reports__mutmut_orig)
x__write_reports__mutmut_orig.__name__ = 'x__write_reports'


# Entraîne un modèle par défaut lorsqu'aucun artefact n'est disponible
def _train_missing_pipeline(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path,
) -> None:
    """Construit un pipeline FFT/PCA/LDA lorsque le modèle manque."""

    # Utilise la fréquence de référence pour aligner extraction et entraînement
    pipeline_config = PipelineConfig(
        sfreq=DEFAULT_SAMPLING_RATE,
        feature_strategy="fft",
        normalize_features=True,
        dim_method="pca",
        n_components=None,
        classifier="lda",
        scaler=None,
    )
    # Prépare la requête pour déléguer l'entraînement à scripts.train
    request = TrainingRequest(
        subject=subject,
        run=run,
        pipeline_config=pipeline_config,
        data_dir=data_dir,
        artifacts_dir=artifacts_dir,
        raw_dir=raw_dir,
    )
    # Lance l'entraînement pour matérialiser model.joblib et w_matrix.joblib
    run_training(request)


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_orig(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_1(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = None
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_2(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(None, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_3(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, None, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_4(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, None, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_5(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, None)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_6(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_7(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_8(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_9(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, )
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_10(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = None
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_11(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject * run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_12(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir * subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_13(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=None, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_14(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=None)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_15(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_16(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, )
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_17(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=False, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_18(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=False)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_19(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = None
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_20(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir * "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_21(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "XXmodel.joblibXX"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_22(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "MODEL.JOBLIB"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_23(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = None
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_24(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir * "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_25(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "XXw_matrix.joblibXX"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_26(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "W_MATRIX.JOBLIB"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_27(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() and not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_28(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_29(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_30(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            None
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_31(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "XXINFO: modèle absent pour XX"
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_32(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "info: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_33(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: MODÈLE ABSENT POUR "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_34(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(None, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_35(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, None, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_36(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, None, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_37(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, None, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_38(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, None)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_39(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_40(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_41(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_42(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_43(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, )
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_44(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = None
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_45(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(None)
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_46(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(None))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_47(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = None
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_48(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(None)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_49(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = None
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_50(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(None)
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_51(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(None, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_52(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, None))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_53(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_54(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, ))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_55(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = None
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_56(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(None)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_57(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = None
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_58(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        None,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_59(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        None,
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_60(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        None,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_61(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        None,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_62(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        None,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_63(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_64(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_65(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_66(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_67(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_68(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"XXsubjectXX": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_69(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"SUBJECT": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_70(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "XXrunXX": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_71(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "RUN": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_72(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "XXrunXX": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_73(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "RUN": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_74(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "XXsubjectXX": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_75(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "SUBJECT": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_76(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "XXaccuracyXX": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_77(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "ACCURACY": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_78(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "XXw_matrixXX": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_79(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "W_MATRIX": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_80(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "XXreportsXX": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_81(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "REPORTS": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_82(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "XXpredictionsXX": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_83(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "PREDICTIONS": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_84(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "XXtruthXX": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_85(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "TRUTH": y,  # <--- clé attendue par mybci
        "y_true": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_86(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "XXy_trueXX": y,  # Aligne la clé avec les attentes des tests CLI
    }


# Évalue un run donné et produit un rapport structuré
def x_evaluate_run__mutmut_87(
    subject: str,
    run: str,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path = DEFAULT_RAW_DIR,
) -> dict:
    """Évalue l'accuracy d'un run en rechargeant le pipeline entraîné."""

    # Charge ou génère les tableaux numpy nécessaires au scoring
    X, y = _load_data(subject, run, data_dir, raw_dir)
    # Construit le dossier d'artefacts spécifique au sujet et au run
    target_dir = artifacts_dir / subject / run
    # Assure la présence du dossier pour pouvoir écrire les rapports
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule les chemins des artefacts attendus pour détecter les absences
    model_path = target_dir / "model.joblib"
    # Vérifie la présence de la matrice W utilisée par le temps-réel
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Déclenche un entraînement si le modèle ou la matrice sont manquants
    if not model_path.exists() or not w_matrix_path.exists():
        # Informe l'utilisateur qu'un auto-train est lancé pour ce couple
        print(
            "INFO: modèle absent pour "
            f"{subject} {run}, entraînement automatique en cours..."
        )
        # Génère les artefacts de base pour permettre l'évaluation
        _train_missing_pipeline(subject, run, data_dir, artifacts_dir, raw_dir)
    # Charge la pipeline entraînée depuis le joblib sauvegardé
    pipeline = load_pipeline(str(model_path))
    # Génère les prédictions individuelles pour le rapport
    y_pred = pipeline.predict(X)
    # Calcule l'accuracy du pipeline sur les données fournies
    accuracy = float(pipeline.score(X, y))
    # Recharge la matrice W pour confirmer sa présence
    w_matrix = _load_w_matrix(w_matrix_path)
    # Écrit les rapports JSON et CSV dans le dossier d'artefacts
    reports = _write_reports(
        target_dir,
        {"subject": subject, "run": run},
        y,
        y_pred,
        accuracy,
    )
    # Retourne le rapport local incluant la matrice pour les tests
    return {
        "run": run,
        "subject": subject,
        "accuracy": accuracy,
        "w_matrix": w_matrix,
        "reports": reports,
        "predictions": y_pred,
        "truth": y,  # <--- clé attendue par mybci
        "Y_TRUE": y,  # Aligne la clé avec les attentes des tests CLI
    }

x_evaluate_run__mutmut_mutants : ClassVar[MutantDict] = {
'x_evaluate_run__mutmut_1': x_evaluate_run__mutmut_1, 
    'x_evaluate_run__mutmut_2': x_evaluate_run__mutmut_2, 
    'x_evaluate_run__mutmut_3': x_evaluate_run__mutmut_3, 
    'x_evaluate_run__mutmut_4': x_evaluate_run__mutmut_4, 
    'x_evaluate_run__mutmut_5': x_evaluate_run__mutmut_5, 
    'x_evaluate_run__mutmut_6': x_evaluate_run__mutmut_6, 
    'x_evaluate_run__mutmut_7': x_evaluate_run__mutmut_7, 
    'x_evaluate_run__mutmut_8': x_evaluate_run__mutmut_8, 
    'x_evaluate_run__mutmut_9': x_evaluate_run__mutmut_9, 
    'x_evaluate_run__mutmut_10': x_evaluate_run__mutmut_10, 
    'x_evaluate_run__mutmut_11': x_evaluate_run__mutmut_11, 
    'x_evaluate_run__mutmut_12': x_evaluate_run__mutmut_12, 
    'x_evaluate_run__mutmut_13': x_evaluate_run__mutmut_13, 
    'x_evaluate_run__mutmut_14': x_evaluate_run__mutmut_14, 
    'x_evaluate_run__mutmut_15': x_evaluate_run__mutmut_15, 
    'x_evaluate_run__mutmut_16': x_evaluate_run__mutmut_16, 
    'x_evaluate_run__mutmut_17': x_evaluate_run__mutmut_17, 
    'x_evaluate_run__mutmut_18': x_evaluate_run__mutmut_18, 
    'x_evaluate_run__mutmut_19': x_evaluate_run__mutmut_19, 
    'x_evaluate_run__mutmut_20': x_evaluate_run__mutmut_20, 
    'x_evaluate_run__mutmut_21': x_evaluate_run__mutmut_21, 
    'x_evaluate_run__mutmut_22': x_evaluate_run__mutmut_22, 
    'x_evaluate_run__mutmut_23': x_evaluate_run__mutmut_23, 
    'x_evaluate_run__mutmut_24': x_evaluate_run__mutmut_24, 
    'x_evaluate_run__mutmut_25': x_evaluate_run__mutmut_25, 
    'x_evaluate_run__mutmut_26': x_evaluate_run__mutmut_26, 
    'x_evaluate_run__mutmut_27': x_evaluate_run__mutmut_27, 
    'x_evaluate_run__mutmut_28': x_evaluate_run__mutmut_28, 
    'x_evaluate_run__mutmut_29': x_evaluate_run__mutmut_29, 
    'x_evaluate_run__mutmut_30': x_evaluate_run__mutmut_30, 
    'x_evaluate_run__mutmut_31': x_evaluate_run__mutmut_31, 
    'x_evaluate_run__mutmut_32': x_evaluate_run__mutmut_32, 
    'x_evaluate_run__mutmut_33': x_evaluate_run__mutmut_33, 
    'x_evaluate_run__mutmut_34': x_evaluate_run__mutmut_34, 
    'x_evaluate_run__mutmut_35': x_evaluate_run__mutmut_35, 
    'x_evaluate_run__mutmut_36': x_evaluate_run__mutmut_36, 
    'x_evaluate_run__mutmut_37': x_evaluate_run__mutmut_37, 
    'x_evaluate_run__mutmut_38': x_evaluate_run__mutmut_38, 
    'x_evaluate_run__mutmut_39': x_evaluate_run__mutmut_39, 
    'x_evaluate_run__mutmut_40': x_evaluate_run__mutmut_40, 
    'x_evaluate_run__mutmut_41': x_evaluate_run__mutmut_41, 
    'x_evaluate_run__mutmut_42': x_evaluate_run__mutmut_42, 
    'x_evaluate_run__mutmut_43': x_evaluate_run__mutmut_43, 
    'x_evaluate_run__mutmut_44': x_evaluate_run__mutmut_44, 
    'x_evaluate_run__mutmut_45': x_evaluate_run__mutmut_45, 
    'x_evaluate_run__mutmut_46': x_evaluate_run__mutmut_46, 
    'x_evaluate_run__mutmut_47': x_evaluate_run__mutmut_47, 
    'x_evaluate_run__mutmut_48': x_evaluate_run__mutmut_48, 
    'x_evaluate_run__mutmut_49': x_evaluate_run__mutmut_49, 
    'x_evaluate_run__mutmut_50': x_evaluate_run__mutmut_50, 
    'x_evaluate_run__mutmut_51': x_evaluate_run__mutmut_51, 
    'x_evaluate_run__mutmut_52': x_evaluate_run__mutmut_52, 
    'x_evaluate_run__mutmut_53': x_evaluate_run__mutmut_53, 
    'x_evaluate_run__mutmut_54': x_evaluate_run__mutmut_54, 
    'x_evaluate_run__mutmut_55': x_evaluate_run__mutmut_55, 
    'x_evaluate_run__mutmut_56': x_evaluate_run__mutmut_56, 
    'x_evaluate_run__mutmut_57': x_evaluate_run__mutmut_57, 
    'x_evaluate_run__mutmut_58': x_evaluate_run__mutmut_58, 
    'x_evaluate_run__mutmut_59': x_evaluate_run__mutmut_59, 
    'x_evaluate_run__mutmut_60': x_evaluate_run__mutmut_60, 
    'x_evaluate_run__mutmut_61': x_evaluate_run__mutmut_61, 
    'x_evaluate_run__mutmut_62': x_evaluate_run__mutmut_62, 
    'x_evaluate_run__mutmut_63': x_evaluate_run__mutmut_63, 
    'x_evaluate_run__mutmut_64': x_evaluate_run__mutmut_64, 
    'x_evaluate_run__mutmut_65': x_evaluate_run__mutmut_65, 
    'x_evaluate_run__mutmut_66': x_evaluate_run__mutmut_66, 
    'x_evaluate_run__mutmut_67': x_evaluate_run__mutmut_67, 
    'x_evaluate_run__mutmut_68': x_evaluate_run__mutmut_68, 
    'x_evaluate_run__mutmut_69': x_evaluate_run__mutmut_69, 
    'x_evaluate_run__mutmut_70': x_evaluate_run__mutmut_70, 
    'x_evaluate_run__mutmut_71': x_evaluate_run__mutmut_71, 
    'x_evaluate_run__mutmut_72': x_evaluate_run__mutmut_72, 
    'x_evaluate_run__mutmut_73': x_evaluate_run__mutmut_73, 
    'x_evaluate_run__mutmut_74': x_evaluate_run__mutmut_74, 
    'x_evaluate_run__mutmut_75': x_evaluate_run__mutmut_75, 
    'x_evaluate_run__mutmut_76': x_evaluate_run__mutmut_76, 
    'x_evaluate_run__mutmut_77': x_evaluate_run__mutmut_77, 
    'x_evaluate_run__mutmut_78': x_evaluate_run__mutmut_78, 
    'x_evaluate_run__mutmut_79': x_evaluate_run__mutmut_79, 
    'x_evaluate_run__mutmut_80': x_evaluate_run__mutmut_80, 
    'x_evaluate_run__mutmut_81': x_evaluate_run__mutmut_81, 
    'x_evaluate_run__mutmut_82': x_evaluate_run__mutmut_82, 
    'x_evaluate_run__mutmut_83': x_evaluate_run__mutmut_83, 
    'x_evaluate_run__mutmut_84': x_evaluate_run__mutmut_84, 
    'x_evaluate_run__mutmut_85': x_evaluate_run__mutmut_85, 
    'x_evaluate_run__mutmut_86': x_evaluate_run__mutmut_86, 
    'x_evaluate_run__mutmut_87': x_evaluate_run__mutmut_87
}

def evaluate_run(*args, **kwargs):
    result = _mutmut_trampoline(x_evaluate_run__mutmut_orig, x_evaluate_run__mutmut_mutants, args, kwargs)
    return result 

evaluate_run.__signature__ = _mutmut_signature(x_evaluate_run__mutmut_orig)
x_evaluate_run__mutmut_orig.__name__ = 'x_evaluate_run'


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_orig(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_1(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = None
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_2(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["XXaccuracyXX"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_3(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["ACCURACY"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_4(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = None
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_5(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["XXrunXX"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_6(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["RUN"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_7(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = None
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_8(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["XXsubjectXX"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_9(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["SUBJECT"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_10(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = None
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_11(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = None
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_12(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["XXreportsXX"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_13(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["REPORTS"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_14(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["XXconfusionXX"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_15(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["CONFUSION"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_16(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "XXby_runXX": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_17(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "BY_RUN": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_18(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "XXby_subjectXX": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_19(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "BY_SUBJECT": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_20(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "XXglobalXX": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_21(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "GLOBAL": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_22(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "XXconfusion_matrixXX": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_23(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "CONFUSION_MATRIX": confusion,
        "reports": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_24(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "XXreportsXX": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_25(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "REPORTS": result["reports"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_26(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["XXreportsXX"],
    }


# Construit un rapport agrégé par run, sujet et global
def x_build_report__mutmut_27(result: dict) -> dict:
    """Structure les métriques d'accuracy pour exploitation CLI."""

    # Extrait l'accuracy du run pour construire l'agrégation
    accuracy = result["accuracy"]
    # Calcule l'accuracy par run sous forme de mapping
    by_run = {result["run"]: accuracy}
    # Calcule l'accuracy par sujet en regroupant le run unique
    by_subject = {result["subject"]: accuracy}
    # Calcule l'accuracy globale sur le run fourni
    global_accuracy = accuracy
    # Récupère la matrice de confusion construite lors de l'évaluation
    confusion = result["reports"]["confusion"]
    # Retourne une structure prête à être sérialisée
    return {
        "by_run": by_run,
        "by_subject": by_subject,
        "global": global_accuracy,
        "confusion_matrix": confusion,
        "reports": result["REPORTS"],
    }

x_build_report__mutmut_mutants : ClassVar[MutantDict] = {
'x_build_report__mutmut_1': x_build_report__mutmut_1, 
    'x_build_report__mutmut_2': x_build_report__mutmut_2, 
    'x_build_report__mutmut_3': x_build_report__mutmut_3, 
    'x_build_report__mutmut_4': x_build_report__mutmut_4, 
    'x_build_report__mutmut_5': x_build_report__mutmut_5, 
    'x_build_report__mutmut_6': x_build_report__mutmut_6, 
    'x_build_report__mutmut_7': x_build_report__mutmut_7, 
    'x_build_report__mutmut_8': x_build_report__mutmut_8, 
    'x_build_report__mutmut_9': x_build_report__mutmut_9, 
    'x_build_report__mutmut_10': x_build_report__mutmut_10, 
    'x_build_report__mutmut_11': x_build_report__mutmut_11, 
    'x_build_report__mutmut_12': x_build_report__mutmut_12, 
    'x_build_report__mutmut_13': x_build_report__mutmut_13, 
    'x_build_report__mutmut_14': x_build_report__mutmut_14, 
    'x_build_report__mutmut_15': x_build_report__mutmut_15, 
    'x_build_report__mutmut_16': x_build_report__mutmut_16, 
    'x_build_report__mutmut_17': x_build_report__mutmut_17, 
    'x_build_report__mutmut_18': x_build_report__mutmut_18, 
    'x_build_report__mutmut_19': x_build_report__mutmut_19, 
    'x_build_report__mutmut_20': x_build_report__mutmut_20, 
    'x_build_report__mutmut_21': x_build_report__mutmut_21, 
    'x_build_report__mutmut_22': x_build_report__mutmut_22, 
    'x_build_report__mutmut_23': x_build_report__mutmut_23, 
    'x_build_report__mutmut_24': x_build_report__mutmut_24, 
    'x_build_report__mutmut_25': x_build_report__mutmut_25, 
    'x_build_report__mutmut_26': x_build_report__mutmut_26, 
    'x_build_report__mutmut_27': x_build_report__mutmut_27
}

def build_report(*args, **kwargs):
    result = _mutmut_trampoline(x_build_report__mutmut_orig, x_build_report__mutmut_mutants, args, kwargs)
    return result 

build_report.__signature__ = _mutmut_signature(x_build_report__mutmut_orig)
x_build_report__mutmut_orig.__name__ = 'x_build_report'


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_orig(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_1(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = None
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_2(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = None
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_3(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(None)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_4(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = None
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_5(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        None, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_6(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, None, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_7(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, None, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_8(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, None, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_9(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, None
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_10(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_11(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_12(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_13(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_14(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_15(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = None

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_16(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(None)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_17(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = None
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_18(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["XXpredictionsXX"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_19(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["PREDICTIONS"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_20(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = None

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_21(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["XXy_trueXX"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_22(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["Y_TRUE"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_23(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print(None)

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_24(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("XXepoch nb: [prediction] [truth] equal?XX")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_25(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("EPOCH NB: [PREDICTION] [TRUTH] EQUAL?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_26(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(None):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_27(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(None, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_28(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, None, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_29(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=None)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_30(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_31(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_32(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, )):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_33(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=False)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_34(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = None
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_35(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(None)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_36(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred != true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_37(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(None)

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_38(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(None)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_39(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(None)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_40(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(None)

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_41(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['XXaccuracyXX']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_42(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['ACCURACY']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_43(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'évaluation."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Évalue le run demandé et récupère la matrice W
    result = evaluate_run(
        args.subject, args.run, args.data_dir, args.artifacts_dir, args.raw_dir
    )
    # Construit le rapport structuré attendu par les tests
    _ = build_report(result)

    # Récupère les prédictions et la vérité terrain pour l'affichage CLI
    y_pred = result["predictions"]
    y_true = result["y_true"]

    # En-tête identique à la version de référence
    print("epoch nb: [prediction] [truth] equal?")

    # Affiche une ligne par epoch : numéro, prédiction, vérité, égalité
    for idx, (pred, true) in enumerate(zip(y_pred, y_true, strict=True)):
        equal = bool(pred == true)
        print(f"epoch {idx:02d}: [{int(pred)}] [{int(true)}] {equal}")

    # Affiche l'accuracy avec 4 décimales comme dans l'exemple
    print(f"Accuracy: {result['accuracy']:.4f}")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 1

x_main__mutmut_mutants : ClassVar[MutantDict] = {
'x_main__mutmut_1': x_main__mutmut_1, 
    'x_main__mutmut_2': x_main__mutmut_2, 
    'x_main__mutmut_3': x_main__mutmut_3, 
    'x_main__mutmut_4': x_main__mutmut_4, 
    'x_main__mutmut_5': x_main__mutmut_5, 
    'x_main__mutmut_6': x_main__mutmut_6, 
    'x_main__mutmut_7': x_main__mutmut_7, 
    'x_main__mutmut_8': x_main__mutmut_8, 
    'x_main__mutmut_9': x_main__mutmut_9, 
    'x_main__mutmut_10': x_main__mutmut_10, 
    'x_main__mutmut_11': x_main__mutmut_11, 
    'x_main__mutmut_12': x_main__mutmut_12, 
    'x_main__mutmut_13': x_main__mutmut_13, 
    'x_main__mutmut_14': x_main__mutmut_14, 
    'x_main__mutmut_15': x_main__mutmut_15, 
    'x_main__mutmut_16': x_main__mutmut_16, 
    'x_main__mutmut_17': x_main__mutmut_17, 
    'x_main__mutmut_18': x_main__mutmut_18, 
    'x_main__mutmut_19': x_main__mutmut_19, 
    'x_main__mutmut_20': x_main__mutmut_20, 
    'x_main__mutmut_21': x_main__mutmut_21, 
    'x_main__mutmut_22': x_main__mutmut_22, 
    'x_main__mutmut_23': x_main__mutmut_23, 
    'x_main__mutmut_24': x_main__mutmut_24, 
    'x_main__mutmut_25': x_main__mutmut_25, 
    'x_main__mutmut_26': x_main__mutmut_26, 
    'x_main__mutmut_27': x_main__mutmut_27, 
    'x_main__mutmut_28': x_main__mutmut_28, 
    'x_main__mutmut_29': x_main__mutmut_29, 
    'x_main__mutmut_30': x_main__mutmut_30, 
    'x_main__mutmut_31': x_main__mutmut_31, 
    'x_main__mutmut_32': x_main__mutmut_32, 
    'x_main__mutmut_33': x_main__mutmut_33, 
    'x_main__mutmut_34': x_main__mutmut_34, 
    'x_main__mutmut_35': x_main__mutmut_35, 
    'x_main__mutmut_36': x_main__mutmut_36, 
    'x_main__mutmut_37': x_main__mutmut_37, 
    'x_main__mutmut_38': x_main__mutmut_38, 
    'x_main__mutmut_39': x_main__mutmut_39, 
    'x_main__mutmut_40': x_main__mutmut_40, 
    'x_main__mutmut_41': x_main__mutmut_41, 
    'x_main__mutmut_42': x_main__mutmut_42, 
    'x_main__mutmut_43': x_main__mutmut_43
}

def main(*args, **kwargs):
    result = _mutmut_trampoline(x_main__mutmut_orig, x_main__mutmut_mutants, args, kwargs)
    return result 

main.__signature__ = _mutmut_signature(x_main__mutmut_orig)
x_main__mutmut_orig.__name__ = 'x_main'


# Protège l'exécution directe pour exposer un exit code explicite
if __name__ == "__main__":  # pragma: no cover - exécution CLI directe
    # Retourne l'issue du main comme code de sortie du processus
    raise SystemExit(main())
