"""CLI d'entraînement pour le pipeline TPV."""

# Préserve argparse pour exposer une interface CLI homogène avec mybci
# Expose les primitives d'analyse des arguments CLI
# Fournit le parsing CLI pour aligner la signature mybci
import argparse

# Fournit l'écriture CSV pour exposer un manifeste tabulaire
import csv

# Fournit la sérialisation JSON pour exposer un manifeste exploitable
import json

# Rassemble la construction de structures immuables orientées données
from dataclasses import asdict, dataclass

# Garantit l'accès aux chemins portables pour données et artefacts
from pathlib import Path

# Expose cast pour documenter les conversions de types
from typing import cast

# Offre la persistance dédiée aux objets scikit-learn pour inspection séparée
import joblib

# Centralise l'accès aux tableaux manipulés par scikit-learn
import numpy as np

# Fournit la validation croisée pour évaluer la pipeline complète
from sklearn.model_selection import StratifiedKFold, cross_val_score

# Centralise le parsing et le contrôle qualité des fichiers EDF
# Extrait les features fréquentielles depuis des epochs EEG
from tpv import preprocessing

# Permet de persister séparément la matrice W apprise
from tpv.dimensionality import TPVDimReducer

# Assemble la pipeline cohérente pour l'entraînement
from tpv.pipeline import PipelineConfig, build_pipeline, save_pipeline


# Déclare la liste des runs moteurs à couvrir pour l'entraînement massif
MOTOR_RUNS = (
    # Couvre le run moteur R03 documenté dans le protocole Physionet
    "R03",
    # Couvre le run moteur R04 documenté dans le protocole Physionet
    "R04",
    # Couvre le run moteur R05 documenté dans le protocole Physionet
    "R05",
    # Couvre le run moteur R06 documenté dans le protocole Physionet
    "R06",
    # Couvre le run moteur R07 documenté dans le protocole Physionet
    "R07",
    # Couvre le run moteur R08 documenté dans le protocole Physionet
    "R08",
    # Couvre le run moteur R09 documenté dans le protocole Physionet
    "R09",
    # Couvre le run moteur R10 documenté dans le protocole Physionet
    "R10",
    # Couvre le run moteur R11 documenté dans le protocole Physionet
    "R11",
    # Couvre le run moteur R12 documenté dans le protocole Physionet
    "R12",
    # Couvre le run moteur R13 documenté dans le protocole Physionet
    "R13",
    # Couvre le run moteur R14 documenté dans le protocole Physionet
    "R14",
)

# Définit le répertoire par défaut où chercher les enregistrements
DEFAULT_DATA_DIR = Path("data")

# Fixe la dimension attendue pour les matrices de features en mémoire
EXPECTED_FEATURES_DIMENSIONS = 3

# Définit le répertoire par défaut pour déposer les artefacts d'entraînement
DEFAULT_ARTIFACTS_DIR = Path("artifacts")

# Définit le répertoire par défaut où résident les fichiers EDF bruts
DEFAULT_RAW_DIR = Path("data")

# Fige la fréquence d'échantillonnage par défaut utilisée pour les features
DEFAULT_SAMPLING_RATE = 50.0

# Déclare le nombre cible de splits utilisé pour la validation croisée
DEFAULT_CV_SPLITS = 10

# Fixe le nombre minimal de splits pour déclencher la validation croisée
MIN_CV_SPLITS = 3


# Stabilise la reproductibilité des splits de cross-validation
DEFAULT_RANDOM_STATE = 42
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


# Regroupe toutes les informations nécessaires à un run d'entraînement
@dataclass
class TrainingRequest:
    """Décrit les paramètres nécessaires pour entraîner un run."""

    # Identifie le sujet cible pour l'entraînement
    subject: str
    # Identifie le run ciblé pour le sujet sélectionné
    run: str
    # Transporte la configuration complète de pipeline
    pipeline_config: PipelineConfig
    # Spécifie le répertoire contenant les données numpy
    data_dir: Path
    # Spécifie le répertoire racine pour déposer les artefacts
    artifacts_dir: Path
    # Spécifie le répertoire des enregistrements EDF bruts
    raw_dir: Path = DEFAULT_RAW_DIR


# Centralise les ressources partagées entre plusieurs entraînements
@dataclass
class TrainingResources:
    """Agrège les chemins et la configuration pipeline pour un batch."""

    # Transporte la configuration partagée pour toutes les exécutions
    pipeline_config: PipelineConfig
    # Spécifie le répertoire contenant les données numpy
    data_dir: Path
    # Spécifie le répertoire racine pour déposer les artefacts
    artifacts_dir: Path
    # Spécifie le répertoire des enregistrements EDF bruts
    raw_dir: Path = DEFAULT_RAW_DIR


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_orig() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_1() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = None
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_2() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description=None,
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_3() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="XXEntraîne une pipeline TPV et sauvegarde ses artefactsXX",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_4() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="entraîne une pipeline tpv et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_5() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="ENTRAÎNE UNE PIPELINE TPV ET SAUVEGARDE SES ARTEFACTS",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_6() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument(None, help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_7() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help=None)
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_8() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument(help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_9() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", )
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_10() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("XXsubjectXX", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_11() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("SUBJECT", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_12() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="XXIdentifiant du sujet (ex: S001)XX")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_13() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="identifiant du sujet (ex: s001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_14() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="IDENTIFIANT DU SUJET (EX: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_15() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument(None, help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_16() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help=None)
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_17() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument(help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_18() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", )
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_19() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("XXrunXX", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_20() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("RUN", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_21() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="XXIdentifiant du run (ex: R01)XX")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_22() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="identifiant du run (ex: r01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_23() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="IDENTIFIANT DU RUN (EX: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_24() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        None,
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_25() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=None,
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_26() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default=None,
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_27() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help=None,
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_28() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_29() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_30() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_31() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_32() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "XX--classifierXX",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_33() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--CLASSIFIER",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_34() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("XXldaXX", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_35() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("LDA", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_36() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "XXlogisticXX", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_37() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "LOGISTIC", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_38() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "XXsvmXX", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_39() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "SVM", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_40() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "XXcentroidXX"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_41() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "CENTROID"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_42() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="XXldaXX",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_43() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="LDA",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_44() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="XXClassifieur final utilisé pour l'entraînementXX",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_45() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_46() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="CLASSIFIEUR FINAL UTILISÉ POUR L'ENTRAÎNEMENT",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_47() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        None,
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_48() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=None,
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_49() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default=None,
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_50() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help=None,
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_51() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_52() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_53() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_54() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_55() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "XX--scalerXX",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_56() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--SCALER",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_57() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("XXstandardXX", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_58() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("STANDARD", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_59() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "XXrobustXX", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_60() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "ROBUST", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_61() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "XXnoneXX"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_62() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "NONE"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_63() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="XXnoneXX",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_64() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="NONE",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_65() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="XXScaler optionnel appliqué après l'extraction de featuresXX",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_66() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_67() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="SCALER OPTIONNEL APPLIQUÉ APRÈS L'EXTRACTION DE FEATURES",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_68() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        None,
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_69() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=None,
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_70() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default=None,
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_71() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help=None,
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_72() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_73() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_74() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_75() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_76() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "XX--feature-strategyXX",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_77() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--FEATURE-STRATEGY",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_78() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("XXfftXX", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_79() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("FFT", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_80() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "XXwaveletXX"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_81() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "WAVELET"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_82() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="XXfftXX",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_83() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="FFT",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_84() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="XXMéthode d'extraction de features spectralesXX",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_85() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_86() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="MÉTHODE D'EXTRACTION DE FEATURES SPECTRALES",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_87() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        None,
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_88() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=None,
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_89() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default=None,
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_90() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help=None,
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_91() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_92() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_93() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_94() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_95() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "XX--dim-methodXX",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_96() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--DIM-METHOD",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_97() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("XXpcaXX", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_98() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("PCA", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_99() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "XXcspXX"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_100() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "CSP"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_101() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="XXpcaXX",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_102() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="PCA",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_103() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="XXMéthode de réduction de dimension pour la pipelineXX",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_104() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_105() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="MÉTHODE DE RÉDUCTION DE DIMENSION POUR LA PIPELINE",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_106() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        None,
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_107() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=None,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_108() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=None,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_109() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help=None,
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_110() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_111() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_112() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_113() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_114() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "XX--n-componentsXX",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_115() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--N-COMPONENTS",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_116() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="XXNombre de composantes conservées par le réducteurXX",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_117() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_118() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="NOMBRE DE COMPOSANTES CONSERVÉES PAR LE RÉDUCTEUR",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_119() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        None,
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_120() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action=None,
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_121() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help=None,
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_122() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_123() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_124() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_125() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "XX--no-normalize-featuresXX",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_126() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--NO-NORMALIZE-FEATURES",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_127() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="XXstore_trueXX",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_128() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="STORE_TRUE",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_129() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="XXDésactive la normalisation des features extraitesXX",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_130() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_131() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="DÉSACTIVE LA NORMALISATION DES FEATURES EXTRAITES",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_132() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        None,
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_133() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=None,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_134() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=None,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_135() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help=None,
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_136() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_137() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_138() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_139() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_140() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "XX--data-dirXX",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_141() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--DATA-DIR",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_142() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="XXRépertoire racine contenant les fichiers numpyXX",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_143() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_144() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="RÉPERTOIRE RACINE CONTENANT LES FICHIERS NUMPY",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_145() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        None,
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_146() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=None,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_147() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=None,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_148() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help=None,
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_149() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_150() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_151() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_152() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_153() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "XX--artifacts-dirXX",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_154() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--ARTIFACTS-DIR",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_155() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="XXRépertoire racine où enregistrer le modèleXX",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_156() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_157() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="RÉPERTOIRE RACINE OÙ ENREGISTRER LE MODÈLE",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_158() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        None,
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_159() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=None,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_160() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=None,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_161() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help=None,
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_162() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_163() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_164() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_165() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_166() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "XX--raw-dirXX",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_167() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--RAW-DIR",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_168() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="XXRépertoire racine contenant les fichiers EDF brutsXX",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_169() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="répertoire racine contenant les fichiers edf bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_170() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="RÉPERTOIRE RACINE CONTENANT LES FICHIERS EDF BRUTS",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_171() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        None,
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_172() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action=None,
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_173() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help=None,
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_174() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_175() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_176() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_177() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "XX--build-allXX",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_178() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--BUILD-ALL",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_179() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="XXstore_trueXX",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_180() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="STORE_TRUE",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_181() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="XXGénère les fichiers _X.npy/_y.npy pour tous les sujets détectésXX",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_182() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="génère les fichiers _x.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_183() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="GÉNÈRE LES FICHIERS _X.NPY/_Y.NPY POUR TOUS LES SUJETS DÉTECTÉS",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_184() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        None,
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_185() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action=None,
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_186() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help=None,
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_187() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_188() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_189() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_190() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "XX--train-allXX",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_191() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--TRAIN-ALL",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_192() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="XXstore_trueXX",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_193() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="STORE_TRUE",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_194() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="XXEntraîne tous les sujets/runs détectés dans data/XX",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_195() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_196() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="ENTRAÎNE TOUS LES SUJETS/RUNS DÉTECTÉS DANS DATA/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_197() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        None,
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_198() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=None,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_199() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=None,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_200() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help=None,
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_201() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_202() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_203() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_204() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_205() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "XX--sfreqXX",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_206() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--SFREQ",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="Fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_207() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="XXFréquence d'échantillonnage utilisée pour les featuresXX",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_208() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="fréquence d'échantillonnage utilisée pour les features",
    )
    # Retourne le parser configuré
    return parser


# Construit un argument parser aligné sur la CLI mybci
def x_build_parser__mutmut_209() -> argparse.ArgumentParser:
    """Construit le parser CLI pour l'entraînement TPV."""

    # Crée le parser avec description lisible pour l'utilisateur
    parser = argparse.ArgumentParser(
        description="Entraîne une pipeline TPV et sauvegarde ses artefacts",
    )
    # Ajoute l'argument positionnel du sujet pour identifier les fichiers
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'argument positionnel du run pour sélectionner la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute l'option classifieur pour synchroniser avec mybci
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Classifieur final utilisé pour l'entraînement",
    )
    # Ajoute le choix du scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après l'extraction de features",
    )
    # Ajoute la stratégie d'extraction de features pour garder la cohérence
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction de features spectrales",
    )
    # Ajoute la méthode de réduction de dimension pour contrôler la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Méthode de réduction de dimension pour la pipeline",
    )
    # Ajoute le nombre de composantes cible pour la réduction
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes conservées par le réducteur",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features extraites",
    )
    # Ajoute une option pour cibler un répertoire de données spécifique
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DEFAULT_DATA_DIR,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute une option pour configurer le répertoire d'artefacts
    parser.add_argument(
        "--artifacts-dir",
        type=Path,
        default=DEFAULT_ARTIFACTS_DIR,
        help="Répertoire racine où enregistrer le modèle",
    )
    # Ajoute une option pour pointer vers les fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        type=Path,
        default=DEFAULT_RAW_DIR,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Ajoute un mode pour générer tous les .npy sans lancer un fit complet
    parser.add_argument(
        "--build-all",
        action="store_true",
        help="Génère les fichiers _X.npy/_y.npy pour tous les sujets détectés",
    )
    # Ajoute un mode pour entraîner tous les runs moteurs disponibles
    parser.add_argument(
        "--train-all",
        action="store_true",
        help="Entraîne tous les sujets/runs détectés dans data/",
    )
    # Ajoute une option pour spécifier la fréquence d'échantillonnage
    parser.add_argument(
        "--sfreq",
        type=float,
        default=DEFAULT_SAMPLING_RATE,
        help="FRÉQUENCE D'ÉCHANTILLONNAGE UTILISÉE POUR LES FEATURES",
    )
    # Retourne le parser configuré
    return parser

x_build_parser__mutmut_mutants : ClassVar[MutantDict] = {
'x_build_parser__mutmut_1': x_build_parser__mutmut_1, 
    'x_build_parser__mutmut_2': x_build_parser__mutmut_2, 
    'x_build_parser__mutmut_3': x_build_parser__mutmut_3, 
    'x_build_parser__mutmut_4': x_build_parser__mutmut_4, 
    'x_build_parser__mutmut_5': x_build_parser__mutmut_5, 
    'x_build_parser__mutmut_6': x_build_parser__mutmut_6, 
    'x_build_parser__mutmut_7': x_build_parser__mutmut_7, 
    'x_build_parser__mutmut_8': x_build_parser__mutmut_8, 
    'x_build_parser__mutmut_9': x_build_parser__mutmut_9, 
    'x_build_parser__mutmut_10': x_build_parser__mutmut_10, 
    'x_build_parser__mutmut_11': x_build_parser__mutmut_11, 
    'x_build_parser__mutmut_12': x_build_parser__mutmut_12, 
    'x_build_parser__mutmut_13': x_build_parser__mutmut_13, 
    'x_build_parser__mutmut_14': x_build_parser__mutmut_14, 
    'x_build_parser__mutmut_15': x_build_parser__mutmut_15, 
    'x_build_parser__mutmut_16': x_build_parser__mutmut_16, 
    'x_build_parser__mutmut_17': x_build_parser__mutmut_17, 
    'x_build_parser__mutmut_18': x_build_parser__mutmut_18, 
    'x_build_parser__mutmut_19': x_build_parser__mutmut_19, 
    'x_build_parser__mutmut_20': x_build_parser__mutmut_20, 
    'x_build_parser__mutmut_21': x_build_parser__mutmut_21, 
    'x_build_parser__mutmut_22': x_build_parser__mutmut_22, 
    'x_build_parser__mutmut_23': x_build_parser__mutmut_23, 
    'x_build_parser__mutmut_24': x_build_parser__mutmut_24, 
    'x_build_parser__mutmut_25': x_build_parser__mutmut_25, 
    'x_build_parser__mutmut_26': x_build_parser__mutmut_26, 
    'x_build_parser__mutmut_27': x_build_parser__mutmut_27, 
    'x_build_parser__mutmut_28': x_build_parser__mutmut_28, 
    'x_build_parser__mutmut_29': x_build_parser__mutmut_29, 
    'x_build_parser__mutmut_30': x_build_parser__mutmut_30, 
    'x_build_parser__mutmut_31': x_build_parser__mutmut_31, 
    'x_build_parser__mutmut_32': x_build_parser__mutmut_32, 
    'x_build_parser__mutmut_33': x_build_parser__mutmut_33, 
    'x_build_parser__mutmut_34': x_build_parser__mutmut_34, 
    'x_build_parser__mutmut_35': x_build_parser__mutmut_35, 
    'x_build_parser__mutmut_36': x_build_parser__mutmut_36, 
    'x_build_parser__mutmut_37': x_build_parser__mutmut_37, 
    'x_build_parser__mutmut_38': x_build_parser__mutmut_38, 
    'x_build_parser__mutmut_39': x_build_parser__mutmut_39, 
    'x_build_parser__mutmut_40': x_build_parser__mutmut_40, 
    'x_build_parser__mutmut_41': x_build_parser__mutmut_41, 
    'x_build_parser__mutmut_42': x_build_parser__mutmut_42, 
    'x_build_parser__mutmut_43': x_build_parser__mutmut_43, 
    'x_build_parser__mutmut_44': x_build_parser__mutmut_44, 
    'x_build_parser__mutmut_45': x_build_parser__mutmut_45, 
    'x_build_parser__mutmut_46': x_build_parser__mutmut_46, 
    'x_build_parser__mutmut_47': x_build_parser__mutmut_47, 
    'x_build_parser__mutmut_48': x_build_parser__mutmut_48, 
    'x_build_parser__mutmut_49': x_build_parser__mutmut_49, 
    'x_build_parser__mutmut_50': x_build_parser__mutmut_50, 
    'x_build_parser__mutmut_51': x_build_parser__mutmut_51, 
    'x_build_parser__mutmut_52': x_build_parser__mutmut_52, 
    'x_build_parser__mutmut_53': x_build_parser__mutmut_53, 
    'x_build_parser__mutmut_54': x_build_parser__mutmut_54, 
    'x_build_parser__mutmut_55': x_build_parser__mutmut_55, 
    'x_build_parser__mutmut_56': x_build_parser__mutmut_56, 
    'x_build_parser__mutmut_57': x_build_parser__mutmut_57, 
    'x_build_parser__mutmut_58': x_build_parser__mutmut_58, 
    'x_build_parser__mutmut_59': x_build_parser__mutmut_59, 
    'x_build_parser__mutmut_60': x_build_parser__mutmut_60, 
    'x_build_parser__mutmut_61': x_build_parser__mutmut_61, 
    'x_build_parser__mutmut_62': x_build_parser__mutmut_62, 
    'x_build_parser__mutmut_63': x_build_parser__mutmut_63, 
    'x_build_parser__mutmut_64': x_build_parser__mutmut_64, 
    'x_build_parser__mutmut_65': x_build_parser__mutmut_65, 
    'x_build_parser__mutmut_66': x_build_parser__mutmut_66, 
    'x_build_parser__mutmut_67': x_build_parser__mutmut_67, 
    'x_build_parser__mutmut_68': x_build_parser__mutmut_68, 
    'x_build_parser__mutmut_69': x_build_parser__mutmut_69, 
    'x_build_parser__mutmut_70': x_build_parser__mutmut_70, 
    'x_build_parser__mutmut_71': x_build_parser__mutmut_71, 
    'x_build_parser__mutmut_72': x_build_parser__mutmut_72, 
    'x_build_parser__mutmut_73': x_build_parser__mutmut_73, 
    'x_build_parser__mutmut_74': x_build_parser__mutmut_74, 
    'x_build_parser__mutmut_75': x_build_parser__mutmut_75, 
    'x_build_parser__mutmut_76': x_build_parser__mutmut_76, 
    'x_build_parser__mutmut_77': x_build_parser__mutmut_77, 
    'x_build_parser__mutmut_78': x_build_parser__mutmut_78, 
    'x_build_parser__mutmut_79': x_build_parser__mutmut_79, 
    'x_build_parser__mutmut_80': x_build_parser__mutmut_80, 
    'x_build_parser__mutmut_81': x_build_parser__mutmut_81, 
    'x_build_parser__mutmut_82': x_build_parser__mutmut_82, 
    'x_build_parser__mutmut_83': x_build_parser__mutmut_83, 
    'x_build_parser__mutmut_84': x_build_parser__mutmut_84, 
    'x_build_parser__mutmut_85': x_build_parser__mutmut_85, 
    'x_build_parser__mutmut_86': x_build_parser__mutmut_86, 
    'x_build_parser__mutmut_87': x_build_parser__mutmut_87, 
    'x_build_parser__mutmut_88': x_build_parser__mutmut_88, 
    'x_build_parser__mutmut_89': x_build_parser__mutmut_89, 
    'x_build_parser__mutmut_90': x_build_parser__mutmut_90, 
    'x_build_parser__mutmut_91': x_build_parser__mutmut_91, 
    'x_build_parser__mutmut_92': x_build_parser__mutmut_92, 
    'x_build_parser__mutmut_93': x_build_parser__mutmut_93, 
    'x_build_parser__mutmut_94': x_build_parser__mutmut_94, 
    'x_build_parser__mutmut_95': x_build_parser__mutmut_95, 
    'x_build_parser__mutmut_96': x_build_parser__mutmut_96, 
    'x_build_parser__mutmut_97': x_build_parser__mutmut_97, 
    'x_build_parser__mutmut_98': x_build_parser__mutmut_98, 
    'x_build_parser__mutmut_99': x_build_parser__mutmut_99, 
    'x_build_parser__mutmut_100': x_build_parser__mutmut_100, 
    'x_build_parser__mutmut_101': x_build_parser__mutmut_101, 
    'x_build_parser__mutmut_102': x_build_parser__mutmut_102, 
    'x_build_parser__mutmut_103': x_build_parser__mutmut_103, 
    'x_build_parser__mutmut_104': x_build_parser__mutmut_104, 
    'x_build_parser__mutmut_105': x_build_parser__mutmut_105, 
    'x_build_parser__mutmut_106': x_build_parser__mutmut_106, 
    'x_build_parser__mutmut_107': x_build_parser__mutmut_107, 
    'x_build_parser__mutmut_108': x_build_parser__mutmut_108, 
    'x_build_parser__mutmut_109': x_build_parser__mutmut_109, 
    'x_build_parser__mutmut_110': x_build_parser__mutmut_110, 
    'x_build_parser__mutmut_111': x_build_parser__mutmut_111, 
    'x_build_parser__mutmut_112': x_build_parser__mutmut_112, 
    'x_build_parser__mutmut_113': x_build_parser__mutmut_113, 
    'x_build_parser__mutmut_114': x_build_parser__mutmut_114, 
    'x_build_parser__mutmut_115': x_build_parser__mutmut_115, 
    'x_build_parser__mutmut_116': x_build_parser__mutmut_116, 
    'x_build_parser__mutmut_117': x_build_parser__mutmut_117, 
    'x_build_parser__mutmut_118': x_build_parser__mutmut_118, 
    'x_build_parser__mutmut_119': x_build_parser__mutmut_119, 
    'x_build_parser__mutmut_120': x_build_parser__mutmut_120, 
    'x_build_parser__mutmut_121': x_build_parser__mutmut_121, 
    'x_build_parser__mutmut_122': x_build_parser__mutmut_122, 
    'x_build_parser__mutmut_123': x_build_parser__mutmut_123, 
    'x_build_parser__mutmut_124': x_build_parser__mutmut_124, 
    'x_build_parser__mutmut_125': x_build_parser__mutmut_125, 
    'x_build_parser__mutmut_126': x_build_parser__mutmut_126, 
    'x_build_parser__mutmut_127': x_build_parser__mutmut_127, 
    'x_build_parser__mutmut_128': x_build_parser__mutmut_128, 
    'x_build_parser__mutmut_129': x_build_parser__mutmut_129, 
    'x_build_parser__mutmut_130': x_build_parser__mutmut_130, 
    'x_build_parser__mutmut_131': x_build_parser__mutmut_131, 
    'x_build_parser__mutmut_132': x_build_parser__mutmut_132, 
    'x_build_parser__mutmut_133': x_build_parser__mutmut_133, 
    'x_build_parser__mutmut_134': x_build_parser__mutmut_134, 
    'x_build_parser__mutmut_135': x_build_parser__mutmut_135, 
    'x_build_parser__mutmut_136': x_build_parser__mutmut_136, 
    'x_build_parser__mutmut_137': x_build_parser__mutmut_137, 
    'x_build_parser__mutmut_138': x_build_parser__mutmut_138, 
    'x_build_parser__mutmut_139': x_build_parser__mutmut_139, 
    'x_build_parser__mutmut_140': x_build_parser__mutmut_140, 
    'x_build_parser__mutmut_141': x_build_parser__mutmut_141, 
    'x_build_parser__mutmut_142': x_build_parser__mutmut_142, 
    'x_build_parser__mutmut_143': x_build_parser__mutmut_143, 
    'x_build_parser__mutmut_144': x_build_parser__mutmut_144, 
    'x_build_parser__mutmut_145': x_build_parser__mutmut_145, 
    'x_build_parser__mutmut_146': x_build_parser__mutmut_146, 
    'x_build_parser__mutmut_147': x_build_parser__mutmut_147, 
    'x_build_parser__mutmut_148': x_build_parser__mutmut_148, 
    'x_build_parser__mutmut_149': x_build_parser__mutmut_149, 
    'x_build_parser__mutmut_150': x_build_parser__mutmut_150, 
    'x_build_parser__mutmut_151': x_build_parser__mutmut_151, 
    'x_build_parser__mutmut_152': x_build_parser__mutmut_152, 
    'x_build_parser__mutmut_153': x_build_parser__mutmut_153, 
    'x_build_parser__mutmut_154': x_build_parser__mutmut_154, 
    'x_build_parser__mutmut_155': x_build_parser__mutmut_155, 
    'x_build_parser__mutmut_156': x_build_parser__mutmut_156, 
    'x_build_parser__mutmut_157': x_build_parser__mutmut_157, 
    'x_build_parser__mutmut_158': x_build_parser__mutmut_158, 
    'x_build_parser__mutmut_159': x_build_parser__mutmut_159, 
    'x_build_parser__mutmut_160': x_build_parser__mutmut_160, 
    'x_build_parser__mutmut_161': x_build_parser__mutmut_161, 
    'x_build_parser__mutmut_162': x_build_parser__mutmut_162, 
    'x_build_parser__mutmut_163': x_build_parser__mutmut_163, 
    'x_build_parser__mutmut_164': x_build_parser__mutmut_164, 
    'x_build_parser__mutmut_165': x_build_parser__mutmut_165, 
    'x_build_parser__mutmut_166': x_build_parser__mutmut_166, 
    'x_build_parser__mutmut_167': x_build_parser__mutmut_167, 
    'x_build_parser__mutmut_168': x_build_parser__mutmut_168, 
    'x_build_parser__mutmut_169': x_build_parser__mutmut_169, 
    'x_build_parser__mutmut_170': x_build_parser__mutmut_170, 
    'x_build_parser__mutmut_171': x_build_parser__mutmut_171, 
    'x_build_parser__mutmut_172': x_build_parser__mutmut_172, 
    'x_build_parser__mutmut_173': x_build_parser__mutmut_173, 
    'x_build_parser__mutmut_174': x_build_parser__mutmut_174, 
    'x_build_parser__mutmut_175': x_build_parser__mutmut_175, 
    'x_build_parser__mutmut_176': x_build_parser__mutmut_176, 
    'x_build_parser__mutmut_177': x_build_parser__mutmut_177, 
    'x_build_parser__mutmut_178': x_build_parser__mutmut_178, 
    'x_build_parser__mutmut_179': x_build_parser__mutmut_179, 
    'x_build_parser__mutmut_180': x_build_parser__mutmut_180, 
    'x_build_parser__mutmut_181': x_build_parser__mutmut_181, 
    'x_build_parser__mutmut_182': x_build_parser__mutmut_182, 
    'x_build_parser__mutmut_183': x_build_parser__mutmut_183, 
    'x_build_parser__mutmut_184': x_build_parser__mutmut_184, 
    'x_build_parser__mutmut_185': x_build_parser__mutmut_185, 
    'x_build_parser__mutmut_186': x_build_parser__mutmut_186, 
    'x_build_parser__mutmut_187': x_build_parser__mutmut_187, 
    'x_build_parser__mutmut_188': x_build_parser__mutmut_188, 
    'x_build_parser__mutmut_189': x_build_parser__mutmut_189, 
    'x_build_parser__mutmut_190': x_build_parser__mutmut_190, 
    'x_build_parser__mutmut_191': x_build_parser__mutmut_191, 
    'x_build_parser__mutmut_192': x_build_parser__mutmut_192, 
    'x_build_parser__mutmut_193': x_build_parser__mutmut_193, 
    'x_build_parser__mutmut_194': x_build_parser__mutmut_194, 
    'x_build_parser__mutmut_195': x_build_parser__mutmut_195, 
    'x_build_parser__mutmut_196': x_build_parser__mutmut_196, 
    'x_build_parser__mutmut_197': x_build_parser__mutmut_197, 
    'x_build_parser__mutmut_198': x_build_parser__mutmut_198, 
    'x_build_parser__mutmut_199': x_build_parser__mutmut_199, 
    'x_build_parser__mutmut_200': x_build_parser__mutmut_200, 
    'x_build_parser__mutmut_201': x_build_parser__mutmut_201, 
    'x_build_parser__mutmut_202': x_build_parser__mutmut_202, 
    'x_build_parser__mutmut_203': x_build_parser__mutmut_203, 
    'x_build_parser__mutmut_204': x_build_parser__mutmut_204, 
    'x_build_parser__mutmut_205': x_build_parser__mutmut_205, 
    'x_build_parser__mutmut_206': x_build_parser__mutmut_206, 
    'x_build_parser__mutmut_207': x_build_parser__mutmut_207, 
    'x_build_parser__mutmut_208': x_build_parser__mutmut_208, 
    'x_build_parser__mutmut_209': x_build_parser__mutmut_209
}

def build_parser(*args, **kwargs):
    result = _mutmut_trampoline(x_build_parser__mutmut_orig, x_build_parser__mutmut_mutants, args, kwargs)
    return result 

build_parser.__signature__ = _mutmut_signature(x_build_parser__mutmut_orig)
x_build_parser__mutmut_orig.__name__ = 'x_build_parser'


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_orig(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_1(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = None
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_2(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir * subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_3(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = None
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_4(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir * f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir / f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_5(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = None
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path


# Construit les chemins des données pour un sujet et un run donnés
def x__resolve_data_paths__mutmut_6(subject: str, run: str, data_dir: Path) -> tuple[Path, Path]:
    """Retourne les chemins des matrices X et y pour un sujet/run."""

    # Localise le sous-dossier spécifique au sujet
    base_dir = data_dir / subject
    # Compose le chemin du fichier de données numpy
    features_path = base_dir / f"{run}_X.npy"
    # Compose le chemin du fichier d'étiquettes numpy
    labels_path = base_dir * f"{run}_y.npy"
    # Retourne les deux chemins pour chargement ultérieur
    return features_path, labels_path

x__resolve_data_paths__mutmut_mutants : ClassVar[MutantDict] = {
'x__resolve_data_paths__mutmut_1': x__resolve_data_paths__mutmut_1, 
    'x__resolve_data_paths__mutmut_2': x__resolve_data_paths__mutmut_2, 
    'x__resolve_data_paths__mutmut_3': x__resolve_data_paths__mutmut_3, 
    'x__resolve_data_paths__mutmut_4': x__resolve_data_paths__mutmut_4, 
    'x__resolve_data_paths__mutmut_5': x__resolve_data_paths__mutmut_5, 
    'x__resolve_data_paths__mutmut_6': x__resolve_data_paths__mutmut_6
}

def _resolve_data_paths(*args, **kwargs):
    result = _mutmut_trampoline(x__resolve_data_paths__mutmut_orig, x__resolve_data_paths__mutmut_mutants, args, kwargs)
    return result 

_resolve_data_paths.__signature__ = _mutmut_signature(x__resolve_data_paths__mutmut_orig)
x__resolve_data_paths__mutmut_orig.__name__ = 'x__resolve_data_paths'


# Construit des matrices numpy à partir d'un EDF lorsqu'elles manquent
def _build_npy_from_edf(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[Path, Path]:
    """Génère X (epochs brutes) et y depuis un fichier EDF Physionet.

    - X est sauvegardé sous forme (n_trials, n_channels, n_times)
      pour être compatible avec la pipeline (tpv.features).
    - Les features fréquentielles sont ensuite calculées *dans* la
      pipeline, pas au moment de la génération des .npy.
    """

    # Calcule les chemins cibles pour les fichiers numpy
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)
    # Calcule le chemin attendu du fichier EDF brut
    raw_path = raw_dir / subject / f"{subject}{run}.edf"

    # Interrompt tôt si l'EDF est absent
    if not raw_path.exists():
        raise FileNotFoundError(
            "EDF introuvable pour "
            f"{subject} {run}: {raw_path}. "
            "Téléchargez les enregistrements Physionet dans data ou "
            "pointez --raw-dir vers un dossier déjà synchronisé."
        )

    # Crée l'arborescence cible pour déposer les .npy
    features_path.parent.mkdir(parents=True, exist_ok=True)

    # Charge l'EDF en conservant les métadonnées essentielles
    raw, _ = preprocessing.load_physionet_raw(raw_path)

    # Mappe les annotations en événements moteurs
    events, event_id, motor_labels = preprocessing.map_events_to_motor_labels(raw)

    # Découpe le signal en epochs exploitables
    epochs = preprocessing.create_epochs_from_raw(raw, events, event_id)

    # Récupère les données brutes des epochs (n_trials, n_channels, n_times)
    epochs_data = epochs.get_data(copy=True)

    # Définit un mapping stable label → entier
    label_mapping = {label: idx for idx, label in enumerate(sorted(set(motor_labels)))}

    # Convertit les labels symboliques en entiers
    numeric_labels = np.array([label_mapping[label] for label in motor_labels])

    # Persiste les epochs brutes
    np.save(features_path, epochs_data)
    # Persiste les labels alignés
    np.save(labels_path, numeric_labels)

    # Retourne les chemins nouvellement générés
    return features_path, labels_path


# Construit les .npy pour l'ensemble des sujets disponibles
def _build_all_npy(raw_dir: Path, data_dir: Path) -> None:
    """Génère les fichiers numpy pour chaque run moteur disponible."""

    # Parcourt les dossiers de sujets triés pour des logs prédictibles
    subject_dirs = sorted(path for path in raw_dir.iterdir() if path.is_dir())

    # Explore chaque sujet détecté dans le répertoire brut
    for subject_dir in subject_dirs:
        # Extrait l'identifiant du sujet à partir du nom de dossier
        subject = subject_dir.name
        # Liste tous les enregistrements EDF associés au sujet courant
        edf_paths = sorted(subject_dir.glob(f"{subject}R*.edf"))

        # Traite chaque enregistrement pour générer les .npy associés
        for edf_path in edf_paths:
            # Déduit le run en retirant le préfixe sujet du nom de fichier
            run = edf_path.stem.replace(subject, "")

            # Ignore explicitement les runs dépourvus d'événements moteurs
            try:
                _build_npy_from_edf(subject, run, data_dir, raw_dir)
            except ValueError as error:
                if "No motor events present" in str(error):
                    print(
                        "INFO: Événements moteurs absents pour "
                        f"{subject} {run}, passage."
                    )
                    continue
                raise


# Liste les sujets disponibles dans le répertoire brut
def _list_subjects(raw_dir: Path) -> list[str]:
    """Retourne les identifiants de sujets triés présents dans raw_dir."""

    # Construit la liste des dossiers de sujets pour préparer l'entraînement
    subjects = [entry.name for entry in raw_dir.iterdir() if entry.is_dir()]
    # Trie les identifiants pour obtenir des logs stables et reproductibles
    subjects.sort()
    # Retourne la liste triée pour l'appelant
    return subjects


# Entraîne un couple sujet/run en réutilisant la configuration partagée
def _train_single_run(
    subject: str,
    run: str,
    resources: TrainingResources,
) -> bool:
    """Lance l'entraînement d'un sujet pour un run donné."""

    # Prépare la requête complète pour exécuter run_training
    request = TrainingRequest(
        subject=subject,
        run=run,
        pipeline_config=resources.pipeline_config,
        data_dir=resources.data_dir,
        artifacts_dir=resources.artifacts_dir,
        raw_dir=resources.raw_dir,
    )
    # Protège l'appel pour signaler les données manquantes sans stopper la boucle
    try:
        # Entraîne la pipeline et persiste les artefacts nécessaires
        _ = run_training(request)
    except FileNotFoundError as error:
        # Alerte l'utilisateur lorsqu'un EDF ou des événements sont absents
        print(f"AVERTISSEMENT: {error}")
        # Indique l'échec pour déclencher un récapitulatif final
        return False
    # Retourne True pour signaler un entraînement réussi
    return True


# Entraîne tous les runs moteurs pour chaque sujet détecté
def _train_all_runs(
    config: PipelineConfig,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path,
) -> int:
    """Parcourt les sujets et runs moteurs pour générer tous les modèles."""

    # Récupère la liste des sujets disponibles dans le répertoire brut
    subjects = _list_subjects(raw_dir)
    # Centralise les ressources immuables pour éviter des répétitions
    resources = TrainingResources(
        pipeline_config=config,
        data_dir=data_dir,
        artifacts_dir=artifacts_dir,
        raw_dir=raw_dir,
    )
    # Prépare un compteur d'échecs pour informer l'utilisateur à la fin
    failures = 0
    # Parcourt chaque sujet détecté
    for subject in subjects:
        # Parcourt chaque run moteur attendu
        for run in MOTOR_RUNS:
            # Calcule le chemin EDF attendu pour vérifier l'existence
            raw_path = raw_dir / subject / f"{subject}{run}.edf"
            # Ignore le couple lorsque l'EDF est absent du disque
            if not raw_path.exists():
                # Informe l'utilisateur de l'absence pour transparence
                print(
                    "INFO: EDF introuvable pour "
                    f"{subject} {run} dans {raw_path.parent}, passage."
                )
                # Passe au run suivant sans incrémenter les échecs
                continue
            # Entraîne le run courant et capture le statut
            success = _train_single_run(
                subject,
                run,
                resources,
            )
            # Incrémente le compteur d'échecs lorsque l'entraînement échoue
            if not success:
                failures += 1
    # Affiche un résumé pour guider l'utilisateur après la boucle
    if failures:
        # Mentionne le nombre total d'entraînements manquants
        print(
            "AVERTISSEMENT: certains entraînements ont échoué. "
            f"Exécutions manquantes: {failures}."
        )
    else:
        # Confirme que tous les artefacts ont été générés avec succès
        print("INFO: modèles entraînés pour tous les runs moteurs détectés.")
    # Retourne 1 si des échecs sont survenus pour refléter l'état global
    return 1 if failures else 0


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_orig(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_1(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim == EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_2(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            None
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_3(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "XXINFO: X chargé depuis XX"
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_4(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "info: x chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_5(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X CHARGÉ DEPUIS "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_6(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return False

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_7(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[1] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_8(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] == candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_9(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[1]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_10(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            None
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_11(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "XXINFO: Désalignement détecté pour XX"
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_12(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "info: désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_13(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: DÉSALIGNEMENT DÉTECTÉ POUR "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_14(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[1]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_15(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[1]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_16(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return False

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_17(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim == 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_18(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 2:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return False


# Vérifie si les caches existants respectent les shapes attendues
def x__needs_rebuild_from_shapes__mutmut_19(
    candidate_X: np.ndarray,
    candidate_y: np.ndarray,
    features_path: Path,
    labels_path: Path,
    run_label: str,
) -> bool:
    """Valide les dimensions des caches existants pour éviter des erreurs."""

    # Cas 2 : X n'a pas la bonne dimension → reconstruction
    if candidate_X.ndim != EXPECTED_FEATURES_DIMENSIONS:
        # Informe l'utilisateur de la dimension inattendue
        print(
            "INFO: X chargé depuis "
            f"'{features_path}' a ndim={candidate_X.ndim} au lieu de "
            f"{EXPECTED_FEATURES_DIMENSIONS}, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver la forme attendue
        return True

    # Cas 3 : désalignement entre n_samples de X et y → reconstruction
    if candidate_X.shape[0] != candidate_y.shape[0]:
        # Signale l'incohérence de longueur entre X et y
        print(
            "INFO: Désalignement détecté pour "
            f"{run_label}: X.shape[0]={candidate_X.shape[0]}, "
            f"y.shape[0]={candidate_y.shape[0]}. Régénération depuis l'EDF..."
        )
        # Demande une reconstruction pour réaligner les labels
        return True

    # Cas 4 : labels mal dimensionnés → reconstruction
    if candidate_y.ndim != 1:
        # Informe l'utilisateur que y possède une dimension inattendue
        print(
            "INFO: y chargé depuis "
            f"'{labels_path}' a ndim={candidate_y.ndim} au lieu de 1, "
            "régénération depuis l'EDF..."
        )
        # Demande une régénération pour retrouver un vecteur 1D
        return True

    # Confirme que les caches existants sont utilisables
    return True

x__needs_rebuild_from_shapes__mutmut_mutants : ClassVar[MutantDict] = {
'x__needs_rebuild_from_shapes__mutmut_1': x__needs_rebuild_from_shapes__mutmut_1, 
    'x__needs_rebuild_from_shapes__mutmut_2': x__needs_rebuild_from_shapes__mutmut_2, 
    'x__needs_rebuild_from_shapes__mutmut_3': x__needs_rebuild_from_shapes__mutmut_3, 
    'x__needs_rebuild_from_shapes__mutmut_4': x__needs_rebuild_from_shapes__mutmut_4, 
    'x__needs_rebuild_from_shapes__mutmut_5': x__needs_rebuild_from_shapes__mutmut_5, 
    'x__needs_rebuild_from_shapes__mutmut_6': x__needs_rebuild_from_shapes__mutmut_6, 
    'x__needs_rebuild_from_shapes__mutmut_7': x__needs_rebuild_from_shapes__mutmut_7, 
    'x__needs_rebuild_from_shapes__mutmut_8': x__needs_rebuild_from_shapes__mutmut_8, 
    'x__needs_rebuild_from_shapes__mutmut_9': x__needs_rebuild_from_shapes__mutmut_9, 
    'x__needs_rebuild_from_shapes__mutmut_10': x__needs_rebuild_from_shapes__mutmut_10, 
    'x__needs_rebuild_from_shapes__mutmut_11': x__needs_rebuild_from_shapes__mutmut_11, 
    'x__needs_rebuild_from_shapes__mutmut_12': x__needs_rebuild_from_shapes__mutmut_12, 
    'x__needs_rebuild_from_shapes__mutmut_13': x__needs_rebuild_from_shapes__mutmut_13, 
    'x__needs_rebuild_from_shapes__mutmut_14': x__needs_rebuild_from_shapes__mutmut_14, 
    'x__needs_rebuild_from_shapes__mutmut_15': x__needs_rebuild_from_shapes__mutmut_15, 
    'x__needs_rebuild_from_shapes__mutmut_16': x__needs_rebuild_from_shapes__mutmut_16, 
    'x__needs_rebuild_from_shapes__mutmut_17': x__needs_rebuild_from_shapes__mutmut_17, 
    'x__needs_rebuild_from_shapes__mutmut_18': x__needs_rebuild_from_shapes__mutmut_18, 
    'x__needs_rebuild_from_shapes__mutmut_19': x__needs_rebuild_from_shapes__mutmut_19
}

def _needs_rebuild_from_shapes(*args, **kwargs):
    result = _mutmut_trampoline(x__needs_rebuild_from_shapes__mutmut_orig, x__needs_rebuild_from_shapes__mutmut_mutants, args, kwargs)
    return result 

_needs_rebuild_from_shapes.__signature__ = _mutmut_signature(x__needs_rebuild_from_shapes__mutmut_orig)
x__needs_rebuild_from_shapes__mutmut_orig.__name__ = 'x__needs_rebuild_from_shapes'


def x__should_check_shapes__mutmut_orig(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild
        and corrupted_reason is None
        and candidate_X is not None
        and candidate_y is not None
    )


def x__should_check_shapes__mutmut_1(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild
        and corrupted_reason is None
        and candidate_X is not None or candidate_y is not None
    )


def x__should_check_shapes__mutmut_2(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild
        and corrupted_reason is None or candidate_X is not None
        and candidate_y is not None
    )


def x__should_check_shapes__mutmut_3(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild or corrupted_reason is None
        and candidate_X is not None
        and candidate_y is not None
    )


def x__should_check_shapes__mutmut_4(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        needs_rebuild
        and corrupted_reason is None
        and candidate_X is not None
        and candidate_y is not None
    )


def x__should_check_shapes__mutmut_5(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild
        and corrupted_reason is not None
        and candidate_X is not None
        and candidate_y is not None
    )


def x__should_check_shapes__mutmut_6(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild
        and corrupted_reason is None
        and candidate_X is None
        and candidate_y is not None
    )


def x__should_check_shapes__mutmut_7(
    needs_rebuild: bool,
    corrupted_reason: str | None,
    candidate_X: np.ndarray | None,
    candidate_y: np.ndarray | None,
) -> bool:
    """Détermine si la validation des shapes est nécessaire."""

    return (
        not needs_rebuild
        and corrupted_reason is None
        and candidate_X is not None
        and candidate_y is None
    )

x__should_check_shapes__mutmut_mutants : ClassVar[MutantDict] = {
'x__should_check_shapes__mutmut_1': x__should_check_shapes__mutmut_1, 
    'x__should_check_shapes__mutmut_2': x__should_check_shapes__mutmut_2, 
    'x__should_check_shapes__mutmut_3': x__should_check_shapes__mutmut_3, 
    'x__should_check_shapes__mutmut_4': x__should_check_shapes__mutmut_4, 
    'x__should_check_shapes__mutmut_5': x__should_check_shapes__mutmut_5, 
    'x__should_check_shapes__mutmut_6': x__should_check_shapes__mutmut_6, 
    'x__should_check_shapes__mutmut_7': x__should_check_shapes__mutmut_7
}

def _should_check_shapes(*args, **kwargs):
    result = _mutmut_trampoline(x__should_check_shapes__mutmut_orig, x__should_check_shapes__mutmut_mutants, args, kwargs)
    return result 

_should_check_shapes.__signature__ = _mutmut_signature(x__should_check_shapes__mutmut_orig)
x__should_check_shapes__mutmut_orig.__name__ = 'x__should_check_shapes'


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_orig(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_1(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = None

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_2(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = None

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_3(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(None, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_4(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, None, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_5(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, None)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_6(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_7(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_8(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, )

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_9(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = None
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_10(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = True
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_11(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = ""
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_12(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = ""
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_13(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = ""

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_14(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() and not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_15(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_16(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_17(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = None
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_18(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = False
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_19(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = None
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_20(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(None, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_21(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode=None)
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_22(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_23(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, )
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_24(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="XXrXX")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_25(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="R")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_26(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = None
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_27(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(None, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_28(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode=None)
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_29(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_30(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, )
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_31(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="XXrXX")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_32(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="R")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_33(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = None
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_34(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = False
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_35(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = None

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_36(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(None)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_37(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(None, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_38(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, None, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_39(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, None, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_40(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, None):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_41(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_42(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_43(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_44(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, ):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_45(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = None
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_46(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(None, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_47(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, None)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_48(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_49(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, )
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_50(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = None
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_51(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(None, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_52(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, None)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_53(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_54(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, )
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_55(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = None

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_56(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            None
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_57(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                None,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_58(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                None,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_59(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                None,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_60(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                None,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_61(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                None,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_62(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_63(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_64(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_65(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_66(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_67(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_68(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            None
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_69(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "XXINFO: Chargement numpy impossible pour XX"
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_70(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "info: chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_71(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: CHARGEMENT NUMPY IMPOSSIBLE POUR "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_72(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = None

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_73(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = False

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_74(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = None

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_75(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = False if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_76(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else True

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_77(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = None

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_78(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            None,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_79(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            None,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_80(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            None,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_81(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            None,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_82(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_83(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_84(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_85(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_86(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = None
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_87(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(None)
    y = np.load(labels_path)

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_88(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = None

    return X, y


# Charge ou génère les matrices numpy attendues pour l'entraînement
def x__load_data__mutmut_89(
    subject: str,
    run: str,
    data_dir: Path,
    raw_dir: Path,
) -> tuple[np.ndarray, np.ndarray]:
    """Charge ou construit les données et étiquettes pour un run.

    - Si les .npy n'existent pas, on les génère depuis l'EDF.
    - Si X existe mais n'est pas 3D, on reconstruit depuis l'EDF.
    - Si X et y n'ont pas le même nombre d'échantillons, on
      reconstruit pour réaligner les labels sur les epochs.
    """

    # Concatène le couple sujet/run pour les messages utilisateur
    run_label = f"{subject} {run}"

    # Détermine les chemins attendus pour les features et labels
    features_path, labels_path = _resolve_data_paths(subject, run, data_dir)

    # Garde un bool strict (évite None, tue le mutant False->None).
    needs_rebuild: bool = False
    # Stocke les chemins invalides pour enrichir les logs utilisateurs
    corrupted_reason: str | None = None
    # Conserve les caches chargés pour valider leurs formes
    candidate_X: np.ndarray | None = None
    # Conserve les labels chargés pour valider la longueur
    candidate_y: np.ndarray | None = None

    # Cas 1 : fichiers manquants → on reconstruira
    if not features_path.exists() or not labels_path.exists():
        needs_rebuild = True
    else:
        # Sécurise le chargement numpy pour tolérer les fichiers corrompus
        try:
            # Charge X en mmap pour inspecter la forme sans tout charger
            candidate_X = np.load(features_path, mmap_mode="r")
            # Charge y en mmap pour inspecter la longueur
            candidate_y = np.load(labels_path, mmap_mode="r")
        except (OSError, ValueError) as error:
            # Demande la reconstruction dès qu'un chargement échoue
            needs_rebuild = True
            # Conserve la raison pour orienter l'utilisateur
            corrupted_reason = str(error)

    # Valide les shapes lorsque les caches ont été chargés avec succès
    if _should_check_shapes(needs_rebuild, corrupted_reason, candidate_X, candidate_y):
        # Convertit X vers un tableau typé pour satisfaire mypy et bandit
        validated_X = cast(np.ndarray, candidate_X)
        # Convertit y vers un vecteur typé pour satisfaire mypy et bandit
        validated_y = cast(np.ndarray, candidate_y)
        # Détecte les incohérences de dimension et déclenche une régénération
        needs_rebuild = bool(
            _needs_rebuild_from_shapes(
                validated_X,
                validated_y,
                features_path,
                labels_path,
                run_label,
            )
        )

    # Informe l'utilisateur lorsqu'un fichier corrompu bloque le chargement
    if corrupted_reason is not None:
        print(
            "INFO: Chargement numpy impossible pour "
            f"{subject} {run}: {corrupted_reason}. "
            "Régénération depuis l'EDF..."
        )
        needs_rebuild = True

    # Force un bool Python strict (évite None / numpy.bool_).
    needs_rebuild = True if needs_rebuild else False

    # Reconstruit les fichiers lorsque nécessaire
    if needs_rebuild:
        features_path, labels_path = _build_npy_from_edf(
            subject,
            run,
            data_dir,
            raw_dir,
        )

    # Charge les données validées (3D) et labels réalignés
    X = np.load(features_path)
    y = np.load(None)

    return X, y

x__load_data__mutmut_mutants : ClassVar[MutantDict] = {
'x__load_data__mutmut_1': x__load_data__mutmut_1, 
    'x__load_data__mutmut_2': x__load_data__mutmut_2, 
    'x__load_data__mutmut_3': x__load_data__mutmut_3, 
    'x__load_data__mutmut_4': x__load_data__mutmut_4, 
    'x__load_data__mutmut_5': x__load_data__mutmut_5, 
    'x__load_data__mutmut_6': x__load_data__mutmut_6, 
    'x__load_data__mutmut_7': x__load_data__mutmut_7, 
    'x__load_data__mutmut_8': x__load_data__mutmut_8, 
    'x__load_data__mutmut_9': x__load_data__mutmut_9, 
    'x__load_data__mutmut_10': x__load_data__mutmut_10, 
    'x__load_data__mutmut_11': x__load_data__mutmut_11, 
    'x__load_data__mutmut_12': x__load_data__mutmut_12, 
    'x__load_data__mutmut_13': x__load_data__mutmut_13, 
    'x__load_data__mutmut_14': x__load_data__mutmut_14, 
    'x__load_data__mutmut_15': x__load_data__mutmut_15, 
    'x__load_data__mutmut_16': x__load_data__mutmut_16, 
    'x__load_data__mutmut_17': x__load_data__mutmut_17, 
    'x__load_data__mutmut_18': x__load_data__mutmut_18, 
    'x__load_data__mutmut_19': x__load_data__mutmut_19, 
    'x__load_data__mutmut_20': x__load_data__mutmut_20, 
    'x__load_data__mutmut_21': x__load_data__mutmut_21, 
    'x__load_data__mutmut_22': x__load_data__mutmut_22, 
    'x__load_data__mutmut_23': x__load_data__mutmut_23, 
    'x__load_data__mutmut_24': x__load_data__mutmut_24, 
    'x__load_data__mutmut_25': x__load_data__mutmut_25, 
    'x__load_data__mutmut_26': x__load_data__mutmut_26, 
    'x__load_data__mutmut_27': x__load_data__mutmut_27, 
    'x__load_data__mutmut_28': x__load_data__mutmut_28, 
    'x__load_data__mutmut_29': x__load_data__mutmut_29, 
    'x__load_data__mutmut_30': x__load_data__mutmut_30, 
    'x__load_data__mutmut_31': x__load_data__mutmut_31, 
    'x__load_data__mutmut_32': x__load_data__mutmut_32, 
    'x__load_data__mutmut_33': x__load_data__mutmut_33, 
    'x__load_data__mutmut_34': x__load_data__mutmut_34, 
    'x__load_data__mutmut_35': x__load_data__mutmut_35, 
    'x__load_data__mutmut_36': x__load_data__mutmut_36, 
    'x__load_data__mutmut_37': x__load_data__mutmut_37, 
    'x__load_data__mutmut_38': x__load_data__mutmut_38, 
    'x__load_data__mutmut_39': x__load_data__mutmut_39, 
    'x__load_data__mutmut_40': x__load_data__mutmut_40, 
    'x__load_data__mutmut_41': x__load_data__mutmut_41, 
    'x__load_data__mutmut_42': x__load_data__mutmut_42, 
    'x__load_data__mutmut_43': x__load_data__mutmut_43, 
    'x__load_data__mutmut_44': x__load_data__mutmut_44, 
    'x__load_data__mutmut_45': x__load_data__mutmut_45, 
    'x__load_data__mutmut_46': x__load_data__mutmut_46, 
    'x__load_data__mutmut_47': x__load_data__mutmut_47, 
    'x__load_data__mutmut_48': x__load_data__mutmut_48, 
    'x__load_data__mutmut_49': x__load_data__mutmut_49, 
    'x__load_data__mutmut_50': x__load_data__mutmut_50, 
    'x__load_data__mutmut_51': x__load_data__mutmut_51, 
    'x__load_data__mutmut_52': x__load_data__mutmut_52, 
    'x__load_data__mutmut_53': x__load_data__mutmut_53, 
    'x__load_data__mutmut_54': x__load_data__mutmut_54, 
    'x__load_data__mutmut_55': x__load_data__mutmut_55, 
    'x__load_data__mutmut_56': x__load_data__mutmut_56, 
    'x__load_data__mutmut_57': x__load_data__mutmut_57, 
    'x__load_data__mutmut_58': x__load_data__mutmut_58, 
    'x__load_data__mutmut_59': x__load_data__mutmut_59, 
    'x__load_data__mutmut_60': x__load_data__mutmut_60, 
    'x__load_data__mutmut_61': x__load_data__mutmut_61, 
    'x__load_data__mutmut_62': x__load_data__mutmut_62, 
    'x__load_data__mutmut_63': x__load_data__mutmut_63, 
    'x__load_data__mutmut_64': x__load_data__mutmut_64, 
    'x__load_data__mutmut_65': x__load_data__mutmut_65, 
    'x__load_data__mutmut_66': x__load_data__mutmut_66, 
    'x__load_data__mutmut_67': x__load_data__mutmut_67, 
    'x__load_data__mutmut_68': x__load_data__mutmut_68, 
    'x__load_data__mutmut_69': x__load_data__mutmut_69, 
    'x__load_data__mutmut_70': x__load_data__mutmut_70, 
    'x__load_data__mutmut_71': x__load_data__mutmut_71, 
    'x__load_data__mutmut_72': x__load_data__mutmut_72, 
    'x__load_data__mutmut_73': x__load_data__mutmut_73, 
    'x__load_data__mutmut_74': x__load_data__mutmut_74, 
    'x__load_data__mutmut_75': x__load_data__mutmut_75, 
    'x__load_data__mutmut_76': x__load_data__mutmut_76, 
    'x__load_data__mutmut_77': x__load_data__mutmut_77, 
    'x__load_data__mutmut_78': x__load_data__mutmut_78, 
    'x__load_data__mutmut_79': x__load_data__mutmut_79, 
    'x__load_data__mutmut_80': x__load_data__mutmut_80, 
    'x__load_data__mutmut_81': x__load_data__mutmut_81, 
    'x__load_data__mutmut_82': x__load_data__mutmut_82, 
    'x__load_data__mutmut_83': x__load_data__mutmut_83, 
    'x__load_data__mutmut_84': x__load_data__mutmut_84, 
    'x__load_data__mutmut_85': x__load_data__mutmut_85, 
    'x__load_data__mutmut_86': x__load_data__mutmut_86, 
    'x__load_data__mutmut_87': x__load_data__mutmut_87, 
    'x__load_data__mutmut_88': x__load_data__mutmut_88, 
    'x__load_data__mutmut_89': x__load_data__mutmut_89
}

def _load_data(*args, **kwargs):
    result = _mutmut_trampoline(x__load_data__mutmut_orig, x__load_data__mutmut_mutants, args, kwargs)
    return result 

_load_data.__signature__ = _mutmut_signature(x__load_data__mutmut_orig)
x__load_data__mutmut_orig.__name__ = 'x__load_data'


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_orig() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_1() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = None
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_2() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") * "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_3() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(None) / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_4() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path("XX.gitXX") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_5() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".GIT") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_6() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "XXHEADXX"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_7() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "head"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_8() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_9() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "XXunknownXX"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_10() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "UNKNOWN"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_11() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = None
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_12() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith(None):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_13() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("XXref:XX"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_14() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("REF:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_15() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = None
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_16() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") * head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_17() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(None) / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_18() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path("XX.gitXX") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_19() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".GIT") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_20() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(None, 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_21() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", None)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_22() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_23() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", )[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_24() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.rsplit(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_25() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split("XX XX", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_26() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 2)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_27() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[2]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_28() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_29() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "XXunknownXX"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_30() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "UNKNOWN"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_31() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content and "unknown"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_32() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "XXunknownXX"


# Récupère le hash git courant pour tracer la reproductibilité
def x__get_git_commit__mutmut_33() -> str:
    """Retourne le hash du commit courant ou "unknown" en secours."""

    # Localise le fichier HEAD pour extraire la référence courante
    head_path = Path(".git") / "HEAD"
    # Retourne unknown lorsque le dépôt git n'est pas disponible
    if not head_path.exists():
        # Fournit une valeur de repli pour conserver un manifeste valide
        return "unknown"
    # Lit le contenu du HEAD pour déterminer la référence active
    head_content = head_path.read_text().strip()
    # Détecte les références symboliques du style "ref: ..."
    if head_content.startswith("ref:"):
        # Isole le chemin relatif vers le fichier de référence
        ref_path = Path(".git") / head_content.split(" ", 1)[1]
        # Retourne unknown si la référence est introuvable
        if not ref_path.exists():
            # Fournit une valeur de repli pour préserver la validation
            return "unknown"
        # Lit le hash contenu dans le fichier de référence
        return ref_path.read_text().strip()
    # Retourne le contenu brut lorsque HEAD contient déjà un hash
    return head_content or "UNKNOWN"

x__get_git_commit__mutmut_mutants : ClassVar[MutantDict] = {
'x__get_git_commit__mutmut_1': x__get_git_commit__mutmut_1, 
    'x__get_git_commit__mutmut_2': x__get_git_commit__mutmut_2, 
    'x__get_git_commit__mutmut_3': x__get_git_commit__mutmut_3, 
    'x__get_git_commit__mutmut_4': x__get_git_commit__mutmut_4, 
    'x__get_git_commit__mutmut_5': x__get_git_commit__mutmut_5, 
    'x__get_git_commit__mutmut_6': x__get_git_commit__mutmut_6, 
    'x__get_git_commit__mutmut_7': x__get_git_commit__mutmut_7, 
    'x__get_git_commit__mutmut_8': x__get_git_commit__mutmut_8, 
    'x__get_git_commit__mutmut_9': x__get_git_commit__mutmut_9, 
    'x__get_git_commit__mutmut_10': x__get_git_commit__mutmut_10, 
    'x__get_git_commit__mutmut_11': x__get_git_commit__mutmut_11, 
    'x__get_git_commit__mutmut_12': x__get_git_commit__mutmut_12, 
    'x__get_git_commit__mutmut_13': x__get_git_commit__mutmut_13, 
    'x__get_git_commit__mutmut_14': x__get_git_commit__mutmut_14, 
    'x__get_git_commit__mutmut_15': x__get_git_commit__mutmut_15, 
    'x__get_git_commit__mutmut_16': x__get_git_commit__mutmut_16, 
    'x__get_git_commit__mutmut_17': x__get_git_commit__mutmut_17, 
    'x__get_git_commit__mutmut_18': x__get_git_commit__mutmut_18, 
    'x__get_git_commit__mutmut_19': x__get_git_commit__mutmut_19, 
    'x__get_git_commit__mutmut_20': x__get_git_commit__mutmut_20, 
    'x__get_git_commit__mutmut_21': x__get_git_commit__mutmut_21, 
    'x__get_git_commit__mutmut_22': x__get_git_commit__mutmut_22, 
    'x__get_git_commit__mutmut_23': x__get_git_commit__mutmut_23, 
    'x__get_git_commit__mutmut_24': x__get_git_commit__mutmut_24, 
    'x__get_git_commit__mutmut_25': x__get_git_commit__mutmut_25, 
    'x__get_git_commit__mutmut_26': x__get_git_commit__mutmut_26, 
    'x__get_git_commit__mutmut_27': x__get_git_commit__mutmut_27, 
    'x__get_git_commit__mutmut_28': x__get_git_commit__mutmut_28, 
    'x__get_git_commit__mutmut_29': x__get_git_commit__mutmut_29, 
    'x__get_git_commit__mutmut_30': x__get_git_commit__mutmut_30, 
    'x__get_git_commit__mutmut_31': x__get_git_commit__mutmut_31, 
    'x__get_git_commit__mutmut_32': x__get_git_commit__mutmut_32, 
    'x__get_git_commit__mutmut_33': x__get_git_commit__mutmut_33
}

def _get_git_commit(*args, **kwargs):
    result = _mutmut_trampoline(x__get_git_commit__mutmut_orig, x__get_git_commit__mutmut_mutants, args, kwargs)
    return result 

_get_git_commit.__signature__ = _mutmut_signature(x__get_git_commit__mutmut_orig)
x__get_git_commit__mutmut_orig.__name__ = 'x__get_git_commit'


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_orig(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(value, ensure_ascii=False)
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_1(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = None
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(value, ensure_ascii=False)
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_2(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = None
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_3(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(None, ensure_ascii=False)
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_4(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(value, ensure_ascii=None)
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_5(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(ensure_ascii=False)
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_6(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(value, )
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened


# Sérialise un manifeste complet à côté du modèle entraîné
def x__flatten_hyperparams__mutmut_7(hyperparams: dict) -> dict[str, str]:
    """Aplati les hyperparamètres pour une exportation CSV lisible."""

    # Prépare un dictionnaire de sortie initialement vide
    flattened: dict[str, str] = {}
    # Parcourt chaque entrée pour extraire les valeurs simples
    for key, value in hyperparams.items():
        # Sérialise chaque valeur pour conserver la lisibilité CSV
        flattened[key] = json.dumps(value, ensure_ascii=True)
    # Retourne le dictionnaire aplati prêt pour l'écriture CSV
    return flattened

x__flatten_hyperparams__mutmut_mutants : ClassVar[MutantDict] = {
'x__flatten_hyperparams__mutmut_1': x__flatten_hyperparams__mutmut_1, 
    'x__flatten_hyperparams__mutmut_2': x__flatten_hyperparams__mutmut_2, 
    'x__flatten_hyperparams__mutmut_3': x__flatten_hyperparams__mutmut_3, 
    'x__flatten_hyperparams__mutmut_4': x__flatten_hyperparams__mutmut_4, 
    'x__flatten_hyperparams__mutmut_5': x__flatten_hyperparams__mutmut_5, 
    'x__flatten_hyperparams__mutmut_6': x__flatten_hyperparams__mutmut_6, 
    'x__flatten_hyperparams__mutmut_7': x__flatten_hyperparams__mutmut_7
}

def _flatten_hyperparams(*args, **kwargs):
    result = _mutmut_trampoline(x__flatten_hyperparams__mutmut_orig, x__flatten_hyperparams__mutmut_mutants, args, kwargs)
    return result 

_flatten_hyperparams.__signature__ = _mutmut_signature(x__flatten_hyperparams__mutmut_orig)
x__flatten_hyperparams__mutmut_orig.__name__ = 'x__flatten_hyperparams'


def x__write_manifest__mutmut_orig(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_1(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = None
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_2(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "XXsubjectXX": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_3(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "SUBJECT": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_4(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "XXrunXX": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_5(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "RUN": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_6(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "XXdata_dirXX": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_7(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "DATA_DIR": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_8(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(None),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_9(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = None
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_10(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(None)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_11(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_12(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(None) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_13(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(None)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_14(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = None
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_15(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "XXcv_scoresXX": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_16(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "CV_SCORES": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_17(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "XXcv_meanXX": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_18(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "CV_MEAN": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_19(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = None
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_20(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = None
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_21(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "XXmodelXX": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_22(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "MODEL": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_23(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(None),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_24(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["XXmodelXX"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_25(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["MODEL"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_26(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "XXscalerXX": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_27(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "SCALER": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_28(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(None) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_29(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["XXscalerXX"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_30(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["SCALER"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_31(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["XXscalerXX"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_32(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["SCALER"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_33(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "XXw_matrixXX": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_34(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "W_MATRIX": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_35(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(None),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_36(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["XXw_matrixXX"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_37(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["W_MATRIX"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_38(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = None
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_39(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "XXdatasetXX": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_40(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "DATASET": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_41(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "XXhyperparamsXX": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_42(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "HYPERPARAMS": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_43(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "XXscoresXX": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_44(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "SCORES": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_45(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "XXgit_commitXX": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_46(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "GIT_COMMIT": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_47(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "XXartifactsXX": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_48(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "ARTIFACTS": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_49(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = None
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_50(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir * "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_51(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "XXmanifest.jsonXX"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_52(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "MANIFEST.JSON"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_53(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(None)
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_54(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(None, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_55(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=None, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_56(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=None))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_57(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_58(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_59(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, ))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_60(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=True, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_61(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=3))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_62(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = None
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_63(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(None)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_64(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = None
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_65(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "XXsubjectXX": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_66(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "SUBJECT": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_67(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "XXrunXX": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_68(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "RUN": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_69(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "XXdata_dirXX": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_70(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "DATA_DIR": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_71(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(None),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_72(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "XXgit_commitXX": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_73(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "GIT_COMMIT": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_74(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "XXcv_scoresXX": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_75(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "CV_SCORES": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_76(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(None),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_77(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": "XX;XX".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_78(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(None) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_79(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "XXcv_meanXX": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_80(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "CV_MEAN": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_81(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "XXXX" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_82(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is not None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_83(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(None),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_84(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = None
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_85(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir * "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_86(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "XXmanifest.csvXX"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_87(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "MANIFEST.CSV"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_88(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open(None, newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_89(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline=None) as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_90(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open(newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_91(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", ) as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_92(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("XXwXX", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_93(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("W", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_94(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="XXXX") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_95(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = None
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_96(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(None, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_97(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=None)
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_98(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_99(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, )
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_100(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(None))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_101(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(None)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_102(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"XXjsonXX": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_103(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"JSON": manifest_json_path, "csv": manifest_csv_path}


def x__write_manifest__mutmut_104(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "XXcsvXX": manifest_csv_path}


def x__write_manifest__mutmut_105(
    request: TrainingRequest,
    target_dir: Path,
    cv_scores: np.ndarray,
    artifacts: dict[str, Path | None],
) -> dict[str, Path]:
    """Écrit des manifestes JSON et CSV décrivant le run d'entraînement."""

    # Prépare la section dataset pour identifier les entrées de données
    dataset = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
    }
    # Convertit la configuration de pipeline en dictionnaire sérialisable
    hyperparams = asdict(request.pipeline_config)
    # Calcule la moyenne des scores si la validation croisée a tourné
    cv_mean = float(np.mean(cv_scores)) if cv_scores.size else None
    # Prépare la section des scores en sérialisant les arrays numpy
    scores = {
        "cv_scores": cv_scores.tolist(),
        "cv_mean": cv_mean,
    }
    # Résout l'identifiant du commit git pour tracer les artefacts
    git_commit = _get_git_commit()
    # Prépare la section chemins pour retrouver rapidement les fichiers
    artifacts_section = {
        "model": str(artifacts["model"]),
        "scaler": str(artifacts["scaler"]) if artifacts["scaler"] else None,
        "w_matrix": str(artifacts["w_matrix"]),
    }
    # Assemble toutes les sections dans un objet manifeste unique
    manifest = {
        "dataset": dataset,
        "hyperparams": hyperparams,
        "scores": scores,
        "git_commit": git_commit,
        "artifacts": artifacts_section,
    }
    # Définit le chemin de sortie du manifeste JSON à côté des artefacts
    manifest_json_path = target_dir / "manifest.json"
    # Écrit le manifeste JSON sur disque en UTF-8 pour la portabilité
    manifest_json_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2))
    # Aplati les hyperparamètres pour faciliter la lecture dans un tableur
    flattened_hyperparams = _flatten_hyperparams(hyperparams)
    # Construit une ligne CSV unique regroupant toutes les informations
    csv_line = {
        "subject": request.subject,
        "run": request.run,
        "data_dir": str(request.data_dir),
        "git_commit": git_commit,
        "cv_scores": ";".join(str(score) for score in cv_scores.tolist()),
        "cv_mean": "" if cv_mean is None else str(cv_mean),
        **flattened_hyperparams,
    }
    # Définit le chemin du manifeste CSV à côté du JSON
    manifest_csv_path = target_dir / "manifest.csv"
    # Ouvre le fichier CSV en écriture sans lignes superflues
    with manifest_csv_path.open("w", newline="") as handle:
        # Initialise l'écriture CSV avec les clés détectées
        writer = csv.DictWriter(handle, fieldnames=list(csv_line.keys()))
        # Inscrit les en-têtes pour faciliter l'import dans un tableur
        writer.writeheader()
        # Inscrit la ligne unique décrivant le run en cours
        writer.writerow(csv_line)
    # Retourne les chemins des manifestes pour les appels appelants
    return {"json": manifest_json_path, "CSV": manifest_csv_path}

x__write_manifest__mutmut_mutants : ClassVar[MutantDict] = {
'x__write_manifest__mutmut_1': x__write_manifest__mutmut_1, 
    'x__write_manifest__mutmut_2': x__write_manifest__mutmut_2, 
    'x__write_manifest__mutmut_3': x__write_manifest__mutmut_3, 
    'x__write_manifest__mutmut_4': x__write_manifest__mutmut_4, 
    'x__write_manifest__mutmut_5': x__write_manifest__mutmut_5, 
    'x__write_manifest__mutmut_6': x__write_manifest__mutmut_6, 
    'x__write_manifest__mutmut_7': x__write_manifest__mutmut_7, 
    'x__write_manifest__mutmut_8': x__write_manifest__mutmut_8, 
    'x__write_manifest__mutmut_9': x__write_manifest__mutmut_9, 
    'x__write_manifest__mutmut_10': x__write_manifest__mutmut_10, 
    'x__write_manifest__mutmut_11': x__write_manifest__mutmut_11, 
    'x__write_manifest__mutmut_12': x__write_manifest__mutmut_12, 
    'x__write_manifest__mutmut_13': x__write_manifest__mutmut_13, 
    'x__write_manifest__mutmut_14': x__write_manifest__mutmut_14, 
    'x__write_manifest__mutmut_15': x__write_manifest__mutmut_15, 
    'x__write_manifest__mutmut_16': x__write_manifest__mutmut_16, 
    'x__write_manifest__mutmut_17': x__write_manifest__mutmut_17, 
    'x__write_manifest__mutmut_18': x__write_manifest__mutmut_18, 
    'x__write_manifest__mutmut_19': x__write_manifest__mutmut_19, 
    'x__write_manifest__mutmut_20': x__write_manifest__mutmut_20, 
    'x__write_manifest__mutmut_21': x__write_manifest__mutmut_21, 
    'x__write_manifest__mutmut_22': x__write_manifest__mutmut_22, 
    'x__write_manifest__mutmut_23': x__write_manifest__mutmut_23, 
    'x__write_manifest__mutmut_24': x__write_manifest__mutmut_24, 
    'x__write_manifest__mutmut_25': x__write_manifest__mutmut_25, 
    'x__write_manifest__mutmut_26': x__write_manifest__mutmut_26, 
    'x__write_manifest__mutmut_27': x__write_manifest__mutmut_27, 
    'x__write_manifest__mutmut_28': x__write_manifest__mutmut_28, 
    'x__write_manifest__mutmut_29': x__write_manifest__mutmut_29, 
    'x__write_manifest__mutmut_30': x__write_manifest__mutmut_30, 
    'x__write_manifest__mutmut_31': x__write_manifest__mutmut_31, 
    'x__write_manifest__mutmut_32': x__write_manifest__mutmut_32, 
    'x__write_manifest__mutmut_33': x__write_manifest__mutmut_33, 
    'x__write_manifest__mutmut_34': x__write_manifest__mutmut_34, 
    'x__write_manifest__mutmut_35': x__write_manifest__mutmut_35, 
    'x__write_manifest__mutmut_36': x__write_manifest__mutmut_36, 
    'x__write_manifest__mutmut_37': x__write_manifest__mutmut_37, 
    'x__write_manifest__mutmut_38': x__write_manifest__mutmut_38, 
    'x__write_manifest__mutmut_39': x__write_manifest__mutmut_39, 
    'x__write_manifest__mutmut_40': x__write_manifest__mutmut_40, 
    'x__write_manifest__mutmut_41': x__write_manifest__mutmut_41, 
    'x__write_manifest__mutmut_42': x__write_manifest__mutmut_42, 
    'x__write_manifest__mutmut_43': x__write_manifest__mutmut_43, 
    'x__write_manifest__mutmut_44': x__write_manifest__mutmut_44, 
    'x__write_manifest__mutmut_45': x__write_manifest__mutmut_45, 
    'x__write_manifest__mutmut_46': x__write_manifest__mutmut_46, 
    'x__write_manifest__mutmut_47': x__write_manifest__mutmut_47, 
    'x__write_manifest__mutmut_48': x__write_manifest__mutmut_48, 
    'x__write_manifest__mutmut_49': x__write_manifest__mutmut_49, 
    'x__write_manifest__mutmut_50': x__write_manifest__mutmut_50, 
    'x__write_manifest__mutmut_51': x__write_manifest__mutmut_51, 
    'x__write_manifest__mutmut_52': x__write_manifest__mutmut_52, 
    'x__write_manifest__mutmut_53': x__write_manifest__mutmut_53, 
    'x__write_manifest__mutmut_54': x__write_manifest__mutmut_54, 
    'x__write_manifest__mutmut_55': x__write_manifest__mutmut_55, 
    'x__write_manifest__mutmut_56': x__write_manifest__mutmut_56, 
    'x__write_manifest__mutmut_57': x__write_manifest__mutmut_57, 
    'x__write_manifest__mutmut_58': x__write_manifest__mutmut_58, 
    'x__write_manifest__mutmut_59': x__write_manifest__mutmut_59, 
    'x__write_manifest__mutmut_60': x__write_manifest__mutmut_60, 
    'x__write_manifest__mutmut_61': x__write_manifest__mutmut_61, 
    'x__write_manifest__mutmut_62': x__write_manifest__mutmut_62, 
    'x__write_manifest__mutmut_63': x__write_manifest__mutmut_63, 
    'x__write_manifest__mutmut_64': x__write_manifest__mutmut_64, 
    'x__write_manifest__mutmut_65': x__write_manifest__mutmut_65, 
    'x__write_manifest__mutmut_66': x__write_manifest__mutmut_66, 
    'x__write_manifest__mutmut_67': x__write_manifest__mutmut_67, 
    'x__write_manifest__mutmut_68': x__write_manifest__mutmut_68, 
    'x__write_manifest__mutmut_69': x__write_manifest__mutmut_69, 
    'x__write_manifest__mutmut_70': x__write_manifest__mutmut_70, 
    'x__write_manifest__mutmut_71': x__write_manifest__mutmut_71, 
    'x__write_manifest__mutmut_72': x__write_manifest__mutmut_72, 
    'x__write_manifest__mutmut_73': x__write_manifest__mutmut_73, 
    'x__write_manifest__mutmut_74': x__write_manifest__mutmut_74, 
    'x__write_manifest__mutmut_75': x__write_manifest__mutmut_75, 
    'x__write_manifest__mutmut_76': x__write_manifest__mutmut_76, 
    'x__write_manifest__mutmut_77': x__write_manifest__mutmut_77, 
    'x__write_manifest__mutmut_78': x__write_manifest__mutmut_78, 
    'x__write_manifest__mutmut_79': x__write_manifest__mutmut_79, 
    'x__write_manifest__mutmut_80': x__write_manifest__mutmut_80, 
    'x__write_manifest__mutmut_81': x__write_manifest__mutmut_81, 
    'x__write_manifest__mutmut_82': x__write_manifest__mutmut_82, 
    'x__write_manifest__mutmut_83': x__write_manifest__mutmut_83, 
    'x__write_manifest__mutmut_84': x__write_manifest__mutmut_84, 
    'x__write_manifest__mutmut_85': x__write_manifest__mutmut_85, 
    'x__write_manifest__mutmut_86': x__write_manifest__mutmut_86, 
    'x__write_manifest__mutmut_87': x__write_manifest__mutmut_87, 
    'x__write_manifest__mutmut_88': x__write_manifest__mutmut_88, 
    'x__write_manifest__mutmut_89': x__write_manifest__mutmut_89, 
    'x__write_manifest__mutmut_90': x__write_manifest__mutmut_90, 
    'x__write_manifest__mutmut_91': x__write_manifest__mutmut_91, 
    'x__write_manifest__mutmut_92': x__write_manifest__mutmut_92, 
    'x__write_manifest__mutmut_93': x__write_manifest__mutmut_93, 
    'x__write_manifest__mutmut_94': x__write_manifest__mutmut_94, 
    'x__write_manifest__mutmut_95': x__write_manifest__mutmut_95, 
    'x__write_manifest__mutmut_96': x__write_manifest__mutmut_96, 
    'x__write_manifest__mutmut_97': x__write_manifest__mutmut_97, 
    'x__write_manifest__mutmut_98': x__write_manifest__mutmut_98, 
    'x__write_manifest__mutmut_99': x__write_manifest__mutmut_99, 
    'x__write_manifest__mutmut_100': x__write_manifest__mutmut_100, 
    'x__write_manifest__mutmut_101': x__write_manifest__mutmut_101, 
    'x__write_manifest__mutmut_102': x__write_manifest__mutmut_102, 
    'x__write_manifest__mutmut_103': x__write_manifest__mutmut_103, 
    'x__write_manifest__mutmut_104': x__write_manifest__mutmut_104, 
    'x__write_manifest__mutmut_105': x__write_manifest__mutmut_105
}

def _write_manifest(*args, **kwargs):
    result = _mutmut_trampoline(x__write_manifest__mutmut_orig, x__write_manifest__mutmut_mutants, args, kwargs)
    return result 

_write_manifest.__signature__ = _mutmut_signature(x__write_manifest__mutmut_orig)
x__write_manifest__mutmut_orig.__name__ = 'x__write_manifest'


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_orig(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_1(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = None
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_2(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(None, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_3(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, None, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_4(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, None, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_5(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, None)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_6(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_7(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_8(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_9(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, )
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_10(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = None
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_11(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(None)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_12(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = None
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_13(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(None)
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_14(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(None).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_15(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = None
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_16(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = None
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_17(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(None, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_18(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, None)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_19(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_20(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, )
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_21(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = None
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_22(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array(None)
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_23(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits <= MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_24(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            None
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_25(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "XXAVERTISSEMENT: effectif par classe insuffisant pour la XX"
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_26(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "avertissement: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_27(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: EFFECTIF PAR CLASSE INSUFFISANT POUR LA "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_28(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = None
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_29(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=None, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_30(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=None, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_31(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=None
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_32(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_33(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_34(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_35(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=False, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_36(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = None
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_37(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(None, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_38(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, None, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_39(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, None, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_40(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=None)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_41(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_42(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_43(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_44(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, )
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_45(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(None, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_46(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, None)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_47(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_48(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, )
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_49(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = None
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_50(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject * request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_51(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir * request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_52(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=None, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_53(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=None)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_54(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_55(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, )    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_56(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=False, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_57(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=False)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_58(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=None, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_59(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=None)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_60(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_61(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, )
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_62(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=False, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_63(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=False)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_64(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = None
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_65(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir * "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_66(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "XXmodel.joblibXX"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_67(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "MODEL.JOBLIB"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_68(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(None, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_69(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, None)
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_70(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_71(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, )
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_72(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(None))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_73(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = None
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_74(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get(None)
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_75(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("XXscalerXX")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_76(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("SCALER")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_77(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_78(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(None, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_79(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, None)
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_80(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_81(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, )
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_82(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir * "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_83(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "XXscaler.joblibXX")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_84(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "SCALER.JOBLIB")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_85(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = None
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_86(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["XXdimensionalityXX"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_87(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["DIMENSIONALITY"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_88(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(None)
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_89(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir * "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_90(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "XXw_matrix.joblibXX")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_91(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "W_MATRIX.JOBLIB")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_92(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = ""
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_93(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_94(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = None
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_95(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir * "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_96(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "XXscaler.joblibXX"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_97(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "SCALER.JOBLIB"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_98(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = None
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_99(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir * "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_100(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "XXw_matrix.joblibXX"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_101(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "W_MATRIX.JOBLIB"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_102(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = None
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_103(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        None,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_104(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        None,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_105(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        None,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_106(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        None,
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_107(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_108(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_109(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_110(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_111(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "XXmodelXX": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_112(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "MODEL": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_113(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "XXscalerXX": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_114(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "SCALER": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_115(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "XXw_matrixXX": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_116(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "W_MATRIX": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_117(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "XXcv_scoresXX": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_118(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "CV_SCORES": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_119(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "XXmodel_pathXX": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_120(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "MODEL_PATH": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_121(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "XXscaler_pathXX": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_122(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "SCALER_PATH": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_123(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "XXw_matrix_pathXX": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_124(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "W_MATRIX_PATH": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_125(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "XXmanifest_pathXX": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_126(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "MANIFEST_PATH": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_127(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["XXjsonXX"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_128(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["JSON"],
        "manifest_csv_path": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_129(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "XXmanifest_csv_pathXX": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_130(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "MANIFEST_CSV_PATH": manifest_paths["csv"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_131(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["XXcsvXX"],
    }


# Exécute la validation croisée et l'entraînement final
def x_run_training__mutmut_132(request: TrainingRequest) -> dict:
    """Entraîne la pipeline et sauvegarde ses artefacts."""

    # Charge ou génère les tableaux numpy nécessaires à l'entraînement
    X, y = _load_data(request.subject, request.run, request.data_dir, request.raw_dir)
    # Construit la pipeline complète sans préprocesseur amont
    pipeline = build_pipeline(request.pipeline_config)
    # Calcule le nombre minimal d'échantillons par classe pour calibrer la CV
    min_class_count = int(np.bincount(y).min())
    # Déclare le nombre de splits cible imposé par la consigne (10)
    requested_splits = DEFAULT_CV_SPLITS
    # Calcule le nombre de splits atteignable avec la classe minoritaire
    n_splits = min(requested_splits, min_class_count)
    # Initialise un tableau vide lorsque la validation croisée est impossible
    cv_scores = np.array([])
    # Vérifie si l'effectif autorise une validation croisée exploitable
    if n_splits < MIN_CV_SPLITS:
        # Signale la désactivation de la validation croisée par manque d'échantillons
        print(
            "AVERTISSEMENT: effectif par classe insuffisant pour la "
            "validation croisée, cross-val ignorée"
        )
    else:
        # Configure une StratifiedKFold stable sur le nombre de splits calculé
        cv = StratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE
        )
        # Calcule les scores de validation croisée sur l'ensemble du pipeline
        cv_scores = cross_val_score(pipeline, X, y, cv=cv)
    # Ajuste la pipeline sur toutes les données après évaluation
    pipeline.fit(X, y)
    # Prépare le dossier d'artefacts spécifique au sujet et au run
    target_dir = request.artifacts_dir / request.subject / request.run
    # Assure l'existence du parent pour stabiliser la création du dossier cible
    target_dir.parent.mkdir(parents=True, exist_ok=True)    
    # Crée les répertoires au besoin pour éviter les erreurs de sauvegarde    
    target_dir.mkdir(parents=True, exist_ok=True)
    # Calcule le chemin du fichier modèle pour joblib
    model_path = target_dir / "model.joblib"
    # Sauvegarde la pipeline complète pour les prédictions futures
    save_pipeline(pipeline, str(model_path))
    # Récupère l'éventuel scaler pour une sauvegarde dédiée
    scaler_step = pipeline.named_steps.get("scaler")
    # Sauvegarde le scaler uniquement s'il est présent dans la pipeline
    if scaler_step is not None:
        # Dépose le scaler dans un fichier distinct pour inspection
        joblib.dump(scaler_step, target_dir / "scaler.joblib")
    # Récupère le réducteur de dimension pour exposer la matrice W
    dim_reducer: TPVDimReducer = pipeline.named_steps["dimensionality"]
    # Sauvegarde la matrice de projection pour les usages temps-réel
    dim_reducer.save(target_dir / "w_matrix.joblib")
    # Calcule le chemin du scaler pour l'ajouter au manifeste
    scaler_path = None
    # Renseigne le chemin du scaler uniquement lorsqu'il existe
    if scaler_step is not None:
        # Stocke le chemin vers le scaler sauvegardé pour le manifeste
        scaler_path = target_dir / "scaler.joblib"
    # Calcule le chemin du fichier W pour le référencer dans le manifeste
    w_matrix_path = target_dir / "w_matrix.joblib"
    # Écrit un manifeste décrivant l'entraînement et ses artefacts
    manifest_paths = _write_manifest(
        request,
        target_dir,
        cv_scores,
        {
            "model": model_path,
            "scaler": scaler_path,
            "w_matrix": w_matrix_path,
        },
    )
    # Retourne un rapport synthétique pour les tests et la CLI
    return {
        "cv_scores": cv_scores,
        "model_path": model_path,
        "scaler_path": scaler_path,
        "w_matrix_path": w_matrix_path,
        "manifest_path": manifest_paths["json"],
        "manifest_csv_path": manifest_paths["CSV"],
    }

x_run_training__mutmut_mutants : ClassVar[MutantDict] = {
'x_run_training__mutmut_1': x_run_training__mutmut_1, 
    'x_run_training__mutmut_2': x_run_training__mutmut_2, 
    'x_run_training__mutmut_3': x_run_training__mutmut_3, 
    'x_run_training__mutmut_4': x_run_training__mutmut_4, 
    'x_run_training__mutmut_5': x_run_training__mutmut_5, 
    'x_run_training__mutmut_6': x_run_training__mutmut_6, 
    'x_run_training__mutmut_7': x_run_training__mutmut_7, 
    'x_run_training__mutmut_8': x_run_training__mutmut_8, 
    'x_run_training__mutmut_9': x_run_training__mutmut_9, 
    'x_run_training__mutmut_10': x_run_training__mutmut_10, 
    'x_run_training__mutmut_11': x_run_training__mutmut_11, 
    'x_run_training__mutmut_12': x_run_training__mutmut_12, 
    'x_run_training__mutmut_13': x_run_training__mutmut_13, 
    'x_run_training__mutmut_14': x_run_training__mutmut_14, 
    'x_run_training__mutmut_15': x_run_training__mutmut_15, 
    'x_run_training__mutmut_16': x_run_training__mutmut_16, 
    'x_run_training__mutmut_17': x_run_training__mutmut_17, 
    'x_run_training__mutmut_18': x_run_training__mutmut_18, 
    'x_run_training__mutmut_19': x_run_training__mutmut_19, 
    'x_run_training__mutmut_20': x_run_training__mutmut_20, 
    'x_run_training__mutmut_21': x_run_training__mutmut_21, 
    'x_run_training__mutmut_22': x_run_training__mutmut_22, 
    'x_run_training__mutmut_23': x_run_training__mutmut_23, 
    'x_run_training__mutmut_24': x_run_training__mutmut_24, 
    'x_run_training__mutmut_25': x_run_training__mutmut_25, 
    'x_run_training__mutmut_26': x_run_training__mutmut_26, 
    'x_run_training__mutmut_27': x_run_training__mutmut_27, 
    'x_run_training__mutmut_28': x_run_training__mutmut_28, 
    'x_run_training__mutmut_29': x_run_training__mutmut_29, 
    'x_run_training__mutmut_30': x_run_training__mutmut_30, 
    'x_run_training__mutmut_31': x_run_training__mutmut_31, 
    'x_run_training__mutmut_32': x_run_training__mutmut_32, 
    'x_run_training__mutmut_33': x_run_training__mutmut_33, 
    'x_run_training__mutmut_34': x_run_training__mutmut_34, 
    'x_run_training__mutmut_35': x_run_training__mutmut_35, 
    'x_run_training__mutmut_36': x_run_training__mutmut_36, 
    'x_run_training__mutmut_37': x_run_training__mutmut_37, 
    'x_run_training__mutmut_38': x_run_training__mutmut_38, 
    'x_run_training__mutmut_39': x_run_training__mutmut_39, 
    'x_run_training__mutmut_40': x_run_training__mutmut_40, 
    'x_run_training__mutmut_41': x_run_training__mutmut_41, 
    'x_run_training__mutmut_42': x_run_training__mutmut_42, 
    'x_run_training__mutmut_43': x_run_training__mutmut_43, 
    'x_run_training__mutmut_44': x_run_training__mutmut_44, 
    'x_run_training__mutmut_45': x_run_training__mutmut_45, 
    'x_run_training__mutmut_46': x_run_training__mutmut_46, 
    'x_run_training__mutmut_47': x_run_training__mutmut_47, 
    'x_run_training__mutmut_48': x_run_training__mutmut_48, 
    'x_run_training__mutmut_49': x_run_training__mutmut_49, 
    'x_run_training__mutmut_50': x_run_training__mutmut_50, 
    'x_run_training__mutmut_51': x_run_training__mutmut_51, 
    'x_run_training__mutmut_52': x_run_training__mutmut_52, 
    'x_run_training__mutmut_53': x_run_training__mutmut_53, 
    'x_run_training__mutmut_54': x_run_training__mutmut_54, 
    'x_run_training__mutmut_55': x_run_training__mutmut_55, 
    'x_run_training__mutmut_56': x_run_training__mutmut_56, 
    'x_run_training__mutmut_57': x_run_training__mutmut_57, 
    'x_run_training__mutmut_58': x_run_training__mutmut_58, 
    'x_run_training__mutmut_59': x_run_training__mutmut_59, 
    'x_run_training__mutmut_60': x_run_training__mutmut_60, 
    'x_run_training__mutmut_61': x_run_training__mutmut_61, 
    'x_run_training__mutmut_62': x_run_training__mutmut_62, 
    'x_run_training__mutmut_63': x_run_training__mutmut_63, 
    'x_run_training__mutmut_64': x_run_training__mutmut_64, 
    'x_run_training__mutmut_65': x_run_training__mutmut_65, 
    'x_run_training__mutmut_66': x_run_training__mutmut_66, 
    'x_run_training__mutmut_67': x_run_training__mutmut_67, 
    'x_run_training__mutmut_68': x_run_training__mutmut_68, 
    'x_run_training__mutmut_69': x_run_training__mutmut_69, 
    'x_run_training__mutmut_70': x_run_training__mutmut_70, 
    'x_run_training__mutmut_71': x_run_training__mutmut_71, 
    'x_run_training__mutmut_72': x_run_training__mutmut_72, 
    'x_run_training__mutmut_73': x_run_training__mutmut_73, 
    'x_run_training__mutmut_74': x_run_training__mutmut_74, 
    'x_run_training__mutmut_75': x_run_training__mutmut_75, 
    'x_run_training__mutmut_76': x_run_training__mutmut_76, 
    'x_run_training__mutmut_77': x_run_training__mutmut_77, 
    'x_run_training__mutmut_78': x_run_training__mutmut_78, 
    'x_run_training__mutmut_79': x_run_training__mutmut_79, 
    'x_run_training__mutmut_80': x_run_training__mutmut_80, 
    'x_run_training__mutmut_81': x_run_training__mutmut_81, 
    'x_run_training__mutmut_82': x_run_training__mutmut_82, 
    'x_run_training__mutmut_83': x_run_training__mutmut_83, 
    'x_run_training__mutmut_84': x_run_training__mutmut_84, 
    'x_run_training__mutmut_85': x_run_training__mutmut_85, 
    'x_run_training__mutmut_86': x_run_training__mutmut_86, 
    'x_run_training__mutmut_87': x_run_training__mutmut_87, 
    'x_run_training__mutmut_88': x_run_training__mutmut_88, 
    'x_run_training__mutmut_89': x_run_training__mutmut_89, 
    'x_run_training__mutmut_90': x_run_training__mutmut_90, 
    'x_run_training__mutmut_91': x_run_training__mutmut_91, 
    'x_run_training__mutmut_92': x_run_training__mutmut_92, 
    'x_run_training__mutmut_93': x_run_training__mutmut_93, 
    'x_run_training__mutmut_94': x_run_training__mutmut_94, 
    'x_run_training__mutmut_95': x_run_training__mutmut_95, 
    'x_run_training__mutmut_96': x_run_training__mutmut_96, 
    'x_run_training__mutmut_97': x_run_training__mutmut_97, 
    'x_run_training__mutmut_98': x_run_training__mutmut_98, 
    'x_run_training__mutmut_99': x_run_training__mutmut_99, 
    'x_run_training__mutmut_100': x_run_training__mutmut_100, 
    'x_run_training__mutmut_101': x_run_training__mutmut_101, 
    'x_run_training__mutmut_102': x_run_training__mutmut_102, 
    'x_run_training__mutmut_103': x_run_training__mutmut_103, 
    'x_run_training__mutmut_104': x_run_training__mutmut_104, 
    'x_run_training__mutmut_105': x_run_training__mutmut_105, 
    'x_run_training__mutmut_106': x_run_training__mutmut_106, 
    'x_run_training__mutmut_107': x_run_training__mutmut_107, 
    'x_run_training__mutmut_108': x_run_training__mutmut_108, 
    'x_run_training__mutmut_109': x_run_training__mutmut_109, 
    'x_run_training__mutmut_110': x_run_training__mutmut_110, 
    'x_run_training__mutmut_111': x_run_training__mutmut_111, 
    'x_run_training__mutmut_112': x_run_training__mutmut_112, 
    'x_run_training__mutmut_113': x_run_training__mutmut_113, 
    'x_run_training__mutmut_114': x_run_training__mutmut_114, 
    'x_run_training__mutmut_115': x_run_training__mutmut_115, 
    'x_run_training__mutmut_116': x_run_training__mutmut_116, 
    'x_run_training__mutmut_117': x_run_training__mutmut_117, 
    'x_run_training__mutmut_118': x_run_training__mutmut_118, 
    'x_run_training__mutmut_119': x_run_training__mutmut_119, 
    'x_run_training__mutmut_120': x_run_training__mutmut_120, 
    'x_run_training__mutmut_121': x_run_training__mutmut_121, 
    'x_run_training__mutmut_122': x_run_training__mutmut_122, 
    'x_run_training__mutmut_123': x_run_training__mutmut_123, 
    'x_run_training__mutmut_124': x_run_training__mutmut_124, 
    'x_run_training__mutmut_125': x_run_training__mutmut_125, 
    'x_run_training__mutmut_126': x_run_training__mutmut_126, 
    'x_run_training__mutmut_127': x_run_training__mutmut_127, 
    'x_run_training__mutmut_128': x_run_training__mutmut_128, 
    'x_run_training__mutmut_129': x_run_training__mutmut_129, 
    'x_run_training__mutmut_130': x_run_training__mutmut_130, 
    'x_run_training__mutmut_131': x_run_training__mutmut_131, 
    'x_run_training__mutmut_132': x_run_training__mutmut_132
}

def run_training(*args, **kwargs):
    result = _mutmut_trampoline(x_run_training__mutmut_orig, x_run_training__mutmut_mutants, args, kwargs)
    return result 

run_training.__signature__ = _mutmut_signature(x_run_training__mutmut_orig)
x_run_training__mutmut_orig.__name__ = 'x_run_training'


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_orig(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_1(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = None
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_2(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = None
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_3(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(None)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_4(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_5(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler != "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_6(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "XXnoneXX" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_7(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "NONE" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_8(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = None
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_9(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_10(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = None
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_11(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(None, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_12(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, None, None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_13(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr("n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_14(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_15(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", )
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_16(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "XXn_componentsXX", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_17(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "N_COMPONENTS", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_18(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = None
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_19(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=None,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_20(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=None,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_21(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=None,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_22(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=None,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_23(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=None,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_24(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=None,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_25(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=None,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_26(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_27(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_28(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_29(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_30(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_31(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_32(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_33(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = None
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_34(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=None,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_35(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=None,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_36(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=None,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_37(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=None,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_38(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=None,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_39(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=None,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_40(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_41(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_42(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_43(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_44(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_45(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_46(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = None
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_47(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(None)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_48(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = None

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_49(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["XXcv_scoresXX"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_50(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["CV_SCORES"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_51(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) or cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_52(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size >= 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_53(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 1:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_54(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = None
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_55(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            None, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_56(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=None, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_57(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=None, floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_58(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode=None
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_59(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_60(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_61(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_62(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_63(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=5, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_64(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator="XX XX", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_65(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="XXfixedXX"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_66(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="FIXED"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_67(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(None)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_68(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = None
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_69(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(None)
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_70(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(None)
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 0


# Point d'entrée principal pour l'exécution en ligne de commande
def x_main__mutmut_71(argv: list[str] | None = None) -> int:
    """Parse les arguments et lance l'entraînement."""

    # Construit le parser pour interpréter les arguments
    parser = build_parser()
    # Parse les arguments fournis par l'utilisateur
    args = parser.parse_args(argv)
    # Exécute la génération massive et s'arrête si le flag est positionné
    if args.build_all:
        _build_all_npy(args.raw_dir, args.data_dir)
        return 0
    # Convertit l'option scaler "none" en None pour la pipeline
    scaler = None if args.scaler == "none" else args.scaler
    # Calcule la valeur de normalisation en inversant le flag d'opt-out
    normalize = not args.no_normalize_features
    # Récupère le paramètre n_components s'il est fourni
    n_components = getattr(args, "n_components", None)
    # Construit la configuration de pipeline alignée sur mybci
    config = PipelineConfig(
        sfreq=args.sfreq,
        feature_strategy=args.feature_strategy,
        normalize_features=normalize,
        dim_method=args.dim_method,
        n_components=n_components,
        classifier=args.classifier,
        scaler=scaler,
    )
    # Déclenche l'entraînement massif si le flag est activé
    if args.train_all:
        # Propulse la configuration commune vers l'ensemble des runs moteurs
        return _train_all_runs(
            config,
            args.data_dir,
            args.artifacts_dir,
            args.raw_dir,
        )
    # Regroupe les paramètres d'entraînement dans une structure dédiée
    request = TrainingRequest(
        subject=args.subject,
        run=args.run,
        pipeline_config=config,
        data_dir=args.data_dir,
        artifacts_dir=args.artifacts_dir,
        raw_dir=args.raw_dir,
    )
    # Exécute l'entraînement et la sauvegarde des artefacts
    # Sécurise l'exécution pour afficher une erreur lisible sans trace
    try:
        # Lance l'entraînement et récupère le rapport pour afficher les scores
        result = run_training(request)
    except FileNotFoundError as error:
        # Remonte l'erreur utilisateur de manière concise pour la CLI
        print(f"ERREUR: {error}")
        # Expose un code de sortie explicite pour signaler l'échec
        return 1

    # Récupère les scores de validation croisée depuis le rapport
    cv_scores = result["cv_scores"]

    # Si des scores ont été calculés, on les affiche au format attendu
    if isinstance(cv_scores, np.ndarray) and cv_scores.size > 0:
        # Force quatre décimales fixes pour suivre l'exemple du sujet
        formatted_scores = np.array2string(
            cv_scores, precision=4, separator=" ", floatmode="fixed"
        )
        # Affiche le tableau numpy (format [0.6666 0.4444 ...])
        print(formatted_scores)
        # Calcule la moyenne pour l'affichage "cross_val_score: 0.5333"
        mean_score = float(cv_scores.mean())
        # Affiche la moyenne arrondie sur quatre décimales pour homogénéiser
        print(f"cross_val_score: {mean_score:.4f}")
    else:
        # Fallback lisible si la CV n'a pas pu être calculée
        print(np.array([]))
        print("cross_val_score: 0.0")

    # Retourne 0 pour signaler un succès CLI à mybci
    return 1

x_main__mutmut_mutants : ClassVar[MutantDict] = {
'x_main__mutmut_1': x_main__mutmut_1, 
    'x_main__mutmut_2': x_main__mutmut_2, 
    'x_main__mutmut_3': x_main__mutmut_3, 
    'x_main__mutmut_4': x_main__mutmut_4, 
    'x_main__mutmut_5': x_main__mutmut_5, 
    'x_main__mutmut_6': x_main__mutmut_6, 
    'x_main__mutmut_7': x_main__mutmut_7, 
    'x_main__mutmut_8': x_main__mutmut_8, 
    'x_main__mutmut_9': x_main__mutmut_9, 
    'x_main__mutmut_10': x_main__mutmut_10, 
    'x_main__mutmut_11': x_main__mutmut_11, 
    'x_main__mutmut_12': x_main__mutmut_12, 
    'x_main__mutmut_13': x_main__mutmut_13, 
    'x_main__mutmut_14': x_main__mutmut_14, 
    'x_main__mutmut_15': x_main__mutmut_15, 
    'x_main__mutmut_16': x_main__mutmut_16, 
    'x_main__mutmut_17': x_main__mutmut_17, 
    'x_main__mutmut_18': x_main__mutmut_18, 
    'x_main__mutmut_19': x_main__mutmut_19, 
    'x_main__mutmut_20': x_main__mutmut_20, 
    'x_main__mutmut_21': x_main__mutmut_21, 
    'x_main__mutmut_22': x_main__mutmut_22, 
    'x_main__mutmut_23': x_main__mutmut_23, 
    'x_main__mutmut_24': x_main__mutmut_24, 
    'x_main__mutmut_25': x_main__mutmut_25, 
    'x_main__mutmut_26': x_main__mutmut_26, 
    'x_main__mutmut_27': x_main__mutmut_27, 
    'x_main__mutmut_28': x_main__mutmut_28, 
    'x_main__mutmut_29': x_main__mutmut_29, 
    'x_main__mutmut_30': x_main__mutmut_30, 
    'x_main__mutmut_31': x_main__mutmut_31, 
    'x_main__mutmut_32': x_main__mutmut_32, 
    'x_main__mutmut_33': x_main__mutmut_33, 
    'x_main__mutmut_34': x_main__mutmut_34, 
    'x_main__mutmut_35': x_main__mutmut_35, 
    'x_main__mutmut_36': x_main__mutmut_36, 
    'x_main__mutmut_37': x_main__mutmut_37, 
    'x_main__mutmut_38': x_main__mutmut_38, 
    'x_main__mutmut_39': x_main__mutmut_39, 
    'x_main__mutmut_40': x_main__mutmut_40, 
    'x_main__mutmut_41': x_main__mutmut_41, 
    'x_main__mutmut_42': x_main__mutmut_42, 
    'x_main__mutmut_43': x_main__mutmut_43, 
    'x_main__mutmut_44': x_main__mutmut_44, 
    'x_main__mutmut_45': x_main__mutmut_45, 
    'x_main__mutmut_46': x_main__mutmut_46, 
    'x_main__mutmut_47': x_main__mutmut_47, 
    'x_main__mutmut_48': x_main__mutmut_48, 
    'x_main__mutmut_49': x_main__mutmut_49, 
    'x_main__mutmut_50': x_main__mutmut_50, 
    'x_main__mutmut_51': x_main__mutmut_51, 
    'x_main__mutmut_52': x_main__mutmut_52, 
    'x_main__mutmut_53': x_main__mutmut_53, 
    'x_main__mutmut_54': x_main__mutmut_54, 
    'x_main__mutmut_55': x_main__mutmut_55, 
    'x_main__mutmut_56': x_main__mutmut_56, 
    'x_main__mutmut_57': x_main__mutmut_57, 
    'x_main__mutmut_58': x_main__mutmut_58, 
    'x_main__mutmut_59': x_main__mutmut_59, 
    'x_main__mutmut_60': x_main__mutmut_60, 
    'x_main__mutmut_61': x_main__mutmut_61, 
    'x_main__mutmut_62': x_main__mutmut_62, 
    'x_main__mutmut_63': x_main__mutmut_63, 
    'x_main__mutmut_64': x_main__mutmut_64, 
    'x_main__mutmut_65': x_main__mutmut_65, 
    'x_main__mutmut_66': x_main__mutmut_66, 
    'x_main__mutmut_67': x_main__mutmut_67, 
    'x_main__mutmut_68': x_main__mutmut_68, 
    'x_main__mutmut_69': x_main__mutmut_69, 
    'x_main__mutmut_70': x_main__mutmut_70, 
    'x_main__mutmut_71': x_main__mutmut_71
}

def main(*args, **kwargs):
    result = _mutmut_trampoline(x_main__mutmut_orig, x_main__mutmut_mutants, args, kwargs)
    return result 

main.__signature__ = _mutmut_signature(x_main__mutmut_orig)
x_main__mutmut_orig.__name__ = 'x_main'


# Protège l'exécution directe pour exposer un exit code explicite
if __name__ == "__main__":  # pragma: no cover - exécution CLI directe
    # Retourne l'issue du main comme code de sortie du processus
    raise SystemExit(main())
