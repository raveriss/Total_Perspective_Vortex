#!/usr/bin/env python3

"""Interface CLI pour piloter les workflows d'entraînement et de prédiction."""

# Préserve argparse pour parser les options CLI avec validation
import argparse

# Préserve subprocess pour lancer les modules en sous-processus isolés
import subprocess

# Préserve sys pour identifier l'interpréteur courant
import sys

# Préserve dataclass pour regrouper les paramètres du pipeline
from dataclasses import dataclass

# Facilite la gestion portable des chemins de données et artefacts
from pathlib import Path

# Centralise la moyenne arithmétique pour agréger les accuracies
from statistics import mean

# Garantit l'accès aux séquences typées pour mypy
from typing import Iterable, Mapping, Sequence

# Fournit une barre de progression compacte pendant l'évaluation globale
from tqdm import tqdm

# Assure l'accès à tpv via src lors d'une exécution locale
sys.path.append(str(Path(__file__).resolve().parent / "src"))

# Fournit les fonctions de prédiction pour l'évaluation globale
from tpv import predict as tpv_predict
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


# Centralise les options nécessaires pour invoquer le mode realtime
@dataclass
class RealtimeCallConfig:
    """Conteneur des paramètres transmis au module realtime."""

    # Identifie le sujet cible pour cibler les artefacts du modèle
    subject: str
    # Identifie le run cible pour sélectionner la session
    run: str
    # Fixe la taille de fenêtre glissante en échantillons
    window_size: int
    # Fixe le pas entre deux fenêtres successives
    step_size: int
    # Fixe la taille du buffer utilisé pour lisser les prédictions
    buffer_size: int
    # Renseigne la fréquence d'échantillonnage pour calculer les offsets
    sfreq: float
    # Spécifie la latence maximale autorisée pour chaque fenêtre
    max_latency: float
    # Spécifie le répertoire contenant les fichiers numpy streamés
    data_dir: str
    # Spécifie le répertoire racine où lire les artefacts entraînés
    artifacts_dir: str


# Centralise les options nécessaires pour invoquer un module TPV
@dataclass
class ModuleCallConfig:
    """Conteneur des paramètres transmis aux modules train/predict."""

    # Identifie le sujet cible pour charger les données correspondantes
    subject: str
    # Identifie le run cible pour charger la bonne session
    run: str
    # Sélectionne le classifieur pour harmoniser train et predict
    classifier: str
    # Sélectionne le scaler optionnel pour stabiliser les features
    scaler: str | None
    # Harmonise la stratégie d'extraction des features
    feature_strategy: str
    # Choisit la méthode de réduction de dimension
    dim_method: str
    # Spécifie le nombre de composantes projetées
    n_components: int | None
    # Indique si les features doivent être normalisées
    normalize_features: bool


# Centralise les répertoires nécessaires pendant l'évaluation globale
@dataclass
class EvaluationPaths:
    """Conteneur des chemins racine utilisés pendant l'évaluation."""

    # Stocke le chemin vers les données prétraitées pour les runs
    data_root: Path
    # Stocke le chemin vers les artefacts entraînés pour les runs
    artifacts_root: Path
    # Stocke le chemin vers les fichiers EDF bruts pour les runs
    raw_root: Path


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_orig(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_1(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = None
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_2(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "XX-mXX",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_3(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-M",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_4(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(None)
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_5(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["XX--classifierXX", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_6(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--CLASSIFIER", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_7(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_8(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(None)
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_9(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["XX--scalerXX", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_10(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--SCALER", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_11(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(None)
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_12(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["XX--feature-strategyXX", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_13(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--FEATURE-STRATEGY", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_14(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(None)
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_15(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["XX--dim-methodXX", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_16(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--DIM-METHOD", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_17(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_18(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(None)
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_19(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["XX--n-componentsXX", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_20(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--N-COMPONENTS", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_21(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(None)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_22(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_23(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append(None)
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_24(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("XX--no-normalize-featuresXX")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_25(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--NO-NORMALIZE-FEATURES")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_26(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = None
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_27(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(None, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_28(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=None)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_29(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_30(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, )
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer un module TPV
def x__call_module__mutmut_31(module_name: str, config: ModuleCallConfig) -> int:
    """Invoke un module TPV en ajoutant les options du pipeline."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        module_name,
        config.subject,
        config.run,
    ]
    # Ajoute le classifieur choisi pour transmettre la préférence utilisateur
    command.extend(["--classifier", config.classifier])
    # Ajoute la stratégie de scaler uniquement lorsqu'elle est définie
    if config.scaler is not None:
        # Propulse le scaler choisi vers le module appelé
        command.extend(["--scaler", config.scaler])
    # Ajoute la stratégie de features pour harmoniser train et predict
    command.extend(["--feature-strategy", config.feature_strategy])
    # Ajoute la méthode de réduction de dimension pour la cohérence
    command.extend(["--dim-method", config.dim_method])
    # Ajoute le nombre de composantes si fourni pour contrôler la compression
    if config.n_components is not None:
        # Passe n_components sous forme de chaîne pour argparse descendant
        command.extend(["--n-components", str(config.n_components)])
    # Ajoute un indicateur pour désactiver la normalisation si demandé
    if not config.normalize_features:
        # Utilise un flag booléen pour inverser la valeur par défaut
        command.append("--no-normalize-features")
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=True)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode

x__call_module__mutmut_mutants : ClassVar[MutantDict] = {
'x__call_module__mutmut_1': x__call_module__mutmut_1, 
    'x__call_module__mutmut_2': x__call_module__mutmut_2, 
    'x__call_module__mutmut_3': x__call_module__mutmut_3, 
    'x__call_module__mutmut_4': x__call_module__mutmut_4, 
    'x__call_module__mutmut_5': x__call_module__mutmut_5, 
    'x__call_module__mutmut_6': x__call_module__mutmut_6, 
    'x__call_module__mutmut_7': x__call_module__mutmut_7, 
    'x__call_module__mutmut_8': x__call_module__mutmut_8, 
    'x__call_module__mutmut_9': x__call_module__mutmut_9, 
    'x__call_module__mutmut_10': x__call_module__mutmut_10, 
    'x__call_module__mutmut_11': x__call_module__mutmut_11, 
    'x__call_module__mutmut_12': x__call_module__mutmut_12, 
    'x__call_module__mutmut_13': x__call_module__mutmut_13, 
    'x__call_module__mutmut_14': x__call_module__mutmut_14, 
    'x__call_module__mutmut_15': x__call_module__mutmut_15, 
    'x__call_module__mutmut_16': x__call_module__mutmut_16, 
    'x__call_module__mutmut_17': x__call_module__mutmut_17, 
    'x__call_module__mutmut_18': x__call_module__mutmut_18, 
    'x__call_module__mutmut_19': x__call_module__mutmut_19, 
    'x__call_module__mutmut_20': x__call_module__mutmut_20, 
    'x__call_module__mutmut_21': x__call_module__mutmut_21, 
    'x__call_module__mutmut_22': x__call_module__mutmut_22, 
    'x__call_module__mutmut_23': x__call_module__mutmut_23, 
    'x__call_module__mutmut_24': x__call_module__mutmut_24, 
    'x__call_module__mutmut_25': x__call_module__mutmut_25, 
    'x__call_module__mutmut_26': x__call_module__mutmut_26, 
    'x__call_module__mutmut_27': x__call_module__mutmut_27, 
    'x__call_module__mutmut_28': x__call_module__mutmut_28, 
    'x__call_module__mutmut_29': x__call_module__mutmut_29, 
    'x__call_module__mutmut_30': x__call_module__mutmut_30, 
    'x__call_module__mutmut_31': x__call_module__mutmut_31
}

def _call_module(*args, **kwargs):
    result = _mutmut_trampoline(x__call_module__mutmut_orig, x__call_module__mutmut_mutants, args, kwargs)
    return result 

_call_module.__signature__ = _mutmut_signature(x__call_module__mutmut_orig)
x__call_module__mutmut_orig.__name__ = 'x__call_module'


# Définit la structure décrivant un protocole expérimental
@dataclass
class ExperimentDefinition:
    """Associe un identifiant d'expérience au run correspondant."""

    # Identifie la position de l'expérience dans la séquence requise
    index: int
    # Associe l'expérience au run Physionet à évaluer
    run: str


# Construit la liste des six expériences décrites dans le sujet
def _build_default_experiments() -> list[ExperimentDefinition]:
    """Expose les six expériences demandées par la consigne."""

    # Mappe chaque expérience à un run Physionet pour l'évaluation
    return [
        # Explore le run R03 pour l'expérience 0
        ExperimentDefinition(index=0, run="R03"),
        # Explore le run R04 pour l'expérience 1
        ExperimentDefinition(index=1, run="R04"),
        # Explore le run R05 pour l'expérience 2
        ExperimentDefinition(index=2, run="R05"),
        # Explore le run R06 pour l'expérience 3
        ExperimentDefinition(index=3, run="R06"),
        # Explore le run R07 pour l'expérience 4
        ExperimentDefinition(index=4, run="R07"),
        # Explore le run R08 pour l'expérience 5
        ExperimentDefinition(index=5, run="R08"),
        # Explore le run R03 pour l'expérience 6
        ExperimentDefinition(index=0, run="R09"),
        # Explore le run R04 pour l'expérience 7
        ExperimentDefinition(index=1, run="R10"),
        # Explore le run R05 pour l'expérience 8
        ExperimentDefinition(index=2, run="R11"),
        # Explore le run R06 pour l'expérience 9
        ExperimentDefinition(index=3, run="R12"),
        # Explore le run R07 pour l'expérience 10
        ExperimentDefinition(index=4, run="R13"),
        # Explore le run R08 pour l'expérience 11
        ExperimentDefinition(index=5, run="R14"),
    ]


# Convertit un numéro de sujet numérique en identifiant Physionet
def _subject_identifier(subject_index: int) -> str:
    """Retourne l'identifiant Sxxx attendu dans les répertoires."""

    # Formate le numéro sur trois chiffres en préfixant le S imposé
    return f"S{subject_index:03d}"


# Calcule l'accuracy pour un couple (expérience, sujet)
def _evaluate_experiment_subject(
    experiment: ExperimentDefinition,
    subject_index: int,
    data_dir: Path,
    artifacts_dir: Path,
    raw_dir: Path,
) -> float:
    """Évalue un sujet sur le run associé à une expérience donnée."""

    # Construit l'identifiant complet du sujet pour les chemins disque
    subject = _subject_identifier(subject_index)
    # Exécute evaluate_run sur le run associé à l'expérience
    result = tpv_predict.evaluate_run(
        subject,
        experiment.run,
        data_dir,
        artifacts_dir,
        raw_dir,
    )
    # Convertit l'accuracy en float natif pour l'agrégation
    return float(result["accuracy"])


# Calcule la moyenne d'accuracies pour une séquence fournie
def _safe_mean(values: Iterable[float]) -> float:
    """Retourne 0.0 si la séquence est vide pour sécuriser l'affichage."""

    # Convertit l'itérable en liste pour gérer la longueur et le calcul
    measurements = list(values)
    # Retourne 0.0 si aucune valeur n'est disponible
    if not measurements:
        # Force une moyenne nulle pour éviter ZeroDivisionError
        return 0.0
    # Calcule la moyenne arithmétique standard
    return mean(measurements)


# Recense les sujets disposant d'un modèle entraîné pour un run donné
def _subjects_with_available_model(run: str, artifacts_root: Path) -> list[int]:
    """Liste les indices de sujets dont le modèle est présent sur disque."""

    # Prépare une collection ordonnée pour les indices extraits
    subjects: list[int] = []
    # Parcourt les fichiers model.joblib correspondant au run demandé
    for model_path in artifacts_root.glob(f"S*/{run}/model.joblib"):
        # Identifie le dossier sujet à partir du chemin du modèle
        subject_dir = model_path.parent.parent
        # Vérifie que le nom de dossier respecte le préfixe attendu
        if not subject_dir.name.startswith("S"):
            # Ignore les dossiers inattendus pour éviter des erreurs de parsing
            continue
        try:
            # Convertit la partie numérique du nom en entier pour l'itération
            subject_index = int(subject_dir.name[1:])
        except ValueError:
            # Ignore les dossiers mal nommés pour maintenir la robustesse
            continue
        # Ajoute l'indice extrait pour inclure le sujet dans l'évaluation
        subjects.append(subject_index)
    # Trie les indices pour respecter l'ordre croissant des sujets
    subjects.sort()
    # Retourne la liste triée pour itérer dans un ordre reproductible
    return subjects


# Prépare la disponibilité des modèles par run avant l'évaluation globale
def _collect_run_availability(
    experiments: Sequence[ExperimentDefinition],
    expected_subjects: Sequence[str],
) -> tuple[dict[str, list[int]], dict[str, list[str]]]:
    """Associe chaque run aux sujets attendus pour déclencher l'auto-train."""

    # Prépare un cache pour associer chaque run aux sujets parcourus
    available_subjects_by_run: dict[str, list[int]] = {}
    # Prépare un relevé vide car l'auto-train doit combler les absences
    missing_models_by_run: dict[str, list[str]] = {}
    # Parcourt chaque expérience pour initialiser la liste des sujets
    for experiment in experiments:
        # Ignore le run déjà traité pour éviter les doublons
        if experiment.run in available_subjects_by_run:
            # Passe au run suivant dès que le cache contient le run
            continue
        # Convertit les identifiants Sxxx en indices numériques exploitables
        subject_indices = [int(subject[1:]) for subject in expected_subjects]
        # Associe tous les sujets au run pour laisser evaluate_run entraîner
        available_subjects_by_run[experiment.run] = subject_indices
        # Marque l'absence de modèles manquants grâce à l'auto-train
        missing_models_by_run[experiment.run] = []
    # Retourne les deux structures pour l'évaluation globale
    return available_subjects_by_run, missing_models_by_run


# Évalue chaque expérience en accumulant les résultats et les absences
def _evaluate_experiments(
    experiments: Sequence[ExperimentDefinition],
    available_subjects_by_run: Mapping[str, Sequence[int]],
    paths: EvaluationPaths,
    progress: tqdm | None = None,
) -> tuple[dict[int, list[float]], list[str], list[ExperimentDefinition]]:
    """Exécute les évaluations et retourne les scores et manquants."""

    # Prépare le stockage des accuracies par expérience
    per_experiment_scores: dict[int, list[float]] = {
        # Initialise la collection d'accuracies pour chaque expérience
        exp.index: []
        for exp in experiments
    }
    # Prépare la liste des sujets ou runs introuvables lors des calculs
    missing_entries: list[str] = []
    # Prépare la liste des expériences sans modèle pour les ignorer
    skipped_experiments: list[ExperimentDefinition] = []
    # Parcourt chaque expérience demandée
    for experiment in experiments:
        # Récupère la liste des sujets disposant d'un modèle pour ce run
        available_subjects = list(available_subjects_by_run.get(experiment.run, []))
        # Informe l'utilisateur si aucun modèle n'est disponible pour ce run
        if not available_subjects:
            # Signale que l'expérience sera ignorée faute de modèle présent
            print(
                "AVERTISSEMENT: aucun modèle disponible pour "
                f"{experiment.run}, expérience {experiment.index} ignorée"
            )
            # Archive l'expérience ignorée pour le résumé des moyennes
            skipped_experiments.append(experiment)
            # Passe à l'expérience suivante pour éviter une boucle vide
            continue
        # Parcourt l'ensemble des sujets disposant d'un modèle
        for subject_index in available_subjects:
            # Évalue le sujet courant sur l'expérience en cours
            try:
                # Calcule l'accuracy en rechargeant le modèle entraîné
                accuracy = _evaluate_experiment_subject(
                    experiment,
                    subject_index,
                    paths.data_root,
                    paths.artifacts_root,
                    paths.raw_root,
                )
            except FileNotFoundError as error:
                # Informe l'utilisateur qu'un prérequis manque pour ce run
                print(f"AVERTISSEMENT: {error}")
                # Calcule l'identifiant du sujet manquant pour le récapitulatif
                subject = _subject_identifier(subject_index)
                # Ajoute l'entrée manquante pour un récapitulatif final
                missing_entries.append(f"{subject}:{experiment.run}")
                # Ignore ce sujet pour poursuivre l'exploration globale
                continue
            finally:
                # Actualise la barre de progression lorsqu'elle est activée
                if progress is not None:
                    # Incrémente la progression d'un sujet évalué ou tenté
                    progress.update(1)
            # Stocke l'accuracy pour le calcul des moyennes
            per_experiment_scores[experiment.index].append(accuracy)
            # Affiche l'accuracy du sujet au format imposé
            print(
                f"experiment {experiment.index}: "
                f"subject {subject_index:03d}: accuracy = {accuracy:.4f}"
            )
    # Retourne les résultats et les expériences ignorées
    return per_experiment_scores, missing_entries, skipped_experiments


# Affiche les moyennes par expérience et retourne la moyenne globale
def _print_experiment_means(
    experiments: Sequence[ExperimentDefinition],
    per_experiment_scores: Mapping[int, Sequence[float]],
) -> float:
    """Calcule et affiche les moyennes d'accuracy par expérience."""

    # Affiche l'entête du bloc de moyennes par expérience
    print("\nMean accuracy of the six different experiments for all 109 subjects:")
    # Parcourt chaque expérience pour calculer sa moyenne
    for experiment in experiments:
        # Extrait les scores accumulés pour l'expérience courante
        experiment_scores = per_experiment_scores[experiment.index]
        # Contrôle la disponibilité d'artefacts avant de calculer la moyenne
        if not experiment_scores:
            # Mentionne explicitement l'absence d'artefacts pour l'expérience
            print(f"experiment {experiment.index}:\t\taccuracy = N/A (skipped)")
            # Passe au run suivant pour éviter une moyenne vide
            continue
        # Calcule la moyenne de l'expérience courante
        experiment_mean = _safe_mean(experiment_scores)
        # Affiche la moyenne alignée sur l'exemple fourni
        print(f"experiment {experiment.index}:\t\taccuracy = " f"{experiment_mean:.4f}")
    # Calcule la moyenne globale des six expériences
    global_mean = _safe_mean(
        # Agrège uniquement les expériences disposant d'artefacts
        _safe_mean(per_experiment_scores[exp.index])
        for exp in experiments
        if per_experiment_scores[exp.index]
    )
    # Affiche la moyenne globale demandée par la consigne
    print(f"\nMean accuracy of 6 experiments: {global_mean:.4f}")
    # Retourne la moyenne pour réutilisation éventuelle
    return global_mean


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_orig(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_1(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            None
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_2(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "XXAVERTISSEMENT: certaines données EDF ou .npy sont manquantes. XX"
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_3(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "avertissement: certaines données edf ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_4(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: CERTAINES DONNÉES EDF OU .NPY SONT MANQUANTES. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_5(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print(None)
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_6(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " - ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_7(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("XXPremiers manquants: XX" + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_8(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_9(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("PREMIERS MANQUANTS: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_10(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(None))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_11(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + "XX, XX".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_12(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:11]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_13(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(None):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_14(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            None
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_15(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "XXAVERTISSEMENT: certains modèles entraînés sont absents. XX"
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_16(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "avertissement: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_17(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: CERTAINS MODÈLES ENTRAÎNÉS SONT ABSENTS. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_18(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = None
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_19(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(None)
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_20(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects or len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_21(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) != expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_22(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print(None)
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_23(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " - ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_24(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("XXRuns sans aucun modèle disponible: XX" + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_25(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_26(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("RUNS SANS AUCUN MODÈLE DISPONIBLE: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_27(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(None))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_28(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + "XX, XX".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_29(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(None):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_30(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_31(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                None
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_32(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(None)})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_33(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {'XX, XX'.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_34(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:6])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_35(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            None
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_36(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "XXPour générer un modèle manquant, lancez par exemple :\nXX"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_37(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_38(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "POUR GÉNÉRER UN MODÈLE MANQUANT, LANCEZ PAR EXEMPLE :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_39(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = None
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_40(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            None
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_41(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = "XX, XX".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_42(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            None
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_43(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "XXAVERTISSEMENT: les expériences suivantes ont été ignorées XX"
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_44(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "avertissement: les expériences suivantes ont été ignorées "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )


# Affiche les messages d'alerte pour guider l'utilisateur
def x__report_missing_artifacts__mutmut_45(
    missing_entries: Sequence[str],
    missing_models_by_run: Mapping[str, Sequence[str]],
    skipped_experiments: Sequence[ExperimentDefinition],
    expected_subject_count: int,
) -> None:
    """Émet les avertissements sur les données et modèles manquants."""

    # Vérifie si des données sont manquantes pour informer l'utilisateur
    if missing_entries:
        # Résume le volume d'entrées absentes pour déclencher une action
        print(
            "AVERTISSEMENT: certaines données EDF ou .npy sont manquantes. "
            f"Couples sujet/run concernés: {len(missing_entries)}. "
            "Téléchargez les EDF dans data ou regénérez les .npy."
        )
        # Affiche un aperçu des premières références manquantes pour guider
        print("Premiers manquants: " + ", ".join(missing_entries[:10]))
    # Vérifie s'il manque des modèles entraînés pour certains runs
    if any(missing_models_by_run.values()):
        # Informe l'utilisateur qu'il manque des artefacts pour plusieurs sujets
        print(
            "AVERTISSEMENT: certains modèles entraînés sont absents. "
            "Générez ou copiez les artifacts manquants pour compléter l'évaluation."
        )
        # Identifie les runs totalement dépourvus de modèles pour prioriser les actions
        fully_missing_runs = [
            run
            for run, subjects in sorted(missing_models_by_run.items())
            if subjects and len(subjects) == expected_subject_count
        ]
        # Met en avant les runs sans modèles pour débloquer la génération
        if fully_missing_runs:
            print("Runs sans aucun modèle disponible: " + ", ".join(fully_missing_runs))
        # Parcourt les runs pour afficher un extrait des sujets à compléter
        for run, subjects in sorted(missing_models_by_run.items()):
            # Ignore l'affichage si aucun modèle ne manque pour ce run
            if not subjects:
                # Passe au run suivant lorsqu'il est complet
                continue
            # Affiche le nombre total de modèles manquants pour ce run
            print(
                f"Run {run}: modèles manquants pour {len(subjects)} sujets "
                f"(exemples: {', '.join(subjects[:5])})"
            )
        # Aide l'utilisateur en rappelant la commande de génération d'artefacts
        print(
            "Pour générer un modèle manquant, lancez par exemple :\n"
            "  poetry run python scripts/train.py S001 R04 --feature-strategy fft "
            "--dim-method pca"
        )
    # Vérifie si certaines expériences ont été ignorées pour le calcul global
    if skipped_experiments:
        # Résume les expériences ignorées pour clarifier le global_mean affiché
        skipped_labels = ", ".join(
            f"{exp.index} ({exp.run})" for exp in skipped_experiments
        )
        # Invite l'utilisateur à générer les artefacts avant de relancer
        print(
            "AVERTISSEMENT: LES EXPÉRIENCES SUIVANTES ONT ÉTÉ IGNORÉES "
            f"faute de modèles: {skipped_labels}. "
            "Générez les artefacts correspondants pour obtenir une moyenne "
            "complète."
        )

x__report_missing_artifacts__mutmut_mutants : ClassVar[MutantDict] = {
'x__report_missing_artifacts__mutmut_1': x__report_missing_artifacts__mutmut_1, 
    'x__report_missing_artifacts__mutmut_2': x__report_missing_artifacts__mutmut_2, 
    'x__report_missing_artifacts__mutmut_3': x__report_missing_artifacts__mutmut_3, 
    'x__report_missing_artifacts__mutmut_4': x__report_missing_artifacts__mutmut_4, 
    'x__report_missing_artifacts__mutmut_5': x__report_missing_artifacts__mutmut_5, 
    'x__report_missing_artifacts__mutmut_6': x__report_missing_artifacts__mutmut_6, 
    'x__report_missing_artifacts__mutmut_7': x__report_missing_artifacts__mutmut_7, 
    'x__report_missing_artifacts__mutmut_8': x__report_missing_artifacts__mutmut_8, 
    'x__report_missing_artifacts__mutmut_9': x__report_missing_artifacts__mutmut_9, 
    'x__report_missing_artifacts__mutmut_10': x__report_missing_artifacts__mutmut_10, 
    'x__report_missing_artifacts__mutmut_11': x__report_missing_artifacts__mutmut_11, 
    'x__report_missing_artifacts__mutmut_12': x__report_missing_artifacts__mutmut_12, 
    'x__report_missing_artifacts__mutmut_13': x__report_missing_artifacts__mutmut_13, 
    'x__report_missing_artifacts__mutmut_14': x__report_missing_artifacts__mutmut_14, 
    'x__report_missing_artifacts__mutmut_15': x__report_missing_artifacts__mutmut_15, 
    'x__report_missing_artifacts__mutmut_16': x__report_missing_artifacts__mutmut_16, 
    'x__report_missing_artifacts__mutmut_17': x__report_missing_artifacts__mutmut_17, 
    'x__report_missing_artifacts__mutmut_18': x__report_missing_artifacts__mutmut_18, 
    'x__report_missing_artifacts__mutmut_19': x__report_missing_artifacts__mutmut_19, 
    'x__report_missing_artifacts__mutmut_20': x__report_missing_artifacts__mutmut_20, 
    'x__report_missing_artifacts__mutmut_21': x__report_missing_artifacts__mutmut_21, 
    'x__report_missing_artifacts__mutmut_22': x__report_missing_artifacts__mutmut_22, 
    'x__report_missing_artifacts__mutmut_23': x__report_missing_artifacts__mutmut_23, 
    'x__report_missing_artifacts__mutmut_24': x__report_missing_artifacts__mutmut_24, 
    'x__report_missing_artifacts__mutmut_25': x__report_missing_artifacts__mutmut_25, 
    'x__report_missing_artifacts__mutmut_26': x__report_missing_artifacts__mutmut_26, 
    'x__report_missing_artifacts__mutmut_27': x__report_missing_artifacts__mutmut_27, 
    'x__report_missing_artifacts__mutmut_28': x__report_missing_artifacts__mutmut_28, 
    'x__report_missing_artifacts__mutmut_29': x__report_missing_artifacts__mutmut_29, 
    'x__report_missing_artifacts__mutmut_30': x__report_missing_artifacts__mutmut_30, 
    'x__report_missing_artifacts__mutmut_31': x__report_missing_artifacts__mutmut_31, 
    'x__report_missing_artifacts__mutmut_32': x__report_missing_artifacts__mutmut_32, 
    'x__report_missing_artifacts__mutmut_33': x__report_missing_artifacts__mutmut_33, 
    'x__report_missing_artifacts__mutmut_34': x__report_missing_artifacts__mutmut_34, 
    'x__report_missing_artifacts__mutmut_35': x__report_missing_artifacts__mutmut_35, 
    'x__report_missing_artifacts__mutmut_36': x__report_missing_artifacts__mutmut_36, 
    'x__report_missing_artifacts__mutmut_37': x__report_missing_artifacts__mutmut_37, 
    'x__report_missing_artifacts__mutmut_38': x__report_missing_artifacts__mutmut_38, 
    'x__report_missing_artifacts__mutmut_39': x__report_missing_artifacts__mutmut_39, 
    'x__report_missing_artifacts__mutmut_40': x__report_missing_artifacts__mutmut_40, 
    'x__report_missing_artifacts__mutmut_41': x__report_missing_artifacts__mutmut_41, 
    'x__report_missing_artifacts__mutmut_42': x__report_missing_artifacts__mutmut_42, 
    'x__report_missing_artifacts__mutmut_43': x__report_missing_artifacts__mutmut_43, 
    'x__report_missing_artifacts__mutmut_44': x__report_missing_artifacts__mutmut_44, 
    'x__report_missing_artifacts__mutmut_45': x__report_missing_artifacts__mutmut_45
}

def _report_missing_artifacts(*args, **kwargs):
    result = _mutmut_trampoline(x__report_missing_artifacts__mutmut_orig, x__report_missing_artifacts__mutmut_mutants, args, kwargs)
    return result 

_report_missing_artifacts.__signature__ = _mutmut_signature(x__report_missing_artifacts__mutmut_orig)
x__report_missing_artifacts__mutmut_orig.__name__ = 'x__report_missing_artifacts'


# Parcourt les 6 expériences et les 109 sujets en affichant les accuracies
def _run_global_evaluation(
    experiments: Sequence[ExperimentDefinition] | None = None,
    data_dir: Path | None = None,
    artifacts_dir: Path | None = None,
    raw_dir: Path | None = None,
) -> int:
    """Exécute la boucle d'évaluation globale décrite dans le sujet."""

    # Utilise les expériences par défaut si aucune liste n'est fournie
    experiment_definitions = list(experiments or _build_default_experiments())
    # Normalise les chemins racine de données pour les appels descendants
    data_root = data_dir or Path("data")
    # Normalise le répertoire d'artefacts pour les modèles entraînés
    artifacts_root = artifacts_dir or Path("artifacts")
    # Normalise le répertoire des EDF bruts désormais stockés dans data/
    raw_root = raw_dir or Path("data")
    # Construit la liste des identifiants attendus pour les 109 sujets
    expected_subjects = [_subject_identifier(idx) for idx in range(1, 110)]
    # Calcule la disponibilité des modèles pour chaque run
    available_subjects_by_run, missing_models_by_run = _collect_run_availability(
        experiment_definitions, expected_subjects
    )
    # Calcule le nombre total de sujets évalués pour calibrer la progression
    total_subjects = sum(
        # Calcule le nombre de sujets associés à chaque run
        len(available_subjects_by_run.get(experiment.run, []))
        # Parcourt toutes les expériences à traiter
        for experiment in experiment_definitions
    )
    # Instancie une barre de progression pour suivre la boucle exhaustive
    progress = tqdm(
        # Fixe le nombre total d'itérations attendues
        total=total_subjects,
        # Décrit l'étape pour contextualiser la jauge
        desc="Évaluation globale",
        # Spécifie l'unité pour l'affichage lisible
        unit="sujets",
        # Colore la jauge pour la rendre visible dans le flux INFO
        colour="cyan",
        # Contient la largeur pour éviter une jauge interminable
        ncols=120,
        # Évite de laisser la barre bloquer la sortie finale
        leave=False,
    )
    # Exécute les évaluations et collecte les résultats
    per_experiment_scores, missing_entries, skipped_experiments = _evaluate_experiments(
        experiment_definitions,
        available_subjects_by_run,
        EvaluationPaths(
            # Fournit le répertoire racine des données prétraitées
            data_root=data_root,
            # Fournit le répertoire racine des artefacts entraînés
            artifacts_root=artifacts_root,
            # Fournit le répertoire racine des fichiers EDF bruts
            raw_root=raw_root,
        ),
        progress,
    )
    # Termine la barre de progression pour libérer la sortie standard
    progress.close()
    # Calcule et affiche les moyennes par expérience
    _print_experiment_means(experiment_definitions, per_experiment_scores)
    # Émet un récapitulatif des artefacts manquants
    _report_missing_artifacts(
        missing_entries,
        missing_models_by_run,
        skipped_experiments,
        len(expected_subjects),
    )
    # Retourne 0 pour signaler le succès global
    return 0


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_orig(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_1(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = None
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_2(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "XX-mXX",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_3(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-M",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_4(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "XXtpv.realtimeXX",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_5(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "TPV.REALTIME",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_6(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(None)
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_7(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["XX--window-sizeXX", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_8(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--WINDOW-SIZE", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_9(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(None)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_10(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(None)
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_11(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["XX--step-sizeXX", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_12(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--STEP-SIZE", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_13(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(None)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_14(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(None)
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_15(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["XX--buffer-sizeXX", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_16(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--BUFFER-SIZE", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_17(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(None)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_18(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(None)
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_19(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["XX--max-latencyXX", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_20(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--MAX-LATENCY", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_21(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(None)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_22(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(None)
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_23(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["XX--sfreqXX", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_24(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--SFREQ", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_25(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(None)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_26(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(None)
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_27(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["XX--data-dirXX", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_28(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--DATA-DIR", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_29(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(None)
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_30(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["XX--artifacts-dirXX", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_31(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--ARTIFACTS-DIR", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_32(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = None
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_33(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(None, check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_34(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=None)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_35(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(check=False)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_36(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, )
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode


# Construit la ligne de commande pour invoquer le mode realtime
def x__call_realtime__mutmut_37(config: RealtimeCallConfig) -> int:
    """Invoke le module tpv.realtime avec les paramètres streaming."""

    # Initialise la commande avec l'interpréteur courant et le module ciblé
    command: list[str] = [
        sys.executable,
        "-m",
        "tpv.realtime",
        config.subject,
        config.run,
    ]
    # Ajoute la taille de fenêtre demandée pour le streaming
    command.extend(["--window-size", str(config.window_size)])
    # Ajoute le pas de glissement entre fenêtres successives
    command.extend(["--step-size", str(config.step_size)])
    # Ajoute la taille du buffer utilisé pour lisser les prédictions
    command.extend(["--buffer-size", str(config.buffer_size)])
    # Ajoute la latence maximale autorisée pour surveiller le SLA
    command.extend(["--max-latency", str(config.max_latency)])
    # Ajoute la fréquence d'échantillonnage pour calculer les offsets
    command.extend(["--sfreq", str(config.sfreq)])
    # Ajoute le répertoire des données streamées
    command.extend(["--data-dir", config.data_dir])
    # Ajoute le répertoire d'artefacts contenant le modèle entraîné
    command.extend(["--artifacts-dir", config.artifacts_dir])
    # Exécute la commande en capturant le code retour sans lever d'exception
    completed = subprocess.run(command, check=True)
    # Retourne le code retour pour propagation à l'appelant principal
    return completed.returncode

x__call_realtime__mutmut_mutants : ClassVar[MutantDict] = {
'x__call_realtime__mutmut_1': x__call_realtime__mutmut_1, 
    'x__call_realtime__mutmut_2': x__call_realtime__mutmut_2, 
    'x__call_realtime__mutmut_3': x__call_realtime__mutmut_3, 
    'x__call_realtime__mutmut_4': x__call_realtime__mutmut_4, 
    'x__call_realtime__mutmut_5': x__call_realtime__mutmut_5, 
    'x__call_realtime__mutmut_6': x__call_realtime__mutmut_6, 
    'x__call_realtime__mutmut_7': x__call_realtime__mutmut_7, 
    'x__call_realtime__mutmut_8': x__call_realtime__mutmut_8, 
    'x__call_realtime__mutmut_9': x__call_realtime__mutmut_9, 
    'x__call_realtime__mutmut_10': x__call_realtime__mutmut_10, 
    'x__call_realtime__mutmut_11': x__call_realtime__mutmut_11, 
    'x__call_realtime__mutmut_12': x__call_realtime__mutmut_12, 
    'x__call_realtime__mutmut_13': x__call_realtime__mutmut_13, 
    'x__call_realtime__mutmut_14': x__call_realtime__mutmut_14, 
    'x__call_realtime__mutmut_15': x__call_realtime__mutmut_15, 
    'x__call_realtime__mutmut_16': x__call_realtime__mutmut_16, 
    'x__call_realtime__mutmut_17': x__call_realtime__mutmut_17, 
    'x__call_realtime__mutmut_18': x__call_realtime__mutmut_18, 
    'x__call_realtime__mutmut_19': x__call_realtime__mutmut_19, 
    'x__call_realtime__mutmut_20': x__call_realtime__mutmut_20, 
    'x__call_realtime__mutmut_21': x__call_realtime__mutmut_21, 
    'x__call_realtime__mutmut_22': x__call_realtime__mutmut_22, 
    'x__call_realtime__mutmut_23': x__call_realtime__mutmut_23, 
    'x__call_realtime__mutmut_24': x__call_realtime__mutmut_24, 
    'x__call_realtime__mutmut_25': x__call_realtime__mutmut_25, 
    'x__call_realtime__mutmut_26': x__call_realtime__mutmut_26, 
    'x__call_realtime__mutmut_27': x__call_realtime__mutmut_27, 
    'x__call_realtime__mutmut_28': x__call_realtime__mutmut_28, 
    'x__call_realtime__mutmut_29': x__call_realtime__mutmut_29, 
    'x__call_realtime__mutmut_30': x__call_realtime__mutmut_30, 
    'x__call_realtime__mutmut_31': x__call_realtime__mutmut_31, 
    'x__call_realtime__mutmut_32': x__call_realtime__mutmut_32, 
    'x__call_realtime__mutmut_33': x__call_realtime__mutmut_33, 
    'x__call_realtime__mutmut_34': x__call_realtime__mutmut_34, 
    'x__call_realtime__mutmut_35': x__call_realtime__mutmut_35, 
    'x__call_realtime__mutmut_36': x__call_realtime__mutmut_36, 
    'x__call_realtime__mutmut_37': x__call_realtime__mutmut_37
}

def _call_realtime(*args, **kwargs):
    result = _mutmut_trampoline(x__call_realtime__mutmut_orig, x__call_realtime__mutmut_mutants, args, kwargs)
    return result 

_call_realtime.__signature__ = _mutmut_signature(x__call_realtime__mutmut_orig)
x__call_realtime__mutmut_orig.__name__ = 'x__call_realtime'


# Imprime les prédictions epoch par epoch dans un format compact
def _print_epoch_predictions(
    y_true: Sequence[int],
    y_pred: Sequence[int],
    accuracy: float,
) -> None:
    """Affiche les prédictions détaillées comme dans l'exemple mybci."""

    # Affiche l'en-tête décrivant les colonnes
    print("epoch nb: [prediction] [truth] equal?")
    # Calcule la largeur minimale pour l'index d'epoch
    n_epochs = len(y_true)
    # Utilise au moins deux chiffres pour mimer l'exemple fourni
    index_width = max(2, len(str(max(n_epochs - 1, 0))))
    # Parcourt chaque paire vérité terrain / prédiction
    for idx, (pred, truth) in enumerate(zip(y_pred, y_true, strict=True)):
        # Calcule si la prédiction correspond à la vérité terrain
        is_equal = bool(int(pred) == int(truth))
        # Affiche la ligne formatée pour l'epoch courante
        print(
            f"epoch {idx:0{index_width}d}: " f"[{int(pred)}] [{int(truth)}] {is_equal}"
        )
    # Affiche l'accuracy globale formatée sur quatre décimales
    print(f"Accuracy: {accuracy:.4f}")


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_orig() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_1() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = None
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_2() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description=None,
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_3() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage=None,
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_4() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_5() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_6() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="XXPilote un workflow d'entraînement ou de prédiction TPVXX",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_7() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="pilote un workflow d'entraînement ou de prédiction tpv",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_8() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="PILOTE UN WORKFLOW D'ENTRAÎNEMENT OU DE PRÉDICTION TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_9() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="XXpython mybci.py <subject> <run> {train,predict,realtime}XX",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_10() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="PYTHON MYBCI.PY <SUBJECT> <RUN> {TRAIN,PREDICT,REALTIME}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_11() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument(None, help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_12() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help=None)
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_13() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument(help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_14() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", )
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_15() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("XXsubjectXX", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_16() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("SUBJECT", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_17() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="XXIdentifiant du sujet (ex: S001)XX")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_18() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="identifiant du sujet (ex: s001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_19() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="IDENTIFIANT DU SUJET (EX: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_20() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument(None, help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_21() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help=None)
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_22() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument(help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_23() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", )
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_24() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("XXrunXX", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_25() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("RUN", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_26() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="XXIdentifiant du run (ex: R01)XX")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_27() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="identifiant du run (ex: r01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_28() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="IDENTIFIANT DU RUN (EX: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_29() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        None,
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_30() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=None,
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_31() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help=None,
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_32() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_33() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_34() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_35() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "XXmodeXX",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_36() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "MODE",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_37() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("XXtrainXX", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_38() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("TRAIN", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_39() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "XXpredictXX", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_40() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "PREDICT", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_41() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "XXrealtimeXX"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_42() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "REALTIME"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_43() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="XXChoix du pipeline à lancerXX",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_44() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_45() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="CHOIX DU PIPELINE À LANCER",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_46() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        None,
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_47() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=None,
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_48() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default=None,
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_49() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help=None,
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_50() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_51() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_52() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_53() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_54() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "XX--classifierXX",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_55() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--CLASSIFIER",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_56() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("XXldaXX", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_57() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("LDA", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_58() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "XXlogisticXX", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_59() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "LOGISTIC", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_60() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "XXsvmXX", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_61() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "SVM", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_62() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "XXcentroidXX"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_63() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "CENTROID"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_64() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="XXldaXX",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_65() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="LDA",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_66() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="XXChoix du classifieur finalXX",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_67() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_68() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="CHOIX DU CLASSIFIEUR FINAL",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_69() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        None,
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_70() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=None,
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_71() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default=None,
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_72() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help=None,
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_73() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_74() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_75() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_76() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_77() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "XX--scalerXX",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_78() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--SCALER",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_79() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("XXstandardXX", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_80() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("STANDARD", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_81() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "XXrobustXX", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_82() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "ROBUST", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_83() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "XXnoneXX"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_84() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "NONE"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_85() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="XXnoneXX",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_86() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="NONE",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_87() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="XXScaler optionnel appliqué après les featuresXX",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_88() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_89() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="SCALER OPTIONNEL APPLIQUÉ APRÈS LES FEATURES",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_90() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        None,
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_91() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=None,
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_92() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default=None,
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_93() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help=None,
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_94() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_95() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_96() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_97() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_98() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "XX--feature-strategyXX",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_99() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--FEATURE-STRATEGY",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_100() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("XXfftXX", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_101() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("FFT", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_102() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "XXwaveletXX"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_103() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "WAVELET"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_104() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="XXfftXX",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_105() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="FFT",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_106() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="XXMéthode d'extraction des featuresXX",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_107() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_108() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="MÉTHODE D'EXTRACTION DES FEATURES",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_109() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        None,
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_110() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=None,
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_111() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default=None,
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_112() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help=None,
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_113() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_114() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_115() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_116() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_117() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "XX--dim-methodXX",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_118() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--DIM-METHOD",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_119() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("XXpcaXX", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_120() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("PCA", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_121() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "XXcspXX"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_122() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "CSP"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_123() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="XXpcaXX",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_124() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="PCA",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_125() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="XXTechnique de réduction de dimensionXX",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_126() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_127() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="TECHNIQUE DE RÉDUCTION DE DIMENSION",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_128() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        None,
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_129() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=None,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_130() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=None,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_131() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help=None,
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_132() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_133() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_134() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_135() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_136() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "XX--n-componentsXX",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_137() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--N-COMPONENTS",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_138() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="XXNombre de composantes à conserverXX",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_139() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_140() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="NOMBRE DE COMPOSANTES À CONSERVER",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_141() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        None,
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_142() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action=None,
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_143() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help=None,
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_144() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_145() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_146() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_147() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "XX--no-normalize-featuresXX",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_148() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--NO-NORMALIZE-FEATURES",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_149() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="XXstore_trueXX",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_150() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="STORE_TRUE",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_151() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="XXDésactive la normalisation des featuresXX",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_152() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_153() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="DÉSACTIVE LA NORMALISATION DES FEATURES",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_154() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        None,
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_155() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=None,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_156() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=None,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_157() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help=None,
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_158() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_159() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_160() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_161() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_162() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "XX--window-sizeXX",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_163() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--WINDOW-SIZE",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_164() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=51,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_165() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="XXTaille de fenêtre glissante pour le mode realtimeXX",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_166() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_167() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="TAILLE DE FENÊTRE GLISSANTE POUR LE MODE REALTIME",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_168() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        None,
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_169() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=None,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_170() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=None,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_171() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help=None,
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_172() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_173() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_174() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_175() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_176() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "XX--step-sizeXX",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_177() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--STEP-SIZE",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_178() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=26,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_179() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="XXPas entre deux fenêtres en streaming realtimeXX",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_180() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_181() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="PAS ENTRE DEUX FENÊTRES EN STREAMING REALTIME",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_182() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        None,
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_183() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=None,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_184() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=None,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_185() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help=None,
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_186() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_187() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_188() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_189() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_190() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "XX--buffer-sizeXX",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_191() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--BUFFER-SIZE",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_192() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=4,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_193() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="XXTaille du buffer de lissage pour le mode realtimeXX",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_194() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_195() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="TAILLE DU BUFFER DE LISSAGE POUR LE MODE REALTIME",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_196() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        None,
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_197() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=None,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_198() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=None,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_199() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help=None,
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_200() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_201() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_202() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_203() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_204() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "XX--sfreqXX",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_205() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--SFREQ",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_206() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=51.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_207() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="XXFréquence d'échantillonnage appliquée au flux realtimeXX",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_208() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_209() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="FRÉQUENCE D'ÉCHANTILLONNAGE APPLIQUÉE AU FLUX REALTIME",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_210() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        None,
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_211() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=None,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_212() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=None,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_213() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help=None,
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_214() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_215() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_216() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_217() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_218() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "XX--max-latencyXX",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_219() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--MAX-LATENCY",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_220() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=3.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_221() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="XXLatence maximale autorisée par fenêtre realtimeXX",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_222() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_223() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="LATENCE MAXIMALE AUTORISÉE PAR FENÊTRE REALTIME",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_224() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        None,
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_225() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default=None,
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_226() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help=None,
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_227() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_228() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_229() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_230() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "XX--data-dirXX",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_231() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--DATA-DIR",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_232() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="XXdataXX",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_233() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="DATA",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_234() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="XXRépertoire racine contenant les fichiers numpyXX",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_235() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_236() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="RÉPERTOIRE RACINE CONTENANT LES FICHIERS NUMPY",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_237() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        None,
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_238() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default=None,
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_239() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help=None,
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_240() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_241() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_242() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_243() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "XX--artifacts-dirXX",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_244() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--ARTIFACTS-DIR",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_245() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="XXartifactsXX",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_246() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="ARTIFACTS",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_247() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="XXRépertoire racine où récupérer le modèle entraînéXX",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_248() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_249() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="RÉPERTOIRE RACINE OÙ RÉCUPÉRER LE MODÈLE ENTRAÎNÉ",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_250() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        None,
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_251() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default=None,
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_252() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help=None,
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_253() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_254() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_255() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_256() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "XX--raw-dirXX",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_257() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--RAW-DIR",
        default="data",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_258() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="XXdataXX",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_259() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="DATA",
        help="Répertoire racine contenant les fichiers EDF bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_260() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="XXRépertoire racine contenant les fichiers EDF brutsXX",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_261() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="répertoire racine contenant les fichiers edf bruts",
    )
    # Retourne le parser configuré
    return parser


# Construit le parser CLI avec toutes les options du pipeline
def x_build_parser__mutmut_262() -> argparse.ArgumentParser:
    """Construit l'argument parser pour mybci."""

    # Instancie le parser avec description et usage explicite
    parser = argparse.ArgumentParser(
        description="Pilote un workflow d'entraînement ou de prédiction TPV",
        usage="python mybci.py <subject> <run> {train,predict,realtime}",
    )
    # Ajoute l'identifiant du sujet pour cibler les données
    parser.add_argument("subject", help="Identifiant du sujet (ex: S001)")
    # Ajoute l'identifiant du run pour cibler la session
    parser.add_argument("run", help="Identifiant du run (ex: R01)")
    # Ajoute le mode pour distinguer entraînement et prédiction
    parser.add_argument(
        "mode",
        choices=("train", "predict", "realtime"),
        help="Choix du pipeline à lancer",
    )
    # Ajoute le choix du classifieur pour composer le pipeline
    parser.add_argument(
        "--classifier",
        choices=("lda", "logistic", "svm", "centroid"),
        default="lda",
        help="Choix du classifieur final",
    )
    # Ajoute le scaler optionnel pour stabiliser les features
    parser.add_argument(
        "--scaler",
        choices=("standard", "robust", "none"),
        default="none",
        help="Scaler optionnel appliqué après les features",
    )
    # Ajoute la stratégie d'extraction de features pour harmoniser train/predict
    parser.add_argument(
        "--feature-strategy",
        choices=("fft", "wavelet"),
        default="fft",
        help="Méthode d'extraction des features",
    )
    # Ajoute la méthode de réduction de dimension pour ajuster la compression
    parser.add_argument(
        "--dim-method",
        choices=("pca", "csp"),
        default="pca",
        help="Technique de réduction de dimension",
    )
    # Ajoute le nombre de composantes pour contrôler la taille projetée
    parser.add_argument(
        "--n-components",
        type=int,
        default=argparse.SUPPRESS,
        help="Nombre de composantes à conserver",
    )
    # Ajoute un flag pour désactiver la normalisation des features
    parser.add_argument(
        "--no-normalize-features",
        action="store_true",
        help="Désactive la normalisation des features",
    )
    # Ajoute la taille de fenêtre pour la lecture streaming en realtime
    parser.add_argument(
        "--window-size",
        type=int,
        default=50,
        help="Taille de fenêtre glissante pour le mode realtime",
    )
    # Ajoute le pas entre deux fenêtres successives
    parser.add_argument(
        "--step-size",
        type=int,
        default=25,
        help="Pas entre deux fenêtres en streaming realtime",
    )
    # Ajoute la taille du buffer pour lisser les prédictions instantanées
    parser.add_argument(
        "--buffer-size",
        type=int,
        default=3,
        help="Taille du buffer de lissage pour le mode realtime",
    )
    # Ajoute la fréquence d'échantillonnage utilisée pour les offsets
    parser.add_argument(
        "--sfreq",
        type=float,
        default=50.0,
        help="Fréquence d'échantillonnage appliquée au flux realtime",
    )
    # Ajoute la latence maximale tolérée pour surveiller la boucle
    parser.add_argument(
        "--max-latency",
        type=float,
        default=2.0,
        help="Latence maximale autorisée par fenêtre realtime",
    )
    # Ajoute le répertoire de données nécessaire au streaming
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Répertoire racine contenant les fichiers numpy",
    )
    # Ajoute le répertoire d'artefacts où lire le modèle entraîné
    parser.add_argument(
        "--artifacts-dir",
        default="artifacts",
        help="Répertoire racine où récupérer le modèle entraîné",
    )
    # Ajoute le répertoire racine des fichiers EDF bruts
    parser.add_argument(
        "--raw-dir",
        default="data",
        help="RÉPERTOIRE RACINE CONTENANT LES FICHIERS EDF BRUTS",
    )
    # Retourne le parser configuré
    return parser

x_build_parser__mutmut_mutants : ClassVar[MutantDict] = {
'x_build_parser__mutmut_1': x_build_parser__mutmut_1, 
    'x_build_parser__mutmut_2': x_build_parser__mutmut_2, 
    'x_build_parser__mutmut_3': x_build_parser__mutmut_3, 
    'x_build_parser__mutmut_4': x_build_parser__mutmut_4, 
    'x_build_parser__mutmut_5': x_build_parser__mutmut_5, 
    'x_build_parser__mutmut_6': x_build_parser__mutmut_6, 
    'x_build_parser__mutmut_7': x_build_parser__mutmut_7, 
    'x_build_parser__mutmut_8': x_build_parser__mutmut_8, 
    'x_build_parser__mutmut_9': x_build_parser__mutmut_9, 
    'x_build_parser__mutmut_10': x_build_parser__mutmut_10, 
    'x_build_parser__mutmut_11': x_build_parser__mutmut_11, 
    'x_build_parser__mutmut_12': x_build_parser__mutmut_12, 
    'x_build_parser__mutmut_13': x_build_parser__mutmut_13, 
    'x_build_parser__mutmut_14': x_build_parser__mutmut_14, 
    'x_build_parser__mutmut_15': x_build_parser__mutmut_15, 
    'x_build_parser__mutmut_16': x_build_parser__mutmut_16, 
    'x_build_parser__mutmut_17': x_build_parser__mutmut_17, 
    'x_build_parser__mutmut_18': x_build_parser__mutmut_18, 
    'x_build_parser__mutmut_19': x_build_parser__mutmut_19, 
    'x_build_parser__mutmut_20': x_build_parser__mutmut_20, 
    'x_build_parser__mutmut_21': x_build_parser__mutmut_21, 
    'x_build_parser__mutmut_22': x_build_parser__mutmut_22, 
    'x_build_parser__mutmut_23': x_build_parser__mutmut_23, 
    'x_build_parser__mutmut_24': x_build_parser__mutmut_24, 
    'x_build_parser__mutmut_25': x_build_parser__mutmut_25, 
    'x_build_parser__mutmut_26': x_build_parser__mutmut_26, 
    'x_build_parser__mutmut_27': x_build_parser__mutmut_27, 
    'x_build_parser__mutmut_28': x_build_parser__mutmut_28, 
    'x_build_parser__mutmut_29': x_build_parser__mutmut_29, 
    'x_build_parser__mutmut_30': x_build_parser__mutmut_30, 
    'x_build_parser__mutmut_31': x_build_parser__mutmut_31, 
    'x_build_parser__mutmut_32': x_build_parser__mutmut_32, 
    'x_build_parser__mutmut_33': x_build_parser__mutmut_33, 
    'x_build_parser__mutmut_34': x_build_parser__mutmut_34, 
    'x_build_parser__mutmut_35': x_build_parser__mutmut_35, 
    'x_build_parser__mutmut_36': x_build_parser__mutmut_36, 
    'x_build_parser__mutmut_37': x_build_parser__mutmut_37, 
    'x_build_parser__mutmut_38': x_build_parser__mutmut_38, 
    'x_build_parser__mutmut_39': x_build_parser__mutmut_39, 
    'x_build_parser__mutmut_40': x_build_parser__mutmut_40, 
    'x_build_parser__mutmut_41': x_build_parser__mutmut_41, 
    'x_build_parser__mutmut_42': x_build_parser__mutmut_42, 
    'x_build_parser__mutmut_43': x_build_parser__mutmut_43, 
    'x_build_parser__mutmut_44': x_build_parser__mutmut_44, 
    'x_build_parser__mutmut_45': x_build_parser__mutmut_45, 
    'x_build_parser__mutmut_46': x_build_parser__mutmut_46, 
    'x_build_parser__mutmut_47': x_build_parser__mutmut_47, 
    'x_build_parser__mutmut_48': x_build_parser__mutmut_48, 
    'x_build_parser__mutmut_49': x_build_parser__mutmut_49, 
    'x_build_parser__mutmut_50': x_build_parser__mutmut_50, 
    'x_build_parser__mutmut_51': x_build_parser__mutmut_51, 
    'x_build_parser__mutmut_52': x_build_parser__mutmut_52, 
    'x_build_parser__mutmut_53': x_build_parser__mutmut_53, 
    'x_build_parser__mutmut_54': x_build_parser__mutmut_54, 
    'x_build_parser__mutmut_55': x_build_parser__mutmut_55, 
    'x_build_parser__mutmut_56': x_build_parser__mutmut_56, 
    'x_build_parser__mutmut_57': x_build_parser__mutmut_57, 
    'x_build_parser__mutmut_58': x_build_parser__mutmut_58, 
    'x_build_parser__mutmut_59': x_build_parser__mutmut_59, 
    'x_build_parser__mutmut_60': x_build_parser__mutmut_60, 
    'x_build_parser__mutmut_61': x_build_parser__mutmut_61, 
    'x_build_parser__mutmut_62': x_build_parser__mutmut_62, 
    'x_build_parser__mutmut_63': x_build_parser__mutmut_63, 
    'x_build_parser__mutmut_64': x_build_parser__mutmut_64, 
    'x_build_parser__mutmut_65': x_build_parser__mutmut_65, 
    'x_build_parser__mutmut_66': x_build_parser__mutmut_66, 
    'x_build_parser__mutmut_67': x_build_parser__mutmut_67, 
    'x_build_parser__mutmut_68': x_build_parser__mutmut_68, 
    'x_build_parser__mutmut_69': x_build_parser__mutmut_69, 
    'x_build_parser__mutmut_70': x_build_parser__mutmut_70, 
    'x_build_parser__mutmut_71': x_build_parser__mutmut_71, 
    'x_build_parser__mutmut_72': x_build_parser__mutmut_72, 
    'x_build_parser__mutmut_73': x_build_parser__mutmut_73, 
    'x_build_parser__mutmut_74': x_build_parser__mutmut_74, 
    'x_build_parser__mutmut_75': x_build_parser__mutmut_75, 
    'x_build_parser__mutmut_76': x_build_parser__mutmut_76, 
    'x_build_parser__mutmut_77': x_build_parser__mutmut_77, 
    'x_build_parser__mutmut_78': x_build_parser__mutmut_78, 
    'x_build_parser__mutmut_79': x_build_parser__mutmut_79, 
    'x_build_parser__mutmut_80': x_build_parser__mutmut_80, 
    'x_build_parser__mutmut_81': x_build_parser__mutmut_81, 
    'x_build_parser__mutmut_82': x_build_parser__mutmut_82, 
    'x_build_parser__mutmut_83': x_build_parser__mutmut_83, 
    'x_build_parser__mutmut_84': x_build_parser__mutmut_84, 
    'x_build_parser__mutmut_85': x_build_parser__mutmut_85, 
    'x_build_parser__mutmut_86': x_build_parser__mutmut_86, 
    'x_build_parser__mutmut_87': x_build_parser__mutmut_87, 
    'x_build_parser__mutmut_88': x_build_parser__mutmut_88, 
    'x_build_parser__mutmut_89': x_build_parser__mutmut_89, 
    'x_build_parser__mutmut_90': x_build_parser__mutmut_90, 
    'x_build_parser__mutmut_91': x_build_parser__mutmut_91, 
    'x_build_parser__mutmut_92': x_build_parser__mutmut_92, 
    'x_build_parser__mutmut_93': x_build_parser__mutmut_93, 
    'x_build_parser__mutmut_94': x_build_parser__mutmut_94, 
    'x_build_parser__mutmut_95': x_build_parser__mutmut_95, 
    'x_build_parser__mutmut_96': x_build_parser__mutmut_96, 
    'x_build_parser__mutmut_97': x_build_parser__mutmut_97, 
    'x_build_parser__mutmut_98': x_build_parser__mutmut_98, 
    'x_build_parser__mutmut_99': x_build_parser__mutmut_99, 
    'x_build_parser__mutmut_100': x_build_parser__mutmut_100, 
    'x_build_parser__mutmut_101': x_build_parser__mutmut_101, 
    'x_build_parser__mutmut_102': x_build_parser__mutmut_102, 
    'x_build_parser__mutmut_103': x_build_parser__mutmut_103, 
    'x_build_parser__mutmut_104': x_build_parser__mutmut_104, 
    'x_build_parser__mutmut_105': x_build_parser__mutmut_105, 
    'x_build_parser__mutmut_106': x_build_parser__mutmut_106, 
    'x_build_parser__mutmut_107': x_build_parser__mutmut_107, 
    'x_build_parser__mutmut_108': x_build_parser__mutmut_108, 
    'x_build_parser__mutmut_109': x_build_parser__mutmut_109, 
    'x_build_parser__mutmut_110': x_build_parser__mutmut_110, 
    'x_build_parser__mutmut_111': x_build_parser__mutmut_111, 
    'x_build_parser__mutmut_112': x_build_parser__mutmut_112, 
    'x_build_parser__mutmut_113': x_build_parser__mutmut_113, 
    'x_build_parser__mutmut_114': x_build_parser__mutmut_114, 
    'x_build_parser__mutmut_115': x_build_parser__mutmut_115, 
    'x_build_parser__mutmut_116': x_build_parser__mutmut_116, 
    'x_build_parser__mutmut_117': x_build_parser__mutmut_117, 
    'x_build_parser__mutmut_118': x_build_parser__mutmut_118, 
    'x_build_parser__mutmut_119': x_build_parser__mutmut_119, 
    'x_build_parser__mutmut_120': x_build_parser__mutmut_120, 
    'x_build_parser__mutmut_121': x_build_parser__mutmut_121, 
    'x_build_parser__mutmut_122': x_build_parser__mutmut_122, 
    'x_build_parser__mutmut_123': x_build_parser__mutmut_123, 
    'x_build_parser__mutmut_124': x_build_parser__mutmut_124, 
    'x_build_parser__mutmut_125': x_build_parser__mutmut_125, 
    'x_build_parser__mutmut_126': x_build_parser__mutmut_126, 
    'x_build_parser__mutmut_127': x_build_parser__mutmut_127, 
    'x_build_parser__mutmut_128': x_build_parser__mutmut_128, 
    'x_build_parser__mutmut_129': x_build_parser__mutmut_129, 
    'x_build_parser__mutmut_130': x_build_parser__mutmut_130, 
    'x_build_parser__mutmut_131': x_build_parser__mutmut_131, 
    'x_build_parser__mutmut_132': x_build_parser__mutmut_132, 
    'x_build_parser__mutmut_133': x_build_parser__mutmut_133, 
    'x_build_parser__mutmut_134': x_build_parser__mutmut_134, 
    'x_build_parser__mutmut_135': x_build_parser__mutmut_135, 
    'x_build_parser__mutmut_136': x_build_parser__mutmut_136, 
    'x_build_parser__mutmut_137': x_build_parser__mutmut_137, 
    'x_build_parser__mutmut_138': x_build_parser__mutmut_138, 
    'x_build_parser__mutmut_139': x_build_parser__mutmut_139, 
    'x_build_parser__mutmut_140': x_build_parser__mutmut_140, 
    'x_build_parser__mutmut_141': x_build_parser__mutmut_141, 
    'x_build_parser__mutmut_142': x_build_parser__mutmut_142, 
    'x_build_parser__mutmut_143': x_build_parser__mutmut_143, 
    'x_build_parser__mutmut_144': x_build_parser__mutmut_144, 
    'x_build_parser__mutmut_145': x_build_parser__mutmut_145, 
    'x_build_parser__mutmut_146': x_build_parser__mutmut_146, 
    'x_build_parser__mutmut_147': x_build_parser__mutmut_147, 
    'x_build_parser__mutmut_148': x_build_parser__mutmut_148, 
    'x_build_parser__mutmut_149': x_build_parser__mutmut_149, 
    'x_build_parser__mutmut_150': x_build_parser__mutmut_150, 
    'x_build_parser__mutmut_151': x_build_parser__mutmut_151, 
    'x_build_parser__mutmut_152': x_build_parser__mutmut_152, 
    'x_build_parser__mutmut_153': x_build_parser__mutmut_153, 
    'x_build_parser__mutmut_154': x_build_parser__mutmut_154, 
    'x_build_parser__mutmut_155': x_build_parser__mutmut_155, 
    'x_build_parser__mutmut_156': x_build_parser__mutmut_156, 
    'x_build_parser__mutmut_157': x_build_parser__mutmut_157, 
    'x_build_parser__mutmut_158': x_build_parser__mutmut_158, 
    'x_build_parser__mutmut_159': x_build_parser__mutmut_159, 
    'x_build_parser__mutmut_160': x_build_parser__mutmut_160, 
    'x_build_parser__mutmut_161': x_build_parser__mutmut_161, 
    'x_build_parser__mutmut_162': x_build_parser__mutmut_162, 
    'x_build_parser__mutmut_163': x_build_parser__mutmut_163, 
    'x_build_parser__mutmut_164': x_build_parser__mutmut_164, 
    'x_build_parser__mutmut_165': x_build_parser__mutmut_165, 
    'x_build_parser__mutmut_166': x_build_parser__mutmut_166, 
    'x_build_parser__mutmut_167': x_build_parser__mutmut_167, 
    'x_build_parser__mutmut_168': x_build_parser__mutmut_168, 
    'x_build_parser__mutmut_169': x_build_parser__mutmut_169, 
    'x_build_parser__mutmut_170': x_build_parser__mutmut_170, 
    'x_build_parser__mutmut_171': x_build_parser__mutmut_171, 
    'x_build_parser__mutmut_172': x_build_parser__mutmut_172, 
    'x_build_parser__mutmut_173': x_build_parser__mutmut_173, 
    'x_build_parser__mutmut_174': x_build_parser__mutmut_174, 
    'x_build_parser__mutmut_175': x_build_parser__mutmut_175, 
    'x_build_parser__mutmut_176': x_build_parser__mutmut_176, 
    'x_build_parser__mutmut_177': x_build_parser__mutmut_177, 
    'x_build_parser__mutmut_178': x_build_parser__mutmut_178, 
    'x_build_parser__mutmut_179': x_build_parser__mutmut_179, 
    'x_build_parser__mutmut_180': x_build_parser__mutmut_180, 
    'x_build_parser__mutmut_181': x_build_parser__mutmut_181, 
    'x_build_parser__mutmut_182': x_build_parser__mutmut_182, 
    'x_build_parser__mutmut_183': x_build_parser__mutmut_183, 
    'x_build_parser__mutmut_184': x_build_parser__mutmut_184, 
    'x_build_parser__mutmut_185': x_build_parser__mutmut_185, 
    'x_build_parser__mutmut_186': x_build_parser__mutmut_186, 
    'x_build_parser__mutmut_187': x_build_parser__mutmut_187, 
    'x_build_parser__mutmut_188': x_build_parser__mutmut_188, 
    'x_build_parser__mutmut_189': x_build_parser__mutmut_189, 
    'x_build_parser__mutmut_190': x_build_parser__mutmut_190, 
    'x_build_parser__mutmut_191': x_build_parser__mutmut_191, 
    'x_build_parser__mutmut_192': x_build_parser__mutmut_192, 
    'x_build_parser__mutmut_193': x_build_parser__mutmut_193, 
    'x_build_parser__mutmut_194': x_build_parser__mutmut_194, 
    'x_build_parser__mutmut_195': x_build_parser__mutmut_195, 
    'x_build_parser__mutmut_196': x_build_parser__mutmut_196, 
    'x_build_parser__mutmut_197': x_build_parser__mutmut_197, 
    'x_build_parser__mutmut_198': x_build_parser__mutmut_198, 
    'x_build_parser__mutmut_199': x_build_parser__mutmut_199, 
    'x_build_parser__mutmut_200': x_build_parser__mutmut_200, 
    'x_build_parser__mutmut_201': x_build_parser__mutmut_201, 
    'x_build_parser__mutmut_202': x_build_parser__mutmut_202, 
    'x_build_parser__mutmut_203': x_build_parser__mutmut_203, 
    'x_build_parser__mutmut_204': x_build_parser__mutmut_204, 
    'x_build_parser__mutmut_205': x_build_parser__mutmut_205, 
    'x_build_parser__mutmut_206': x_build_parser__mutmut_206, 
    'x_build_parser__mutmut_207': x_build_parser__mutmut_207, 
    'x_build_parser__mutmut_208': x_build_parser__mutmut_208, 
    'x_build_parser__mutmut_209': x_build_parser__mutmut_209, 
    'x_build_parser__mutmut_210': x_build_parser__mutmut_210, 
    'x_build_parser__mutmut_211': x_build_parser__mutmut_211, 
    'x_build_parser__mutmut_212': x_build_parser__mutmut_212, 
    'x_build_parser__mutmut_213': x_build_parser__mutmut_213, 
    'x_build_parser__mutmut_214': x_build_parser__mutmut_214, 
    'x_build_parser__mutmut_215': x_build_parser__mutmut_215, 
    'x_build_parser__mutmut_216': x_build_parser__mutmut_216, 
    'x_build_parser__mutmut_217': x_build_parser__mutmut_217, 
    'x_build_parser__mutmut_218': x_build_parser__mutmut_218, 
    'x_build_parser__mutmut_219': x_build_parser__mutmut_219, 
    'x_build_parser__mutmut_220': x_build_parser__mutmut_220, 
    'x_build_parser__mutmut_221': x_build_parser__mutmut_221, 
    'x_build_parser__mutmut_222': x_build_parser__mutmut_222, 
    'x_build_parser__mutmut_223': x_build_parser__mutmut_223, 
    'x_build_parser__mutmut_224': x_build_parser__mutmut_224, 
    'x_build_parser__mutmut_225': x_build_parser__mutmut_225, 
    'x_build_parser__mutmut_226': x_build_parser__mutmut_226, 
    'x_build_parser__mutmut_227': x_build_parser__mutmut_227, 
    'x_build_parser__mutmut_228': x_build_parser__mutmut_228, 
    'x_build_parser__mutmut_229': x_build_parser__mutmut_229, 
    'x_build_parser__mutmut_230': x_build_parser__mutmut_230, 
    'x_build_parser__mutmut_231': x_build_parser__mutmut_231, 
    'x_build_parser__mutmut_232': x_build_parser__mutmut_232, 
    'x_build_parser__mutmut_233': x_build_parser__mutmut_233, 
    'x_build_parser__mutmut_234': x_build_parser__mutmut_234, 
    'x_build_parser__mutmut_235': x_build_parser__mutmut_235, 
    'x_build_parser__mutmut_236': x_build_parser__mutmut_236, 
    'x_build_parser__mutmut_237': x_build_parser__mutmut_237, 
    'x_build_parser__mutmut_238': x_build_parser__mutmut_238, 
    'x_build_parser__mutmut_239': x_build_parser__mutmut_239, 
    'x_build_parser__mutmut_240': x_build_parser__mutmut_240, 
    'x_build_parser__mutmut_241': x_build_parser__mutmut_241, 
    'x_build_parser__mutmut_242': x_build_parser__mutmut_242, 
    'x_build_parser__mutmut_243': x_build_parser__mutmut_243, 
    'x_build_parser__mutmut_244': x_build_parser__mutmut_244, 
    'x_build_parser__mutmut_245': x_build_parser__mutmut_245, 
    'x_build_parser__mutmut_246': x_build_parser__mutmut_246, 
    'x_build_parser__mutmut_247': x_build_parser__mutmut_247, 
    'x_build_parser__mutmut_248': x_build_parser__mutmut_248, 
    'x_build_parser__mutmut_249': x_build_parser__mutmut_249, 
    'x_build_parser__mutmut_250': x_build_parser__mutmut_250, 
    'x_build_parser__mutmut_251': x_build_parser__mutmut_251, 
    'x_build_parser__mutmut_252': x_build_parser__mutmut_252, 
    'x_build_parser__mutmut_253': x_build_parser__mutmut_253, 
    'x_build_parser__mutmut_254': x_build_parser__mutmut_254, 
    'x_build_parser__mutmut_255': x_build_parser__mutmut_255, 
    'x_build_parser__mutmut_256': x_build_parser__mutmut_256, 
    'x_build_parser__mutmut_257': x_build_parser__mutmut_257, 
    'x_build_parser__mutmut_258': x_build_parser__mutmut_258, 
    'x_build_parser__mutmut_259': x_build_parser__mutmut_259, 
    'x_build_parser__mutmut_260': x_build_parser__mutmut_260, 
    'x_build_parser__mutmut_261': x_build_parser__mutmut_261, 
    'x_build_parser__mutmut_262': x_build_parser__mutmut_262
}

def build_parser(*args, **kwargs):
    result = _mutmut_trampoline(x_build_parser__mutmut_orig, x_build_parser__mutmut_mutants, args, kwargs)
    return result 

build_parser.__signature__ = _mutmut_signature(x_build_parser__mutmut_orig)
x_build_parser__mutmut_orig.__name__ = 'x_build_parser'


# Parse les arguments fournis à la CLI
def x_parse_args__mutmut_orig(argv: Sequence[str] | None = None) -> argparse.Namespace:
    """Parse les arguments passés à mybci."""

    # Construit le parser pour traiter argv
    parser = build_parser()
    # Retourne l'espace de noms après parsing
    return parser.parse_args(argv)


# Parse les arguments fournis à la CLI
def x_parse_args__mutmut_1(argv: Sequence[str] | None = None) -> argparse.Namespace:
    """Parse les arguments passés à mybci."""

    # Construit le parser pour traiter argv
    parser = None
    # Retourne l'espace de noms après parsing
    return parser.parse_args(argv)


# Parse les arguments fournis à la CLI
def x_parse_args__mutmut_2(argv: Sequence[str] | None = None) -> argparse.Namespace:
    """Parse les arguments passés à mybci."""

    # Construit le parser pour traiter argv
    parser = build_parser()
    # Retourne l'espace de noms après parsing
    return parser.parse_args(None)

x_parse_args__mutmut_mutants : ClassVar[MutantDict] = {
'x_parse_args__mutmut_1': x_parse_args__mutmut_1, 
    'x_parse_args__mutmut_2': x_parse_args__mutmut_2
}

def parse_args(*args, **kwargs):
    result = _mutmut_trampoline(x_parse_args__mutmut_orig, x_parse_args__mutmut_mutants, args, kwargs)
    return result 

parse_args.__signature__ = _mutmut_signature(x_parse_args__mutmut_orig)
x_parse_args__mutmut_orig.__name__ = 'x_parse_args'


# Point d'entrée principal de la CLI
def x_main__mutmut_orig(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_1(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = None
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_2(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(None) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_3(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_4(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(None)
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_5(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[2:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_6(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_7(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = None
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_8(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(None)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_9(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_10(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler != "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_11(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "XXnoneXX" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_12(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "NONE" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_13(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = None
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_14(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_15(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = None

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_16(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(None, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_17(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, None, None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_18(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr("n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_19(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_20(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", )

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_21(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "XXn_componentsXX", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_22(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "N_COMPONENTS", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_23(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = None

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_24(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=None,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_25(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=None,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_26(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=None,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_27(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=None,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_28(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=None,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_29(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=None,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_30(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=None,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_31(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=None,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_32(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_33(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_34(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_35(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_36(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_37(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_38(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_39(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_40(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode != "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_41(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "XXtrainXX":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_42(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "TRAIN":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_43(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            None,
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_44(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            None,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_45(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_46(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_47(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "XXtpv.trainXX",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_48(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "TPV.TRAIN",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_49(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode != "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_50(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "XXpredictXX":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_51(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "PREDICT":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_52(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            None,
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_53(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            None,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_54(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_55(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_56(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "XXtpv.predictXX",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_57(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "TPV.PREDICT",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_58(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode != "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_59(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "XXrealtimeXX":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_60(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "REALTIME":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_61(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = None
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_62(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=None,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_63(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=None,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_64(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=None,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_65(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=None,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_66(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=None,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_67(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=None,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_68(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=None,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_69(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=None,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_70(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=None,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_71(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_72(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_73(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_74(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_75(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_76(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_77(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_78(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_79(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(realtime_config)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0


# Point d'entrée principal de la CLI
def x_main__mutmut_80(argv: Sequence[str] | None = None) -> int:
    """Point d'entrée exécutable de mybci."""

    # Capture les arguments fournis ou la ligne de commande réelle
    provided_args = list(argv) if argv is not None else list(sys.argv[1:])
    # Lance le runner global lorsque la commande ne fournit aucun argument
    if not provided_args:
        # Exécute la boucle des six expériences sur les 109 sujets
        return _run_global_evaluation()
    # Parse les arguments fournis par l'utilisateur
    args = parse_args(provided_args)
    # Interprète le choix du scaler pour convertir "none" en None
    scaler = None if args.scaler == "none" else args.scaler
    # Applique la normalisation en inversant le flag d'opt-out
    normalize_features = not args.no_normalize_features
    # Construit la configuration partagée entre les modules train et predict
    n_components = getattr(args, "n_components", None)

    # Construit la configuration de pipeline commune
    config = ModuleCallConfig(
        subject=args.subject,
        run=args.run,
        classifier=args.classifier,
        scaler=scaler,
        feature_strategy=args.feature_strategy,
        dim_method=args.dim_method,
        n_components=n_components,
        normalize_features=normalize_features,
    )

    # Appelle le module train si le mode le demande
    if args.mode == "train":
        # Retourne le code retour du module train avec la configuration
        return _call_module(
            "tpv.train",
            config,
        )

    # Appelle le module predict pour préserver la sortie CLI attendue
    if args.mode == "predict":
        # Retourne le code retour du module predict avec la configuration
        return _call_module(
            "tpv.predict",
            config,
        )

    # Bascule vers le module realtime pour le streaming fenêtré
    if args.mode == "realtime":
        # Construit la configuration spécifique au lissage et fenêtrage
        realtime_config = RealtimeCallConfig(
            subject=args.subject,
            run=args.run,
            window_size=args.window_size,
            step_size=args.step_size,
            buffer_size=args.buffer_size,
            sfreq=args.sfreq,
            max_latency=args.max_latency,
            data_dir=args.data_dir,
            artifacts_dir=args.artifacts_dir,
        )
        # Retourne le code retour du module realtime avec la configuration
        return _call_realtime(None)

    # Convertit les répertoires en Path pour l'appel direct
    data_dir = Path(args.data_dir)
    artifacts_dir = Path(args.artifacts_dir)
    raw_dir = Path(args.raw_dir)

    # Évalue le run en récupérant prédictions et vérité terrain
    try:
        # Lance l'évaluation en rechargeant la pipeline entraînée
        result = tpv_predict.evaluate_run(
            args.subject,
            args.run,
            data_dir,
            artifacts_dir,
            raw_dir,
        )
    except FileNotFoundError as error:
        # Affiche une erreur compréhensible pour l'utilisateur CLI
        print(f"ERREUR: {error}")
        # Retourne un code non nul pour signaler l'échec
        return 1

    # Construit le rapport agrégé (accuracy globale, confusion, etc.)
    _ = tpv_predict.build_report(result)
    # Récupère les prédictions calculées par evaluate_run
    y_pred = result["predictions"]
    # Récupère la vérité terrain en privilégiant la clé harmonisée
    y_true = result.get("y_true", result["truth"])
    # Récupère l'accuracy globale pour l'affichage final
    accuracy = float(result["accuracy"])
    # Affiche le détail epoch par epoch dans le format attendu
    _print_epoch_predictions(y_true, y_pred, accuracy)
    # Retourne 0 pour signaler un succès CLI à l'utilisateur
    return 0

x_main__mutmut_mutants : ClassVar[MutantDict] = {
'x_main__mutmut_1': x_main__mutmut_1, 
    'x_main__mutmut_2': x_main__mutmut_2, 
    'x_main__mutmut_3': x_main__mutmut_3, 
    'x_main__mutmut_4': x_main__mutmut_4, 
    'x_main__mutmut_5': x_main__mutmut_5, 
    'x_main__mutmut_6': x_main__mutmut_6, 
    'x_main__mutmut_7': x_main__mutmut_7, 
    'x_main__mutmut_8': x_main__mutmut_8, 
    'x_main__mutmut_9': x_main__mutmut_9, 
    'x_main__mutmut_10': x_main__mutmut_10, 
    'x_main__mutmut_11': x_main__mutmut_11, 
    'x_main__mutmut_12': x_main__mutmut_12, 
    'x_main__mutmut_13': x_main__mutmut_13, 
    'x_main__mutmut_14': x_main__mutmut_14, 
    'x_main__mutmut_15': x_main__mutmut_15, 
    'x_main__mutmut_16': x_main__mutmut_16, 
    'x_main__mutmut_17': x_main__mutmut_17, 
    'x_main__mutmut_18': x_main__mutmut_18, 
    'x_main__mutmut_19': x_main__mutmut_19, 
    'x_main__mutmut_20': x_main__mutmut_20, 
    'x_main__mutmut_21': x_main__mutmut_21, 
    'x_main__mutmut_22': x_main__mutmut_22, 
    'x_main__mutmut_23': x_main__mutmut_23, 
    'x_main__mutmut_24': x_main__mutmut_24, 
    'x_main__mutmut_25': x_main__mutmut_25, 
    'x_main__mutmut_26': x_main__mutmut_26, 
    'x_main__mutmut_27': x_main__mutmut_27, 
    'x_main__mutmut_28': x_main__mutmut_28, 
    'x_main__mutmut_29': x_main__mutmut_29, 
    'x_main__mutmut_30': x_main__mutmut_30, 
    'x_main__mutmut_31': x_main__mutmut_31, 
    'x_main__mutmut_32': x_main__mutmut_32, 
    'x_main__mutmut_33': x_main__mutmut_33, 
    'x_main__mutmut_34': x_main__mutmut_34, 
    'x_main__mutmut_35': x_main__mutmut_35, 
    'x_main__mutmut_36': x_main__mutmut_36, 
    'x_main__mutmut_37': x_main__mutmut_37, 
    'x_main__mutmut_38': x_main__mutmut_38, 
    'x_main__mutmut_39': x_main__mutmut_39, 
    'x_main__mutmut_40': x_main__mutmut_40, 
    'x_main__mutmut_41': x_main__mutmut_41, 
    'x_main__mutmut_42': x_main__mutmut_42, 
    'x_main__mutmut_43': x_main__mutmut_43, 
    'x_main__mutmut_44': x_main__mutmut_44, 
    'x_main__mutmut_45': x_main__mutmut_45, 
    'x_main__mutmut_46': x_main__mutmut_46, 
    'x_main__mutmut_47': x_main__mutmut_47, 
    'x_main__mutmut_48': x_main__mutmut_48, 
    'x_main__mutmut_49': x_main__mutmut_49, 
    'x_main__mutmut_50': x_main__mutmut_50, 
    'x_main__mutmut_51': x_main__mutmut_51, 
    'x_main__mutmut_52': x_main__mutmut_52, 
    'x_main__mutmut_53': x_main__mutmut_53, 
    'x_main__mutmut_54': x_main__mutmut_54, 
    'x_main__mutmut_55': x_main__mutmut_55, 
    'x_main__mutmut_56': x_main__mutmut_56, 
    'x_main__mutmut_57': x_main__mutmut_57, 
    'x_main__mutmut_58': x_main__mutmut_58, 
    'x_main__mutmut_59': x_main__mutmut_59, 
    'x_main__mutmut_60': x_main__mutmut_60, 
    'x_main__mutmut_61': x_main__mutmut_61, 
    'x_main__mutmut_62': x_main__mutmut_62, 
    'x_main__mutmut_63': x_main__mutmut_63, 
    'x_main__mutmut_64': x_main__mutmut_64, 
    'x_main__mutmut_65': x_main__mutmut_65, 
    'x_main__mutmut_66': x_main__mutmut_66, 
    'x_main__mutmut_67': x_main__mutmut_67, 
    'x_main__mutmut_68': x_main__mutmut_68, 
    'x_main__mutmut_69': x_main__mutmut_69, 
    'x_main__mutmut_70': x_main__mutmut_70, 
    'x_main__mutmut_71': x_main__mutmut_71, 
    'x_main__mutmut_72': x_main__mutmut_72, 
    'x_main__mutmut_73': x_main__mutmut_73, 
    'x_main__mutmut_74': x_main__mutmut_74, 
    'x_main__mutmut_75': x_main__mutmut_75, 
    'x_main__mutmut_76': x_main__mutmut_76, 
    'x_main__mutmut_77': x_main__mutmut_77, 
    'x_main__mutmut_78': x_main__mutmut_78, 
    'x_main__mutmut_79': x_main__mutmut_79, 
    'x_main__mutmut_80': x_main__mutmut_80
}

def main(*args, **kwargs):
    result = _mutmut_trampoline(x_main__mutmut_orig, x_main__mutmut_mutants, args, kwargs)
    return result 

main.__signature__ = _mutmut_signature(x_main__mutmut_orig)
x_main__mutmut_orig.__name__ = 'x_main'


# Protège l'exécution directe pour déléguer au main
if __name__ == "__main__":  # pragma: no cover - CLI entrypoint
    # Expose le code retour comme exit code du processus
    raise SystemExit(main())
